url,content
https://en.wikipedia.org/wiki/Machine_learning,machine learning study computer algorithms improve automatically experience seen subset artificial intelligence machine learning algorithms build mathematical model based sample data known training data order make predictions decisions without explicitly programmed machine learning algorithms used wide variety applications email filtering computer vision difficult infeasible develop conventional algorithms perform needed tasks machine learning closely related computational statistics focuses making predictions using computers study mathematical optimization delivers methods theory application domains field machine learning data mining related field study focusing exploratory data analysis unsupervised learning application across business problems machine learning also referred predictive analytics simple definition machine learning application artificial intelligence provides systems ability automatically learn improve experience without explicitly programmed machine learning focuses development computer programs access data use learn machine learning involves computers discovering perform tasks without explicitly programmed involves computers learning data provided carry certain tasks simple tasks assigned computers possible program algorithms telling machine execute steps required solve problem hand computer part learning needed advanced tasks challenging human manually create needed algorithms practice turn effective help machine develop algorithm rather human programmers specify every needed step discipline machine learning employs various approaches teach computers accomplish tasks fully satisfactory algorithm available cases vast numbers potential answers exist one approach label correct answers valid used training data computer improve algorithm uses determine correct answers example train system task digital character recognition mnist dataset handwritten digits often used machine learning approaches traditionally divided three broad categories depending nature signal feedback available learning system approaches developed fit neatly three fold categorisation sometimes one used machine learning system example topic modeling dimensionality reduction meta learning deep learning become dominant approach much ongoing work field machine learning term machine learning coined arthur samuel american ibmer pioneer field computer gaming artificial intelligence representative book machine learning research nilsson book learning machines dealing mostly machine learning pattern classification interest related pattern recognition continued described duda hart report given using teaching strategies neural network learns recognize characters computer terminal tom mitchell provided widely quoted formal definition algorithms studied machine learning field computer program said learn experience e respect class tasks performance measure p performance tasks measured p improves experience e definition tasks machine learning concerned offers fundamentally operational definition rather defining field cognitive terms follows alan turing proposal paper computing machinery intelligence question machines think replaced question machines scientific endeavor machine learning grew quest artificial intelligence early days ai academic discipline researchers interested machines learn data attempted approach problem various symbolic methods well termed neural networks mostly perceptrons models later found reinventions generalized linear models statistics probabilistic reasoning also employed especially automated medical diagnosis however increasing emphasis logical knowledge based approach caused rift ai machine learning probabilistic systems plagued theoretical practical problems data acquisition representation expert systems come dominate ai statistics favor work symbolic knowledge based learning continue within ai leading inductive logic programming statistical line research outside field ai proper pattern recognition information retrieval neural networks research abandoned ai computer science around time line continued outside ai cs field connectionism researchers disciplines including hopfield rumelhart hinton main success came mid reinvention backpropagation machine learning reorganized separate field started flourish field changed goal achieving artificial intelligence tackling solvable problems practical nature shifted focus away symbolic approaches inherited ai toward methods models borrowed statistics probability theory many sources continue assert machine learning remains subfield ai yet practitioners example dr daniel hulme teaches ai runs company operating field argues machine learning ai separate machine learning data mining often employ methods overlap significantly machine learning focuses prediction based known properties learned training data data mining focuses discovery unknown properties data data mining uses many machine learning methods different goals hand machine learning also employs data mining methods unsupervised learning preprocessing step improve learner accuracy much confusion two research communities comes basic assumptions work machine learning performance usually evaluated respect ability reproduce known knowledge knowledge discovery data mining key task discovery previously unknown knowledge evaluated respect known knowledge uninformed method easily outperformed supervised methods typical kdd task supervised methods cannot used due unavailability training data machine learning also intimate ties optimization many learning problems formulated minimization loss function training set examples loss functions express discrepancy predictions model trained actual problem instances difference two fields arises goal generalization optimization algorithms minimize loss training set machine learning concerned minimizing loss unseen samples machine learning statistics closely related fields terms methods distinct principal goal statistics draws population inferences sample machine learning finds generalizable predictive patterns according michael jordan ideas machine learning methodological principles theoretical tools long pre history statistics also suggested term data science placeholder call overall field leo breiman distinguished two statistical modeling paradigms data model algorithmic model wherein algorithmic model means less machine learning algorithms like random forest statisticians adopted methods machine learning leading combined field call statistical learning core objective learner generalize experience generalization context ability learning machine perform accurately new unseen examples tasks experienced learning data set training examples come generally unknown probability distribution learner build general model space enables produce sufficiently accurate predictions new cases computational analysis machine learning algorithms performance branch theoretical computer science known computational learning theory training sets finite future uncertain learning theory usually yield guarantees performance algorithms instead probabilistic bounds performance quite common bias variance decomposition one way quantify generalization error best performance context generalization complexity hypothesis match complexity function underlying data hypothesis less complex function model fitted data complexity model increased response training error decreases hypothesis complex model subject overfitting generalization poorer addition performance bounds learning theorists study time complexity feasibility learning computational learning theory computation considered feasible done polynomial time two kinds time complexity results positive results show certain class functions learned polynomial time negative results show certain classes cannot learned polynomial time types machine learning algorithms differ approach type data input output type task problem intended solve supervised learning algorithms build mathematical model set data contains inputs desired outputs data known training data consists set training examples training example one inputs desired output also known supervisory signal mathematical model training example represented array vector sometimes called feature vector training data represented matrix iterative optimization objective function supervised learning algorithms learn function used predict output associated new inputs optimal function allow algorithm correctly determine output inputs part training data algorithm improves accuracy outputs predictions time said learned perform task types supervised learning algorithms include active learning classification regression classification algorithms used outputs restricted limited set values regression algorithms used outputs may numerical value within range example classification algorithm filters emails input would incoming email output would name folder file email similarity learning area supervised machine learning closely related regression classification goal learn examples using similarity function measures similar related two objects applications ranking recommendation systems visual identity tracking face verification speaker verification unsupervised learning algorithms take set data contains inputs find structure data like grouping clustering data points algorithms therefore learn test data labeled classified categorized instead responding feedback unsupervised learning algorithms identify commonalities data react based presence absence commonalities new piece data central application unsupervised learning field density estimation statistics finding probability density function though unsupervised learning encompasses domains involving summarizing explaining data features cluster analysis assignment set observations subsets observations within cluster similar according one predesignated criteria observations drawn different clusters dissimilar different clustering techniques make different assumptions structure data often defined similarity metric evaluated example internal compactness similarity members cluster separation difference clusters methods based estimated density graph connectivity semi supervised learning falls unsupervised learning supervised learning training examples missing training labels yet many machine learning researchers found unlabeled data used conjunction small amount labeled data produce considerable improvement learning accuracy weakly supervised learning training labels noisy limited imprecise however labels often cheaper obtain resulting larger effective training sets reinforcement learning area machine learning concerned software agents ought take actions environment maximize notion cumulative reward due generality field studied many disciplines game theory control theory operations research information theory simulation based optimization multi agent systems swarm intelligence statistics genetic algorithms machine learning environment typically represented markov decision process many reinforcement learning algorithms use dynamic programming techniques reinforcement learning algorithms assume knowledge exact mathematical model mdp used exact models infeasible reinforcement learning algorithms used autonomous vehicles learning play game human opponent self learning machine learning paradigm introduced along neural network capable self learning named crossbar adaptive array learning external rewards external teacher advice caa self learning algorithm computes crossbar fashion decisions actions emotions consequence situations system driven interaction cognition emotion self learning algorithm updates memory matrix w w iteration executes following machine learning routine system one input situation one output action neither separate reinforcement input advice input environment backpropagated value emotion toward consequence situation caa exists two environments one behavioral environment behaves genetic environment wherefrom initially receives initial emotions situations encountered behavioral environment receiving genome vector genetic environment caa learns goal seeking behavior environment contains desirable undesirable situations several learning algorithms aim discovering better representations inputs provided training classic examples include principal components analysis cluster analysis feature learning algorithms also called representation learning algorithms often attempt preserve information input also transform way makes useful often pre processing step performing classification predictions technique allows reconstruction inputs coming unknown data generating distribution necessarily faithful configurations implausible distribution replaces manual feature engineering allows machine learn features use perform specific task feature learning either supervised unsupervised supervised feature learning features learned using labeled input data examples include artificial neural networks multilayer perceptrons supervised dictionary learning unsupervised feature learning features learned unlabeled input data examples include dictionary learning independent component analysis autoencoders matrix factorization various forms clustering manifold learning algorithms attempt constraint learned representation low dimensional sparse coding algorithms attempt constraint learned representation sparse meaning mathematical model many zeros multilinear subspace learning algorithms aim learn low dimensional representations directly tensor representations multidimensional data without reshaping higher dimensional vectors deep learning algorithms discover multiple levels representation hierarchy features higher level abstract features defined terms lower level features argued intelligent machine one learns representation disentangles underlying factors variation explain observed data feature learning motivated fact machine learning tasks classification often require input mathematically computationally convenient process however real world data images video sensory data yielded attempts algorithmically define specific features alternative discover features representations thorough examination without relying explicit algorithms sparse dictionary learning feature learning method training example represented linear combination basis functions assumed sparse matrix method strongly np hard difficult solve approximately popular heuristic method sparse dictionary learning k svd algorithm sparse dictionary learning applied several contexts classification problem determine class previously unseen training example belongs dictionary class already built new training example associated class best sparsely represented corresponding dictionary sparse dictionary learning also applied image de noising key idea clean image patch sparsely represented image dictionary noise cannot data mining anomaly detection also known outlier detection identification rare items events observations raise suspicions differing significantly majority data typically anomalous items represent issue bank fraud structural defect medical problems errors text anomalies referred outliers novelties noise deviations exceptions particular context abuse network intrusion detection interesting objects often rare objects unexpected bursts inactivity pattern adhere common statistical definition outlier rare object many outlier detection methods fail data unless aggregated appropriately instead cluster analysis algorithm may able detect micro clusters formed patterns three broad categories anomaly detection techniques exist unsupervised anomaly detection techniques detect anomalies unlabeled test data set assumption majority instances data set normal looking instances seem fit least remainder data set supervised anomaly detection techniques require data set labeled normal abnormal involves training classifier semi supervised anomaly detection techniques construct model representing normal behavior given normal training data set test likelihood test instance generated model developmental robotics robot learning algorithms generate sequences learning experiences also known curriculum cumulatively acquire new skills self guided exploration social interaction humans robots use guidance mechanisms active learning maturation motor synergies imitation association rule learning rule based machine learning method discovering relationships variables large databases intended identify strong rules discovered databases using measure interestingness rule based machine learning general term machine learning method identifies learns evolves rules store manipulate apply knowledge defining characteristic rule based machine learning algorithm identification utilization set relational rules collectively represent knowledge captured system contrast machine learning algorithms commonly identify singular model universally applied instance order make prediction rule based machine learning approaches include learning classifier systems association rule learning artificial immune systems based concept strong rules rakesh agrawal tomasz imieli ski arun swami introduced association rules discovering regularities products large scale transaction data recorded point sale systems supermarkets example rule n n p e b u r g e r displaystyle mathrm onions potatoes rightarrow mathrm burger found sales data supermarket would indicate customer buys onions potatoes together likely also buy hamburger meat information used basis decisions marketing activities promotional pricing product placements addition market basket analysis association rules employed today application areas including web usage mining intrusion detection continuous production bioinformatics contrast sequence mining association rule learning typically consider order items either within transaction across transactions learning classifier systems family rule based machine learning algorithms combine discovery component typically genetic algorithm learning component performing either supervised learning reinforcement learning unsupervised learning seek identify set context dependent rules collectively store apply knowledge piecewise manner order make predictions inductive logic programming approach rule learning using logic programming uniform representation input examples background knowledge hypotheses given encoding known background knowledge set examples represented logical database facts ilp system derive hypothesized logic program entails positive negative examples inductive programming related field considers kind programming language representing hypotheses functional programs inductive logic programming particularly useful bioinformatics natural language processing gordon plotkin ehud shapiro laid initial theoretical foundation inductive machine learning logical setting shapiro built first implementation prolog program inductively inferred logic programs positive negative examples term inductive refers philosophical induction suggesting theory explain observed facts rather mathematical induction proving property members well ordered set performing machine learning involves creating model trained training data process additional data make predictions various types models used researched machine learning systems artificial neural networks connectionist systems computing systems vaguely inspired biological neural networks constitute animal brains systems learn perform tasks considering examples generally without programmed task specific rules ann model based collection connected units nodes called artificial neurons loosely model neurons biological brain connection like synapses biological brain transmit information signal one artificial neuron another artificial neuron receives signal process signal additional artificial neurons connected common ann implementations signal connection artificial neurons real number output artificial neuron computed non linear function sum inputs connections artificial neurons called edges artificial neurons edges typically weight adjusts learning proceeds weight increases decreases strength signal connection artificial neurons may threshold signal sent aggregate signal crosses threshold typically artificial neurons aggregated layers different layers may perform different kinds transformations inputs signals travel first layer last layer possibly traversing layers multiple times original goal ann approach solve problems way human brain would however time attention moved performing specific tasks leading deviations biology artificial neural networks used variety tasks including computer vision speech recognition machine translation social network filtering playing board video games medical diagnosis deep learning consists multiple hidden layers artificial neural network approach tries model way human brain processes light sound vision hearing successful applications deep learning computer vision speech recognition decision tree learning uses decision tree predictive model go observations item conclusions item target value one predictive modeling approaches used statistics data mining machine learning tree models target variable take discrete set values called classification trees tree structures leaves represent class labels branches represent conjunctions features lead class labels decision trees target variable take continuous values called regression trees decision analysis decision tree used visually explicitly represent decisions decision making data mining decision tree describes data resulting classification tree input decision making support vector machines also known support vector networks set related supervised learning methods used classification regression given set training examples marked belonging one two categories svm training algorithm builds model predicts whether new example falls one category svm training algorithm non probabilistic binary linear classifier although methods platt scaling exist use svm probabilistic classification setting addition performing linear classification svms efficiently perform non linear classification using called kernel trick implicitly mapping inputs high dimensional feature spaces regression analysis encompasses large variety statistical methods estimate relationship input variables associated features common form linear regression single line drawn best fit given data according mathematical criterion ordinary least squares latter often extended regularization methods mitigate overfitting bias ridge regression dealing non linear problems go models include polynomial regression logistic regression even kernel regression introduces non linearity taking advantage kernel trick implicitly map input variables higher dimensional space bayesian network belief network directed acyclic graphical model probabilistic graphical model represents set random variables conditional independence directed acyclic graph example bayesian network could represent probabilistic relationships diseases symptoms given symptoms network used compute probabilities presence various diseases efficient algorithms exist perform inference learning bayesian networks model sequences variables like speech signals protein sequences called dynamic bayesian networks generalizations bayesian networks represent solve decision problems uncertainty called influence diagrams genetic algorithm search algorithm heuristic technique mimics process natural selection using methods mutation crossover generate new genotypes hope finding good solutions given problem machine learning genetic algorithms used conversely machine learning techniques used improve performance genetic evolutionary algorithms usually machine learning models require lot data order perform well usually training machine learning model one needs collect large representative sample data training set data training set varied corpus text collection images data collected individual users service overfitting something watch training machine learning model federated learning adapted form distributed artificial intelligence training machine learning models decentralizes training process allowing users privacy maintained needing send data centralized server also increases efficiency decentralizing training process many devices example gboard uses federated machine learning train search query prediction models users mobile phones without send individual searches back google many applications machine learning including media services provider netflix held first netflix prize competition find program better predict user preferences improve accuracy existing cinematch movie recommendation algorithm least joint team made researchers labs research collaboration teams big chaos pragmatic theory built ensemble model win grand prize million shortly prize awarded netflix realized viewers ratings best indicators viewing patterns changed recommendation engine accordingly wall street journal wrote firm rebellion research use machine learning predict financial crisis co founder sun microsystems vinod khosla predicted medical doctors jobs would lost next two decades automated machine learning medical diagnostic software reported machine learning algorithm applied field art history study fine art paintings may revealed previously unrecognized influences among artists springer nature published first research book created using machine learning although machine learning transformative fields machine learning programs often fail deliver expected results reasons numerous lack data lack access data data bias privacy problems badly chosen tasks algorithms wrong tools people lack resources evaluation problems self driving car uber failed detect pedestrian killed collision attempts use machine learning healthcare ibm watson system failed deliver even years time billions dollars invested machine learning approaches particular suffer different data biases machine learning system trained current customers may able predict needs new customer groups represented training data trained man made data machine learning likely pick constitutional unconscious biases already present society language models learned data shown contain human like biases machine learning systems used criminal risk assessment found biased black people google photos would often tag black people gorillas still well resolved google reportedly still using workaround remove gorillas training data thus able recognize real gorillas similar issues recognizing non white people found many systems microsoft tested chatbot learned twitter quickly picked racist sexist language challenges effective use machine learning may take longer adopted domains concern fairness machine learning reducing bias machine learning propelling use human good increasingly expressed artificial intelligence scientists including fei fei li reminds engineers nothing artificial ai inspired people created people importantly impacts people powerful tool beginning understand profound responsibility classification machine learning models validated accuracy estimation techniques like holdout method splits data training test set evaluates performance training model test set comparison k fold cross validation method randomly partitions data k subsets k experiments performed respectively considering subset evaluation remaining k subsets training model addition holdout cross validation methods bootstrap samples n instances replacement dataset used assess model accuracy addition overall accuracy investigators frequently report sensitivity specificity meaning true positive rate true negative rate respectively similarly investigators sometimes report false positive rate well false negative rate however rates ratios fail reveal numerators denominators total operating characteristic effective method express model diagnostic ability toc shows numerators denominators previously mentioned rates thus toc provides information commonly used receiver operating characteristic roc associated area curve machine learning poses host ethical questions systems trained datasets collected biases may exhibit biases upon use thus digitizing cultural prejudices example using job hiring data firm racist hiring policies may lead machine learning system duplicating bias scoring job applicants similarity previous successful applicants responsible collection data documentation algorithmic rules used system thus critical part machine learning human languages contain biases machines trained language corpora necessarily also learn biases forms ethical challenges related personal biases seen health care concerns among health care professionals systems might designed public interest income generating machines especially true united states long standing ethical dilemma improving health care also increasing profits example algorithms could designed provide patients unnecessary tests medication algorithm proprietary owners hold stakes huge potential machine learning health care provide professionals great tool diagnose medicate even plan recovery paths patients happen personal biases mentioned previously greed biases addressed since advances machine learning algorithms computer hardware led efficient methods training deep neural networks contain many layers non linear hidden units graphic processing units often ai specific enhancements displaced cpus dominant method training large scale commercial cloud ai openai estimated hardware compute used largest deep learning projects alexnet alphazero found fold increase amount compute required doubling time trendline months software suites containing variety machine learning algorithms include following
https://en.wikipedia.org/wiki/Machine_Learning_(journal),machine learning peer reviewed scientific journal published since distinguished journal machine intelligence established mid forty editors members editorial board machine learning resigned order support journal machine learning research saying era internet detrimental researchers continue publishing papers expensive journals pay access archives instead wrote supported model jmlr authors retained copyright papers archives freely available internet following mass resignation kluwer changed publishing policy allow authors self archive papers online peer review
https://en.wikipedia.org/wiki/Statistical_learning_in_language_acquisition,statistical learning ability humans animals extract statistical regularities world around learn environment although statistical learning thought generalized learning mechanism phenomenon first identified human infant language acquisition earliest evidence statistical learning abilities comes study jenny saffran richard aslin elissa newport month old infants presented nonsense streams monotone speech stream composed four three syllable pseudowords repeated randomly exposure speech streams two minutes infants reacted differently hearing pseudowords opposed nonwords speech stream nonwords composed syllables infants exposed different order suggests infants able learn statistical relationships syllables even limited exposure language infants learn syllables always paired together ones occur together relatively rarely suggesting parts two different units method learning thought one way children learn groups syllables form individual words citation needed since initial discovery role statistical learning lexical acquisition mechanism proposed elements phonological acquisition syntactical acquisition well non linguistic domains research also indicated statistical learning likely domain general even species general learning mechanism occurring visual well auditory information primates non primates role statistical learning language acquisition particularly well documented area lexical acquisition one important contribution infants understanding segmenting words continuous stream speech ability recognize statistical regularities speech heard environments although many factors play important role specific mechanism powerful operate short time scale well established finding unlike written language spoken language clear boundaries words spoken language continuous stream sound rather individual words silences lack segmentation linguistic units presents problem young children learning language must able pick individual units continuous speech streams hear one proposed method children able solve problem attentive statistical regularities world around example phrase pretty baby children likely hear sounds pre ty heard together entirety lexical input around hear sounds ty ba together artificial grammar learning study adult participants saffran newport aslin found participants able locate word boundaries based transitional probabilities suggesting adults capable using statistical regularities language learning task robust finding widely replicated determine young children abilities saffran aslin newport exposed month old infants artificial grammar grammar composed four words composed three nonsense syllables experiment infants heard continuous speech stream words importantly speech presented monotone cues word boundaries statistical probabilities within word transitional probability two syllable pairs word bidaku example probability hearing syllable da immediately syllable bi words however transitional probability hearing syllable pair much lower given word presented one three words could follow likelihood hearing given syllable ku determine infants picking statistical information infant presented multiple presentations either word artificial grammar nonword made syllables presented random order infants presented nonwords test phase listened significantly longer words infants presented words artificial grammar showing novelty preference new nonwords however implementation test could also due infants learning serial order information actually learning transitional probabilities words test infants heard strings dapiku tilado never presented learning could simply learned syllable ku never followed syllable pi look closely issue saffran aslin newport conducted another study infants underwent training artificial grammar presented either words part words rather words nonwords part words syllable sequences composed last syllable one word first two syllables another part words heard time children listening artificial grammar preferential listening part words would indicate children learning serial order information also statistical likelihood hearing particular syllable sequences infants showed greater listening times novel words indicating month old infants able extract statistical regularities continuous speech stream result impetus much research role statistical learning lexical acquisition areas follow original report aslin saffran newport found even words part words occurred equally often speech stream different transitional probabilities syllables words part words infants still able detect statistical regularities still preferred listen novel part words familiarized words finding provides stronger evidence infants able pick transitional probabilities speech hear rather aware frequencies individual syllable sequences another follow study examined extent statistical information learned type artificial grammar learning feeds knowledge infants may already native language infants preferred listen words part words whereas significant difference nonsense frame condition finding suggests even pre linguistic infants able integrate statistical cues learn laboratory previously acquired knowledge language words infants acquired linguistic knowledge incorporate newly acquired information previously acquired learning related finding indicates slightly older infants acquire lexical grammatical regularities single set input suggesting able use outputs one type statistical learning input second type structure learning grammatical regularities requires infants able determine boundaries individual words indicates infants still quite young able acquire multiple levels language knowledge simultaneously indicating statistical learning powerful mechanism play language learning despite large role statistical learning appears play lexical acquisition likely mechanism infants learn segment words statistical learning studies generally conducted artificial grammars cues word boundary information transitional probabilities words real speech though many different types cues word boundaries including prosodic phonotactic information together findings studies statistical learning language acquisition indicate statistical properties language strong cue helping infants learn first language much evidence statistical learning important component discovering phonemes important given language contrasts within phonemes important knowledge important aspects speech perception speech production since discovery infants statistical learning abilities word learning general mechanism also studied facets language learning example well established infants discriminate phonemes many different languages eventually become unable discriminate phonemes appear native language however clear decrease discriminatory ability came maye et al suggested mechanism responsible might statistical learning mechanism infants track distributional regularities sounds native language test idea maye et al exposed month old infants continuum speech sounds varied degree voiced distribution infants heard either bimodal sounds ends voicing continuum heard often unimodal sounds middle distribution heard often results indicated infants age groups sensitive distribution phonemes test infants heard either non alternating alternating exposures specific phonemes continuum infants exposed bimodal distribution listened longer alternating trials non alternating trials difference listening times infants exposed unimodal distribution finding indicates infants exposed bimodal distribution better able discriminate sounds two ends distribution infants unimodal condition regardless age type statistical learning differs used lexical acquisition requires infants track frequencies rather transitional probabilities named distributional learning distributional learning also found help infants contrast two phonemes initially difficulty discriminating maye weiss aslin found infants exposed bimodal distribution non native contrast initially difficult discriminate better able discriminate contrast infants exposed unimodal distribution contrast maye et al also found infants able abstract features contrast generalize feature type contrast different place articulation finding found adults review role distributional learning phonological acquisition werker et al note distributional learning cannot mechanism phonetic categories acquired however seem clear type statistical learning mechanism play role skill although research ongoing related finding regarding statistical cues phonological acquisition phenomenon known perceptual magnet effect effect prototypical phoneme person native language acts magnet similar phonemes perceived belonging category prototypical phoneme original test effect adult participants asked indicate given exemplar particular phoneme differed referent phoneme referent phoneme non prototypical phoneme language adults month old infants show less generalization sounds prototypical phonemes even subjective distance sounds adults infants likely notice particular phoneme differs referent phoneme referent phoneme non prototypical exemplar prototypical exemplar prototypes apparently discovered distributional learning process infants sensitive frequencies certain sounds occur treat occur often prototypical phonemes language statistical learning device also proposed component syntactical acquisition young children early evidence mechanism came largely studies computer modeling analyses natural language corpora early studies focused largely distributional information specifically rather statistical learning mechanisms generally specifically early papers proposed children created templates possible sentence structures involving unnamed categories word types children thought learn words belonged categories tracking similar contexts words category appeared later studies expanded results looking actual behavior children adults exposed artificial grammars later studies also considered role statistical learning broadly earlier studies placing results context statistical learning mechanisms thought involved aspects language learning lexical acquisition evidence series four experiments conducted gomez gerken suggests children able generalize grammatical structures less two minutes exposure artificial grammar first experiment month old infants trained artificial grammar composed nonsense words set grammatical structure test infants heard novel grammatical ungrammatical sentences infants oriented longer towards grammatical sentences line previous research suggests infants generally orient longer amount time natural instances language rather altered instances language e g finding indicates young children sensitive grammatical structure language even minimal exposure gomez gerken also found sensitivity evident ungrammatical transitions located middle sentence results could due innate preference grammatical sentences caused something grammar children able generalize grammatical rules new vocabulary together studies suggest infants able extract substantial amount syntactic knowledge even limited exposure language children apparently detected grammatical anomalies whether grammatical violation test sentences occurred end middle sentence additionally even individual words grammar changed infants still able discriminate grammatical ungrammatical strings test phase generalization indicates infants learning vocabulary specific grammatical structures abstracting general rules grammar applying rules novel vocabulary furthermore four experiments test grammatical structures occurred five minutes initial exposure artificial grammar ended suggesting infants able maintain grammatical abstractions learned even short delay similar study saffran found adults older children also sensitive syntactical information exposure artificial language cues phrase structure statistical regularities present adults children able pick sentences ungrammatical rate greater chance even incidental exposure condition participants primary goal complete different task hearing language although number studies dealing statistical learning syntactical information limited available evidence indicate statistical learning mechanisms likely contributing factor children ability learn language much early work using statistical learning paradigms focused ability children adults learn single language consistent process language acquisition monolingual speakers learners however estimated approximately people world bilingual recently researchers begun looking role statistical learning speak one language although reviews topic yet weiss gerfen mitchel examined hearing input multiple artificial languages simultaneously affect ability learn either languages four experiments weiss et al found exposure two artificial languages adult learners capable determining word boundaries languages language spoken different speaker however two languages spoken speaker participants able learn languages congruent word boundaries one language matched word boundaries languages incongruent syllable appeared middle word one language appeared end word language spoken single speaker participants able learn best one two languages final experiment showed inability learn incongruent languages spoken voice due syllable overlap languages due differing word boundaries similar work replicates finding learners able learn two sets statistical representations additional cue present paradigm two languages presented consecutively rather interleaved weiss et al paradigm participants learn first artificial language exposed better second although participants performance chance languages statistical learning improves strengthens multilingualism appears inverse true study yim rudoy found monolingual bilingual children perform statistical learning tasks equally well antovich graf estes found month old bilingual children better monolinguals segmenting two different artificial languages using transitional probability cues suggest bilingual environment early childhood trains children rely statistical regularities segment speech flow access two lexical systems statistical learning mechanism also proposed learning meaning words specifically yu smith conducted pair studies adults exposed pictures objects heard nonsense words nonsense word paired particular object total word referent pairs participant presented either objects time depending condition heard nonsense word associated one objects word referent pair presented times course training trials completion training trials participants completed forced alternative test asked choose correct referent matched nonsense word given participants able choose correct item often would happen chance indicating according authors using statistical learning mechanisms track co occurrence probabilities across training trials alternative hypothesis learners type task may using propose verify mechanism rather statistical learning mechanism medina et al trueswell et al argue yu smith tracked knowledge end training rather tracking knowledge trial trial basis impossible know participants truly updating statistical probabilities co occurrence instead forming single hypothesis checking next trial example participant presented picture dog picture shoe hears nonsense word vash might hypothesize vash refers dog future trial may see picture shoe picture door hear word vash statistical learning mechanism word referent mappings learned participant would likely select picture shoe door shoe would appeared conjunction word vash time however participants simply forming single hypothesis may fail remember context previous presentation vash therefore chance second trial according proposed mechanism word learning participant correctly guessed vash referred shoe first trial hypothesis would confirmed subsequent trial distinguish two possibilities trueswell et al conducted series experiments similar conducted yu smith except participants asked indicate choice word referent mapping trial single object name presented trial participants would therefore chance forced make choice first trial results subsequent trials indicate participants using statistical learning mechanism experiments instead using propose verify mechanism holding one potential hypothesis mind time specifically participants chosen incorrect word referent mapping initial presentation nonsense word likelihood choosing correct word referent mapping next trial word still chance though participant chosen correct word referent mapping initial presentation nonsense word likelihood choosing correct word referent mapping subsequent presentation word approximately results also replicated condition participants choosing two alternatives results suggest participants remember surrounding context individual presentations therefore using statistical cues determine word referent mappings instead participants make hypothesis regarding word referent mapping next presentation word either confirm reject hypothesis accordingly overall results along similar results medina et al indicate word meanings may learned statistical learning mechanism experiments ask participants hypothesize mapping even first occurrence however propose verify mechanism compared statistical learning mechanism former unable reproduce individual learning trajectories fit well latter additionally statistical learning cannot account even aspects language acquisition shown play large role example kuhl tsao liu found young english learning infants spent time laboratory session native mandarin speaker able distinguish phonemes occur mandarin english unlike infants control condition infants control condition came lab often infants experimental condition exposed english tested later date unable distinguish mandarin phonemes second experiment authors presented infants audio audiovisual recordings mandarin speakers tested infants ability distinguish mandarin phonemes condition infants failed distinguish foreign language phonemes finding indicates social interaction necessary component language learning even infants presented raw data hearing language unable take advantage statistical cues present data also experiencing social interaction although phenomenon statistical learning first discovered context language acquisition much evidence role purpose work since original discovery suggested statistical learning may domain general skill likely unique humans example saffran johnson aslin newport found adults infants able learn statistical probabilities words created playing different musical tones non auditory domains evidence humans able learn statistical visual information whether information presented across space e g time e g evidence statistical learning also found primates e g limited statistical learning abilities found even non primates like rats together findings suggest statistical learning may generalized learning mechanism happens utilized language acquisition rather mechanism unique human infant ability learn language evidence domain general statistical learning suggested study run university cornell department psychology concerning visual statistical learning infancy researchers study questioned whether domain generality statistical learning infancy would seen using visual information first viewing images statistically predictable patterns infants exposed familiar patterns addition novel sequences identical stimulus components interest visuals measured amount time child looked stimuli researchers named looking time ages infant participants showed interest novel sequence relative familiar sequence demonstrating preference novel sequences results study support likelihood domain general statistical learning infancy
https://en.wikipedia.org/wiki/Data_mining,data mining process discovering patterns large data sets involving methods intersection machine learning statistics database systems data mining interdisciplinary subfield computer science statistics overall goal extract information data set transform information comprehensible structure use data mining analysis step knowledge discovery databases process kdd aside raw analysis step also involves database data management aspects data pre processing model inference considerations interestingness metrics complexity considerations post processing discovered structures visualization online updating term data mining misnomer goal extraction patterns knowledge large amounts data extraction data also buzzword frequently applied form large scale data information processing well application computer decision support system including artificial intelligence business intelligence book data mining practical machine learning tools techniques java originally named practical machine learning term data mining added marketing reasons often general terms data analysis analytics referring actual methods artificial intelligence machine learning appropriate actual data mining task semi automatic automatic analysis large quantities data extract previously unknown interesting patterns groups data records unusual records dependencies usually involves using database techniques spatial indices patterns seen kind summary input data may used analysis example machine learning predictive analytics example data mining step might identify multiple groups data used obtain accurate prediction results decision support system neither data collection data preparation result interpretation reporting part data mining step belong overall kdd process additional steps difference data analysis data mining data analysis used test models hypotheses dataset e g analyzing effectiveness marketing campaign regardless amount data contrast data mining uses machine learning statistical models uncover clandestine hidden patterns large volume data related terms data dredging data fishing data snooping refer use data mining methods sample parts larger population data set small reliable statistical inferences made validity patterns discovered methods however used creating new hypotheses test larger data populations statisticians economists used terms like data fishing data dredging refer considered bad practice analyzing data without priori hypothesis term data mining used similarly critical way economist michael lovell article published review economic studies lovell indicates practice masquerades variety aliases ranging experimentation fishing snooping term data mining appeared around database community generally positive connotations short time phrase database mining used since trademarked hnc san diego based company pitch database mining workstation researchers consequently turned data mining terms used include data archaeology information harvesting information discovery knowledge extraction etc gregory piatetsky shapiro coined term knowledge discovery databases first workshop topic term became popular ai machine learning community however term data mining became popular business press communities currently terms data mining knowledge discovery used interchangeably academic community major forums research started first international conference data mining knowledge discovery started montreal aaai sponsorship co chaired usama fayyad ramasamy uthurusamy year later usama fayyad launched journal kluwer called data mining knowledge discovery founding editor chief later started sigkdd newsletter sigkdd explorations kdd international conference became primary highest quality conference data mining acceptance rate research paper submissions journal data mining knowledge discovery primary research journal field manual extraction patterns data occurred centuries early methods identifying patterns data include bayes theorem regression analysis proliferation ubiquity increasing power computer technology dramatically increased data collection storage manipulation ability data sets grown size complexity direct hands data analysis increasingly augmented indirect automated data processing aided discoveries computer science specially field machine learning neural networks cluster analysis genetic algorithms decision trees decision rules support vector machines data mining process applying methods intention uncovering hidden patterns large data sets bridges gap applied statistics artificial intelligence database management exploiting way data stored indexed databases execute actual learning discovery algorithms efficiently allowing methods applied ever larger data sets knowledge discovery databases process commonly defined stages exists however many variations theme cross industry standard process data mining defines six phases simplified process pre processing data mining results validation polls conducted show crisp dm methodology leading methodology used data miners data mining standard named polls semma however times many people reported using crisp dm several teams researchers published reviews data mining process models azevedo santos conducted comparison crisp dm semma data mining algorithms used target data set must assembled data mining uncover patterns actually present data target data set must large enough contain patterns remaining concise enough mined within acceptable time limit common source data data mart data warehouse pre processing essential analyze multivariate data sets data mining target set cleaned data cleaning removes observations containing noise missing data data mining involves six common classes tasks data mining unintentionally misused produce results appear significant actually predict future behavior cannot reproduced new sample data bear little use often results investigating many hypotheses performing proper statistical hypothesis testing simple version problem machine learning known overfitting problem arise different phases process thus train test split applicable may sufficient prevent happening final step knowledge discovery data verify patterns produced data mining algorithms occur wider data set patterns found data mining algorithms necessarily valid common data mining algorithms find patterns training set present general data set called overfitting overcome evaluation uses test set data data mining algorithm trained learned patterns applied test set resulting output compared desired output example data mining algorithm trying distinguish spam legitimate emails would trained training set sample e mails trained learned patterns would applied test set e mails trained accuracy patterns measured many e mails correctly classify several statistical methods may used evaluate algorithm roc curves learned patterns meet desired standards subsequently necessary evaluate change pre processing data mining steps learned patterns meet desired standards final step interpret learned patterns turn knowledge premier professional body field association computing machinery special interest group knowledge discovery data mining since acm sig hosted annual international conference published proceedings since published biannual academic journal titled sigkdd explorations computer science conferences data mining include data mining topics also present many data management database conferences icde conference sigmod conference international conference large data bases efforts define standards data mining process example european cross industry standard process data mining java data mining standard development successors processes active stalled since jdm withdrawn without reaching final draft exchanging extracted models particular use predictive analytics key standard predictive model markup language xml based language developed data mining group supported exchange format many data mining applications name suggests covers prediction models particular data mining task high importance business applications however extensions cover subspace clustering proposed independently dmg data mining used wherever digital data available today notable examples data mining found throughout business medicine science surveillance term data mining may ethical implications often associated mining information relation peoples behavior ways data mining used cases contexts raise questions regarding privacy legality ethics particular data mining government commercial data sets national security law enforcement purposes total information awareness program advise raised privacy concerns data mining requires data preparation uncovers information patterns compromise confidentiality privacy obligations common way occur data aggregation data aggregation involves combining data together way facilitates analysis data mining per se result preparation data purposes analysis threat individual privacy comes play data compiled cause data miner anyone access newly compiled data set able identify specific individuals especially data originally anonymous recommended according aware following data collected data may also modified become anonymous individuals may readily identified however even anonymized data sets potentially contain enough information allow identification individuals occurred journalists able find several individuals based set search histories inadvertently released aol inadvertent revelation personally identifiable information leading provider violates fair information practices indiscretion cause financial emotional bodily harm indicated individual one instance privacy violation patrons walgreens filed lawsuit company selling prescription information data mining companies turn provided data pharmaceutical companies europe rather strong privacy laws efforts underway strengthen rights consumers however u e u safe harbor principles developed currently effectively expose european users privacy exploitation u companies consequence edward snowden global surveillance disclosure increased discussion revoke agreement particular data fully exposed national security agency attempts reach agreement united states failed united states privacy concerns addressed us congress via passage regulatory controls health insurance portability accountability act hipaa requires individuals give informed consent regarding information provide intended present future uses according article biotech business week n practice hipaa may offer greater protection longstanding regulations research arena says aahc importantly rule goal protection informed consent approach level incomprehensibility average individuals underscores necessity data anonymity data aggregation mining practices u information privacy legislation hipaa family educational rights privacy act applies specific areas law addresses use data mining majority businesses u controlled legislation european copyright database laws mining copyright works without permission copyright owner legal database pure data europe may copyright database rights may exist data mining becomes subject intellectual property owners rights protected database directive recommendation hargreaves review led uk government amend copyright law allow content mining limitation exception uk second country world japan introduced exception data mining however due restriction information society directive uk exception allows content mining non commercial purposes uk copyright law also allow provision overridden contractual terms conditions european commission facilitated stakeholder discussion text data mining title licences europe focus solution legal issue licensing rather limitations exceptions led representatives universities researchers libraries civil society groups open access publishers leave stakeholder dialogue may us copyright law particular provision fair use upholds legality content mining america fair use countries israel taiwan south korea content mining transformative supplant original work viewed lawful fair use example part google book settlement presiding judge case ruled google digitization project copyright books lawful part transformative uses digitization project displayed one text data mining following applications available free open source licenses public access application source code also available following applications available proprietary licenses information extracting information data see
https://en.wikipedia.org/wiki/Statistical_classification,statistics classification problem identifying set categories new observation belongs basis training set data containing observations whose category membership known examples assigning given email spam non spam class assigning diagnosis given patient based observed characteristics patient classification example pattern recognition terminology machine learning classification considered instance supervised learning e learning training set correctly identified observations available corresponding unsupervised procedure known clustering involves grouping data categories based measure inherent similarity distance often individual observations analyzed set quantifiable properties known variously explanatory variables features properties may variously categorical ordinal integer valued real valued classifiers work comparing observations previous observations means similarity distance function algorithm implements classification especially concrete implementation known classifier term classifier sometimes also refers mathematical function implemented classification algorithm maps input data category terminology across fields quite varied statistics classification often done logistic regression similar procedure properties observations termed explanatory variables categories predicted known outcomes considered possible values dependent variable machine learning observations often known instances explanatory variables termed features possible categories predicted classes fields may use different terminology e g community ecology term classification normally refers cluster analysis e type unsupervised learning rather supervised learning described article classification clustering examples general problem pattern recognition assignment sort output value given input value examples regression assigns real valued output input sequence labeling assigns class member sequence values parsing assigns parse tree input sentence describing syntactic structure sentence etc common subclass classification probabilistic classification algorithms nature use statistical inference find best class given instance unlike algorithms simply output best class probabilistic algorithms output probability instance member possible classes best class normally selected one highest probability however algorithm numerous advantages non probabilistic classifiers early work statistical classification undertaken fisher context two group problems leading fisher linear discriminant function rule assigning group new observation early work assumed data values within two groups multivariate normal distribution extension context two groups also considered restriction imposed classification rule linear later work multivariate normal distribution allowed classifier nonlinear several classification rules derived based different adjustments mahalanobis distance new observation assigned group whose centre lowest adjusted distance observation unlike frequentist procedures bayesian classification procedures provide natural way taking account available information relative sizes different groups within overall population bayesian procedures tend computationally expensive days markov chain monte carlo computations developed approximations bayesian clustering rules devised bayesian procedures involve calculation group membership probabilities viewed providing informative outcome data analysis simple attribution single group label new observation classification thought two separate problems binary classification multiclass classification binary classification better understood task two classes involved whereas multiclass classification involves assigning object one several classes since many classification methods developed specifically binary classification multiclass classification often requires combined use multiple binary classifiers algorithms describe individual instance whose category predicted using feature vector individual measurable properties instance property termed feature also known statistics explanatory variable features may variously binary categorical ordinal integer valued real valued instance image feature values might correspond pixels image instance piece text feature values might occurrence frequencies different words algorithms work terms discrete data require real valued integer valued data discretized groups large number algorithms classification phrased terms linear function assigns score possible category k combining feature vector instance vector weights using dot product predicted category one highest score type score function known linear predictor function following general form xi feature vector instance k vector weights corresponding category k score score associated assigning instance category k discrete choice theory instances represent people categories represent choices score considered utility associated person choosing category k algorithms basic setup known linear classifiers distinguishes procedure determining optimal weights coefficients way score interpreted examples algorithms unsupervised learning classifiers form backbone cluster analysis supervised semi supervised learning classifiers system characterizes evaluates unlabeled data cases though classifiers specific set dynamic rules includes interpretation procedure handle vague unknown values tailored type inputs examined since single form classification appropriate data sets large toolkit classification algorithms developed commonly used include classifier performance depends greatly characteristics data classified single classifier works best given problems various empirical tests performed compare classifier performance find characteristics data determine classifier performance determining suitable classifier given problem however still art science measures precision recall popular metrics used evaluate quality classification system recently receiver operating characteristic curves used evaluate tradeoff true false positive rates classification algorithms performance metric uncertainty coefficient advantage simple accuracy affected relative sizes different classes penalize algorithm simply rearranging classes classification many applications employed data mining procedure others detailed statistical modeling undertaken
https://en.wikipedia.org/wiki/Cluster_analysis,cluster analysis clustering task grouping set objects way objects group similar groups main task exploratory data mining common technique statistical data analysis used many fields including pattern recognition image analysis information retrieval bioinformatics data compression computer graphics machine learning cluster analysis one specific algorithm general task solved achieved various algorithms differ significantly understanding constitutes cluster efficiently find popular notions clusters include groups small distances cluster members dense areas data space intervals particular statistical distributions clustering therefore formulated multi objective optimization problem appropriate clustering algorithm parameter settings depend individual data set intended use results cluster analysis automatic task iterative process knowledge discovery interactive multi objective optimization involves trial failure often necessary modify data preprocessing model parameters result achieves desired properties besides term clustering number terms similar meanings including automatic classification numerical taxonomy botryology typological analysis community detection subtle differences often use results data mining resulting groups matter interest automatic classification resulting discriminative power interest cluster analysis originated anthropology driver kroeber introduced psychology joseph zubin robert tryon famously used cattell beginning trait theory classification personality psychology notion cluster cannot precisely defined one reasons many clustering algorithms common denominator group data objects however different researchers employ different cluster models cluster models different algorithms given notion cluster found different algorithms varies significantly properties understanding cluster models key understanding differences various algorithms typical cluster models include clustering essentially set clusters usually containing objects data set additionally may specify relationship clusters example hierarchy clusters embedded clusterings roughly distinguished also finer distinctions possible example listed clustering algorithms categorized based cluster model following overview list prominent examples clustering algorithms possibly published clustering algorithms provide models clusters thus easily categorized overview algorithms explained wikipedia found list statistics algorithms objectively correct clustering algorithm noted clustering eye beholder appropriate clustering algorithm particular problem often needs chosen experimentally unless mathematical reason prefer one cluster model another algorithm designed one kind model generally fail data set contains radically different kind model example k means cannot find non convex clusters connectivity based clustering also known hierarchical clustering based core idea objects related nearby objects objects farther away algorithms connect objects form clusters based distance cluster described largely maximum distance needed connect parts cluster different distances different clusters form represented using dendrogram explains common name hierarchical clustering comes algorithms provide single partitioning data set instead provide extensive hierarchy clusters merge certain distances dendrogram axis marks distance clusters merge objects placed along x axis clusters mix connectivity based clustering whole family methods differ way distances computed apart usual choice distance functions user also needs decide linkage criterion use popular choices known single linkage clustering complete linkage clustering upgma wpgma furthermore hierarchical clustering agglomerative divisive methods produce unique partitioning data set hierarchy user still needs choose appropriate clusters robust towards outliers either show additional clusters even cause clusters merge general case complexity displaystyle mathcal agglomerative clustering displaystyle mathcal divisive clustering makes slow large data sets special cases optimal efficient methods displaystyle mathcal known slink single linkage clink complete linkage clustering data mining community methods recognized theoretical foundation cluster analysis often considered obsolete citation needed however provide inspiration many later methods density based clustering single linkage gaussian data clusters biggest cluster starts fragmenting smaller parts still connected second largest due single link effect single linkage density based clusters clusters extracted contain single elements since linkage clustering notion noise centroid based clustering clusters represented central vector may necessarily member data set number clusters fixed k k means clustering gives formal definition optimization problem find k cluster centers assign objects nearest cluster center squared distances cluster minimized optimization problem known np hard thus common approach search approximate solutions particularly well known approximate method lloyd algorithm often referred k means algorithm however find local optimum commonly run multiple times different random initializations variations k means often include optimizations choosing best multiple runs also restricting centroids members data set choosing medians choosing initial centers less randomly allowing fuzzy cluster assignment k means type algorithms require number clusters k specified advance considered one biggest drawbacks algorithms furthermore algorithms prefer clusters approximately similar size always assign object nearest centroid often leads incorrectly cut borders clusters k means number interesting theoretical properties first partitions data space structure known voronoi diagram second conceptually close nearest neighbor classification popular machine learning third seen variation model based clustering lloyd algorithm variation expectation maximization algorithm model discussed k means separates data voronoi cells assumes equal sized clusters k means cannot represent density based clusters centroid based clustering problems k means k medoids special cases uncapacitated metric facility location problem canonical problem operations research computational geometry communities basic facility location problem task find best warehouse locations optimally service given set consumers one may view warehouses cluster centroids consumer locations data clustered makes possible apply well developed algorithmic solutions facility location literature presently considered centroid based clustering problem clustering model closely related statistics based distribution models clusters easily defined objects belonging likely distribution convenient property approach closely resembles way artificial data sets generated sampling random objects distribution theoretical foundation methods excellent suffer one key problem known overfitting unless constraints put model complexity complex model usually able explain data better makes choosing appropriate model complexity inherently difficult one prominent method known gaussian mixture models data set usually modeled fixed number gaussian distributions initialized randomly whose parameters iteratively optimized better fit data set converge local optimum multiple runs may produce different results order obtain hard clustering objects often assigned gaussian distribution likely belong soft clusterings necessary distribution based clustering produces complex models clusters capture correlation dependence attributes however algorithms put extra burden user many real data sets may concisely defined mathematical model gaussian distributed data em works well since uses gaussians modelling clusters density based clusters cannot modeled using gaussian distributions density based clustering clusters defined areas higher density remainder data set objects sparse areas required separate clusters usually considered noise border points popular density based clustering method dbscan contrast many newer methods features well defined cluster model called density reachability similar linkage based clustering based connecting points within certain distance thresholds however connects points satisfy density criterion original variant defined minimum number objects within radius cluster consists density connected objects plus objects within objects range another interesting property dbscan complexity fairly low requires linear number range queries database discover essentially results run therefore need run multiple times optics generalization dbscan removes need choose appropriate value range parameter displaystyle varepsilon produces hierarchical result related linkage clustering deli clu density link clustering combines ideas single linkage clustering optics eliminating displaystyle varepsilon parameter entirely offering performance improvements optics using r tree index key drawback dbscan optics expect kind density drop detect cluster borders data sets example overlapping gaussian distributions common use case artificial data cluster borders produced algorithms often look arbitrary cluster density decreases continuously data set consisting mixtures gaussians algorithms nearly always outperformed methods em clustering able precisely model kind data mean shift clustering approach object moved densest area vicinity based kernel density estimation eventually objects converge local maxima density similar k means clustering density attractors serve representatives data set mean shift detect arbitrary shaped clusters similar dbscan due expensive iterative procedure density estimation mean shift usually slower dbscan k means besides applicability mean shift algorithm multidimensional data hindered unsmooth behaviour kernel density estimate results fragmentation cluster tails density based clustering dbscan dbscan assumes clusters similar density may problems separating nearby clusters optics dbscan variant improving handling different densities clusters grid based technique used multi dimensional data set technique create grid structure comparison performed grids grid based technique fast low computational complexity two types grid based clustering methods sting clique steps involved grid based clustering algorithm recent years considerable effort put improving performance existing algorithms among clarans birch recent need process larger larger data sets willingness trade semantic meaning generated clusters performance increasing led development pre clustering methods canopy clustering process huge data sets efficiently resulting clusters merely rough pre partitioning data set analyze partitions existing slower methods k means clustering high dimensional data many existing methods fail due curse dimensionality renders particular distance functions problematic high dimensional spaces led new clustering algorithms high dimensional data focus subspace clustering correlation clustering also looks arbitrary rotated subspace clusters modeled giving correlation attributes examples clustering algorithms clique subclu ideas density based clustering methods adapted subspace clustering correlation clustering several different clustering systems based mutual information proposed one marina meil variation information metric another provides hierarchical clustering using genetic algorithms wide range different fit functions optimized including mutual information also belief propagation recent development computer science statistical physics led creation new types clustering algorithms evaluation clustering results difficult clustering popular approaches involve internal evaluation clustering summarized single quality score external evaluation clustering compared existing ground truth classification manual evaluation human expert indirect evaluation evaluating utility clustering intended application internal evaluation measures suffer problem represent functions seen clustering objective example one could cluster data set silhouette coefficient except known efficient algorithm using internal measure evaluation one rather compares similarity optimization problems necessarily useful clustering external evaluation similar problems ground truth labels would need cluster practical applications usually labels hand labels reflect one possible partitioning data set imply exist different maybe even better clustering neither approaches therefore ultimately judge actual quality clustering needs human evaluation highly subjective nevertheless statistics quite informative identifying bad clusterings one dismiss subjective human evaluation clustering result evaluated based data clustered called internal evaluation methods usually assign best score algorithm produces clusters high similarity within cluster low similarity clusters one drawback using internal criteria cluster evaluation high scores internal measure necessarily result effective information retrieval applications additionally evaluation biased towards algorithms use cluster model example k means clustering naturally optimizes object distances distance based internal criterion likely overrate resulting clustering therefore internal evaluation measures best suited get insight situations one algorithm performs better another shall imply one algorithm produces valid results another validity measured index depends claim kind structure exists data set algorithm designed kind models chance data set contains radically different set models evaluation measures radically different criterion example k means clustering find convex clusters many evaluation indexes assume convex clusters data set non convex clusters neither use k means evaluation criterion assumes convexity sound dozen internal evaluation measures exist usually based intuition items cluster similar items different clusters example following methods used assess quality clustering algorithms based internal criterion external evaluation clustering results evaluated based data used clustering known class labels external benchmarks benchmarks consist set pre classified items sets often created humans thus benchmark sets thought gold standard evaluation types evaluation methods measure close clustering predetermined benchmark classes however recently discussed whether adequate real data synthetic data sets factual ground truth since classes contain internal structure attributes present may allow separation clusters classes may contain anomalies additionally knowledge discovery point view reproduction known knowledge may necessarily intended result special scenario constrained clustering meta information used already clustering process hold information evaluation purposes non trivial number measures adapted variants used evaluate classification tasks place counting number times class correctly assigned single data point pair counting metrics assess whether pair data points truly cluster predicted cluster internal evaluation several external evaluation measures exist example one issue rand index false positives false negatives equally weighted may undesirable characteristic clustering applications f measure addresses concern citation needed chance corrected adjusted rand index measure cluster tendency measure degree clusters exist data clustered may performed initial test attempting clustering one way compare data random data average random data clusters
https://en.wikipedia.org/wiki/Regression_analysis,statistical modeling regression analysis set statistical processes estimating relationships dependent variable one independent variables common form regression analysis linear regression researcher finds line closely fits data according specific mathematical criterion example method ordinary least squares computes unique line minimizes sum squared distances true data line specific mathematical reasons allows researcher estimate conditional expectation dependent variable independent variables take given set values less common forms regression use slightly different procedures estimate alternative location parameters estimate conditional expectation across broader collection non linear models regression analysis primarily used two conceptually distinct purposes first regression analysis widely used prediction forecasting use substantial overlap field machine learning second situations regression analysis used infer causal relationships independent dependent variables importantly regressions reveal relationships dependent variable collection independent variables fixed dataset use regressions prediction infer causal relationships respectively researcher must carefully justify existing relationships predictive power new context relationship two variables causal interpretation latter especially important researchers hope estimate causal relationships using observational data earliest form regression method least squares published legendre gauss legendre gauss applied method problem determining astronomical observations orbits bodies sun gauss published development theory least squares including version gauss markov theorem term regression coined francis galton nineteenth century describe biological phenomenon phenomenon heights descendants tall ancestors tend regress towards normal average galton regression biological meaning work later extended udny yule karl pearson general statistical context work yule pearson joint distribution response explanatory variables assumed gaussian assumption weakened r fisher works fisher assumed conditional distribution response variable gaussian joint distribution need respect fisher assumption closer gauss formulation economists used electromechanical desk calculators calculate regressions sometimes took hours receive result one regression regression methods continue area active research recent decades new methods developed robust regression regression involving correlated responses time series growth curves regression predictor response variables curves images graphs complex data objects regression methods accommodating various types missing data nonparametric regression bayesian methods regression regression predictor variables measured error regression predictor variables observations causal inference regression practice researchers first select model would like estimate use chosen method estimate parameters model regression models involve following components various fields application different terminologies used place dependent independent variables regression models propose displaystyle function x displaystyle x displaystyle beta e displaystyle e representing additive error term may stand un modeled determinants displaystyle random statistical noise researchers goal estimate function f displaystyle f closely fits data carry regression analysis form function f displaystyle f must specified sometimes form function based knowledge relationship displaystyle x displaystyle x rely data knowledge available flexible convenient form f displaystyle f chosen example simple univariate regression may propose f x displaystyle f beta beta x suggesting researcher believes x e displaystyle beta beta x e reasonable approximation statistical process generating data researchers determine preferred statistical model different forms regression analysis provide tools estimate parameters displaystyle beta example least squares finds value displaystyle beta minimizes sum squared errors displaystyle sum given regression method ultimately provide estimate displaystyle beta usually denoted displaystyle hat beta distinguish estimate true parameter value generated data using estimate researcher use fitted value f displaystyle hat f prediction assess accuracy model explaining data whether researcher intrinsically interested estimate displaystyle hat beta predicted value displaystyle hat depend context goals described ordinary least squares least squares widely used estimated function f displaystyle f approximates conditional expectation e displaystyle e however alternative variants useful researchers want model functions f displaystyle f important note must sufficient data estimate regression model example suppose researcher access n displaystyle n rows data one dependent two independent variables displaystyle suppose researcher wants estimate bivariate linear model via least squares x x e displaystyle beta beta x beta x e researcher access n displaystyle n data points could find infinitely many combinations displaystyle explain data equally well combination chosen satisfies x x displaystyle hat hat beta hat beta x hat beta x lead e displaystyle sum hat e sum therefore valid solutions minimize sum squared residuals understand infinitely many options note system n displaystyle n equations solved unknowns makes system underdetermined alternatively one visualize infinitely many dimensional planes go n displaystyle n fixed points generally estimate least squares model k displaystyle k distinct parameters one must n k displaystyle n geq k distinct data points n k displaystyle n k generally exist set parameters perfectly fit data quantity n k displaystyle n k appears often regression analysis referred degrees freedom model moreover estimate least squares model independent variables displaystyle must linearly independent one must able reconstruct independent variables adding multiplying remaining independent variables discussed ordinary least squares condition ensures x x displaystyle x x invertible matrix therefore unique solution displaystyle hat beta exists regression simply calculation using data order interpret output regression meaningful statistical quantity measures real world relationships researchers often rely number classical assumptions often include handful conditions sufficient least squares estimator possess desirable properties particular gauss markov assumptions imply parameter estimates unbiased consistent efficient class linear unbiased estimators practitioners developed variety methods maintain desirable properties real world settings classical assumptions unlikely hold exactly example modeling errors variables lead reasonable estimates independent variables measured errors heteroscedasticity consistent standard errors allow variance e displaystyle e change across values x displaystyle x correlated errors exist within subsets data follow specific patterns handled using clustered standard errors geographic weighted regression newey west standard errors among techniques rows data correspond locations space choice model e displaystyle e within geographic units important consequences subfield econometrics largely focused developing techniques allow researchers make reasonable real world conclusions real world settings classical assumptions hold exactly linear regression model specification dependent variable displaystyle linear combination parameters example simple linear regression modeling n displaystyle n data points one independent variable x displaystyle x two parameters displaystyle beta displaystyle beta multiple linear regression several independent variables functions independent variables adding term x displaystyle x preceding regression gives still linear regression although expression right hand side quadratic independent variable x displaystyle x linear parameters displaystyle beta displaystyle beta displaystyle beta cases displaystyle varepsilon error term subscript displaystyle indexes particular observation returning attention straight line case given random sample population estimate population parameters obtain sample linear regression model residual e displaystyle e widehat difference value dependent variable predicted model displaystyle widehat true value dependent variable displaystyle one method estimation ordinary least squares method obtains parameter estimates minimize sum squared residuals ssr minimization function results set normal equations set simultaneous linear equations parameters solved yield parameter estimators displaystyle widehat beta widehat beta case simple regression formulas least squares estimates x displaystyle bar x mean x displaystyle x values displaystyle bar mean displaystyle values assumption population error term constant variance estimate variance given called mean square error regression denominator sample size reduced number model parameters estimated data displaystyle p displaystyle p regressors displaystyle intercept used case p displaystyle p denominator n displaystyle n standard errors parameter estimates given assumption population error term normally distributed researcher use estimated standard errors create confidence intervals conduct hypothesis tests population parameters general multiple regression model p displaystyle p independent variables x j displaystyle x ij displaystyle th observation j displaystyle j th independent variable first independent variable takes value displaystyle x displaystyle x displaystyle beta called regression intercept least squares parameter estimates obtained p displaystyle p normal equations residual written normal equations matrix notation normal equations written j displaystyle ij element x displaystyle mathbf x x j displaystyle x ij displaystyle element column vector displaystyle displaystyle j displaystyle j element displaystyle hat boldsymbol beta j displaystyle hat beta j thus x displaystyle mathbf x n p displaystyle n times p displaystyle n displaystyle n times displaystyle hat boldsymbol beta p displaystyle p times solution regression model constructed may important confirm goodness fit model statistical significance estimated parameters commonly used checks goodness fit include r squared analyses pattern residuals hypothesis testing statistical significance checked f test overall fit followed tests individual parameters interpretations diagnostic tests rest heavily model assumptions although examination residuals used invalidate model results test f test sometimes difficult interpret model assumptions violated example error term normal distribution small samples estimated parameters follow normal distributions complicate inference relatively large samples however central limit theorem invoked hypothesis testing may proceed using asymptotic approximations limited dependent variables response variables categorical variables variables constrained fall certain range often arise econometrics response variable may non continuous binary variables analysis proceeds least squares linear regression model called linear probability model nonlinear models binary dependent variables include probit logit model multivariate probit model standard method estimating joint relationship several binary dependent variables independent variables categorical variables two values multinomial logit ordinal variables two values ordered logit ordered probit models censored regression models may used dependent variable sometimes observed heckman correction type models may used sample randomly selected population interest alternative procedures linear regression based polychoric correlation categorical variables procedures differ assumptions made distribution variables population variable positive low values represents repetition occurrence event count models like poisson regression negative binomial model may used model function linear parameters sum squares must minimized iterative procedure introduces many complications summarized differences linear non linear least squares regression models predict value variable given known values x variables prediction within range values dataset used model fitting known informally interpolation prediction outside range data known extrapolation performing extrapolation relies strongly regression assumptions extrapolation goes outside data room model fail due differences assumptions sample data true values generally advised citation needed performing extrapolation one accompany estimated value dependent variable prediction interval represents uncertainty intervals tend expand rapidly values independent variable moved outside range covered observed data reasons others tend say might unwise undertake extrapolation however cover full set modeling errors may made particular assumption particular form relation x properly conducted regression analysis include assessment well assumed form matched observed data within range values independent variables actually available means extrapolation particularly reliant assumptions made structural form regression relationship best practice advice citation needed linear variables linear parameters relationship chosen simply computational convenience available knowledge deployed constructing regression model knowledge includes fact dependent variable cannot go outside certain range values made use selecting model even observed dataset values particularly near bounds implications step choosing appropriate functional form regression great extrapolation considered minimum ensure extrapolation arising fitted model realistic generally agreed methods relating number observations versus number independent variables model one rule thumb conjectured good hardin n n displaystyle n n n displaystyle n sample size n displaystyle n number independent variables displaystyle number observations needed reach desired precision model one independent variable example researcher building linear regression model using dataset contains patients researcher decides five observations needed precisely define straight line maximum number independent variables model support although parameters regression model usually estimated using method least squares methods used include major statistical software packages perform least squares regression analysis inference simple linear regression multiple regression using least squares done spreadsheet applications calculators many statistical software packages perform various types nonparametric robust regression methods less standardized different software packages implement different methods method given name may implemented differently different packages specialized regression software developed use fields survey analysis neuroimaging
https://en.wikipedia.org/wiki/Anomaly_detection,various domains limited statistics signal processing finance econometrics manufacturing networking data mining anomaly detection identification rare items events observations raise suspicions differing significantly majority data typically anomalous items translate kind problem bank fraud structural defect medical problems errors text anomalies also referred outliers novelties noise deviations exceptions particular context abuse network intrusion detection interesting objects often rare objects unexpected bursts activity pattern adhere common statistical definition outlier rare object many outlier detection methods fail data unless aggregated appropriately instead cluster analysis algorithm may able detect micro clusters formed patterns three broad categories anomaly detection techniques exist unsupervised anomaly detection techniques detect anomalies unlabeled test data set assumption majority instances data set normal looking instances seem fit least remainder data set supervised anomaly detection techniques require data set labeled normal abnormal involves training classifier semi supervised anomaly detection techniques construct model representing normal behavior given normal training data set test likelihood test instance generated learnt model anomaly detection applicable variety domains intrusion detection fraud detection fault detection system health monitoring event detection sensor networks detecting ecosystem disturbances often used preprocessing remove anomalous data dataset supervised learning removing anomalous data dataset often results statistically significant increase accuracy several anomaly detection techniques proposed literature popular techniques performance different methods depends lot data set parameters methods little systematic advantages another compared across many data sets parameters anomaly detection proposed intrusion detection systems dorothy denning anomaly detection ids normally accomplished thresholds statistics also done soft computing inductive learning types statistics proposed included profiles users workstations networks remote hosts groups users programs based frequencies means variances covariances standard deviations counterpart anomaly detection intrusion detection misuse detection
https://en.wikipedia.org/wiki/Automated_machine_learning,automated machine learning process automating process applying machine learning real world problems automl covers complete pipeline raw dataset deployable machine learning model automl proposed artificial intelligence based solution ever growing challenge applying machine learning high degree automation automl allows non experts make use machine learning models techniques without requiring becoming expert field first automating process applying machine learning end end additionally offers advantages producing simpler solutions faster creation solutions models often outperform hand designed models typical machine learning application practitioners set input data points train raw data may form algorithms applied make data amenable machine learning expert may apply appropriate data pre processing feature engineering feature extraction feature selection methods steps practitioners must perform algorithm selection hyperparameter optimization maximize predictive performance model steps induce challenges accumulating significant hurdle get started machine learning automl dramatically simplifies steps non experts automated machine learning target various stages machine learning process steps automate
https://en.wikipedia.org/wiki/Association_rule_learning,association rule learning rule based machine learning method discovering interesting relations variables large databases intended identify strong rules discovered databases using measures interestingness based concept strong rules rakesh agrawal tomasz imieli ski arun swami introduced association rules discovering regularities products large scale transaction data recorded point sale systems supermarkets example rule n n p e b u r g e r displaystyle mathrm onions potatoes rightarrow mathrm burger found sales data supermarket would indicate customer buys onions potatoes together likely also buy hamburger meat information used basis decisions marketing activities e g promotional pricing product placements addition example market basket analysis association rules employed today many application areas including web usage mining intrusion detection continuous production bioinformatics contrast sequence mining association rule learning typically consider order items either within transaction across transactions following original definition agrawal imieli ski swami problem association rule mining defined let n displaystyle ldots n set n displaystyle n binary attributes called items let displaystyle ldots set transactions called database transaction displaystyle unique transaction id contains subset items displaystyle rule defined implication form x displaystyle x rightarrow x displaystyle x subseteq agrawal imieli ski swami rule defined set single item x j displaystyle x rightarrow j j displaystyle j every rule composed two different sets items also known itemsets x displaystyle x displaystyle x displaystyle x called antecedent left hand side displaystyle consequent right hand side illustrate concepts use small example supermarket domain set items l k b r e b u e r b e e r p e r displaystyle mathrm milk bread butter beer diapers table shown small database containing items entry value means presence item corresponding transaction value represents absence item transaction example rule supermarket could b u e r b r e l k displaystyle mathrm butter bread rightarrow mathrm milk meaning butter bread bought customers also buy milk note example extremely small practical applications rule needs support several hundred transactions considered statistically significant citation needed datasets often contain thousands millions transactions order select interesting rules set possible rules constraints various measures significance interest used best known constraints minimum thresholds support confidence let x displaystyle x itemsets x displaystyle x rightarrow association rule displaystyle set transactions given database support indication frequently itemset appears dataset support x displaystyle x respect displaystyle defined proportion transactions displaystyle dataset contains itemset x displaystyle x u p p x displaystyle mathrm supp frac x subseteq example dataset itemset x b e e r p e r displaystyle x mathrm beer diapers support displaystyle since occurs transactions argument u p p displaystyle mathrm supp set preconditions thus becomes restrictive grows confidence indication often rule found true confidence value rule x displaystyle x rightarrow respect set transactions displaystyle proportion transactions contains x displaystyle x also contains displaystyle confidence defined c n f u p p u p p displaystyle mathrm conf mathrm supp mathrm supp example rule b u e r b r e l k displaystyle mathrm butter bread rightarrow mathrm milk confidence displaystyle database means transactions containing butter bread rule correct note u p p displaystyle mathrm supp means support union items x somewhat confusing since normally think terms probabilities events sets items rewrite u p p displaystyle mathrm supp probability p displaystyle p e x displaystyle e x e displaystyle e events transaction contains itemset x displaystyle x displaystyle respectively thus confidence interpreted estimate conditional probability p displaystyle p probability finding rhs rule transactions condition transactions also contain lhs lift rule defined l f u p p u p p u p p displaystyle mathrm lift frac mathrm supp mathrm supp times mathrm supp ratio observed support expected x independent example rule l k b r e b u e r displaystyle mathrm milk bread rightarrow mathrm butter lift displaystyle frac times rule lift would imply probability occurrence antecedent consequent independent two events independent rule drawn involving two events lift lets us know degree two occurrences dependent one another makes rules potentially useful predicting consequent future data sets lift lets us know items substitute means presence one item negative effect presence item vice versa value lift considers support rule overall data set conviction rule defined c n v u p p c n f displaystyle mathrm conv frac mathrm supp mathrm conf example rule l k b r e b u e r displaystyle mathrm milk bread rightarrow mathrm butter conviction displaystyle frac interpreted ratio expected frequency x occurs without x independent divided observed frequency incorrect predictions example conviction value shows rule l k b r e b u e r displaystyle mathrm milk bread rightarrow mathrm butter would incorrect often association x purely random chance addition confidence measures interestingness rules proposed popular measures several measures presented compared tan et al hahsler looking techniques model user known currently active research trend name subjective interestingness association rules usually required satisfy user specified minimum support user specified minimum confidence time association rule generation usually split two separate steps second step straightforward first step needs attention finding frequent itemsets database difficult since involves searching possible itemsets set possible itemsets power set displaystyle size n displaystyle n although size power set grows exponentially number items n displaystyle n displaystyle efficient search possible using downward closure property support guarantees frequent itemset subsets also frequent thus infrequent itemset subset frequent itemset exploiting property efficient algorithms find frequent itemsets concept association rules popularised particularly due article agrawal et al acquired citations according google scholar august thus one cited papers data mining field however called association rules introduced already paper guha general data mining method developed petr h jek et al early use minimum support confidence find association rules feature based modeling framework found rules u p p displaystyle mathrm supp c n f displaystyle mathrm conf greater user defined constraints one limitation standard approach discovering associations searching massive numbers possible associations look collections items appear associated large risk finding many spurious associations collections items co occur unexpected frequency data chance example suppose considering collection items looking rules containing two items left hand side item right hand side approximately rules apply statistical test independence significance level means chance accepting rule association assume associations nonetheless expect find rules statistically sound association discovery controls risk cases reducing risk finding spurious associations user specified significance level many algorithms generating association rules proposed well known algorithms apriori eclat fp growth half job since algorithms mining frequent itemsets another step needs done generate rules frequent itemsets found database apriori uses breadth first search strategy count support itemsets uses candidate generation function exploits downward closure property support eclat depth first search algorithm based set intersection suitable sequential well parallel execution locality enhancing properties fp stands frequent pattern first pass algorithm counts occurrences items dataset transactions stores counts header table second pass builds fp tree structure inserting transactions trie items transaction sorted descending order frequency dataset inserted tree processed quickly items transaction meet minimum support requirement discarded many transactions share frequent items fp tree provides high compression close tree root recursive processing compressed version main dataset grows frequent item sets directly instead generating candidate items testing entire database growth begins bottom header table e item smallest support finding sorted transactions end item call item displaystyle new conditional tree created original fp tree projected onto displaystyle supports nodes projected tree counted node getting sum children counts nodes meet minimum support pruned recursive growth ends individual items conditional displaystyle meet minimum support threshold resulting paths root displaystyle frequent itemsets step processing continues next least supported header item original fp tree recursive process completed frequent item sets found association rule creation begins assoc procedure guha method mines generalized association rules using fast bitstrings operations association rules mined method general output apriori example items connected conjunction disjunctions relation antecedent consequent rule restricted setting minimum support confidence apriori arbitrary combination supported interest measures used opus efficient algorithm rule discovery contrast alternatives require either monotone anti monotone constraints minimum support initially used find rules fixed consequent subsequently extended find rules item consequent opus search core technology popular magnum opus association discovery system famous story association rule mining beer diaper story purported survey behavior supermarket shoppers discovered customers buy diapers tend also buy beer anecdote became popular example unexpected association rules might found everyday data varying opinions much story true daniel powers says thomas blischok manager retail consulting group teradata staff prepared analysis million market baskets osco drug stores database queries developed identify affinities analysis discover p consumers bought beer diapers osco managers exploit beer diapers relationship moving products closer together shelves multi relation association rules multi relation association rules association rules item may several relations relations indicate indirect relationship entities consider following mrar first item consists three relations live nearby humid live place nearby city humid climate type also younger health condition good association rules extractable rdbms data semantic web data contrast set learning form associative learning contrast set learners use rules differ meaningfully distribution across subsets weighted class learning another form associative learning weight may assigned classes give focus particular issue concern consumer data mining results high order pattern discovery facilitate capture high order patterns event associations intrinsic complex real world data k optimal pattern discovery provides alternative standard approach association rule learning requires pattern appear frequently data approximate frequent itemset mining relaxed version frequent itemset mining allows items rows generalized association rules hierarchical taxonomy quantitative association rules categorical quantitative data interval data association rules e g partition age year increment ranged sequential pattern mining discovers subsequences common minsup clarification needed sequences sequence database minsup set user sequence ordered list transactions subspace clustering specific type clustering high dimensional data many variants also based downward closure property specific clustering models warmr shipped part ace data mining suite allows association rule learning first order relational rules
https://en.wikipedia.org/wiki/Reinforcement_learning,reinforcement learning area machine learning concerned software agents ought take actions environment order maximize notion cumulative reward reinforcement learning one three basic machine learning paradigms alongside supervised learning unsupervised learning reinforcement learning differs supervised learning needing labelled input output pairs presented needing sub optimal actions explicitly corrected instead focus finding balance exploration exploitation environment typically stated form markov decision process many reinforcement learning algorithms context utilize dynamic programming techniques main difference classical dynamic programming methods reinforcement learning algorithms latter assume knowledge exact mathematical model mdp target large mdps exact methods become infeasible mw parser output toclimit toclevel ul mw parser output toclimit toclevel ul mw parser output toclimit toclevel ul mw parser output toclimit toclevel ul mw parser output toclimit toclevel ul mw parser output toclimit toclevel ul display none due generality reinforcement learning studied many disciplines game theory control theory operations research information theory simulation based optimization multi agent systems swarm intelligence statistics operations research control literature reinforcement learning called approximate dynamic programming neuro dynamic programming problems interest reinforcement learning also studied theory optimal control concerned mostly existence characterization optimal solutions algorithms exact computation less learning approximation particularly absence mathematical model environment economics game theory reinforcement learning may used explain equilibrium may arise bounded rationality basic reinforcement modeled markov decision process reinforcement learning agent interacts environment discrete time steps time agent receives current state displaystyle reward r displaystyle r chooses action displaystyle set available actions subsequently sent environment environment moves new state displaystyle reward r displaystyle r associated transition displaystyle determined goal reinforcement learning agent learn policy displaystyle pi times rightarrow pr displaystyle pi pr maximizes expected cumulative reward formulating problem mdp assumes agent directly observes current environmental state case problem said full observability agent access subset states observed states corrupted noise agent said partial observability formally problem must formulated partially observable markov decision process cases set actions available agent restricted example state account balance could restricted positive current value state state transition attempts reduce value transition allowed agent performance compared agent acts optimally difference performance gives rise notion regret order act near optimally agent must reason long term consequences actions although immediate reward associated might negative thus reinforcement learning particularly well suited problems include long term versus short term reward trade applied successfully various problems including robot control elevator scheduling telecommunications backgammon checkers go two elements make reinforcement learning powerful use samples optimize performance use function approximation deal large environments thanks two key components reinforcement learning used large environments following situations first two problems could considered planning problems last one could considered genuine learning problem however reinforcement learning converts planning problems machine learning problems exploration vs exploitation trade thoroughly studied multi armed bandit problem finite state space mdps burnetas katehakis reinforcement learning requires clever exploration mechanisms randomly selecting actions without reference estimated probability distribution shows poor performance case finite markov decision processes relatively well understood however due lack algorithms scale well number states simple exploration methods practical one method displaystyle varepsilon greedy displaystyle
https://en.wikipedia.org/wiki/Structured_prediction,structured prediction structured learning umbrella term supervised machine learning techniques involves predicting structured objects rather scalar discrete real values similar commonly used supervised learning techniques structured prediction models typically trained means observed data true prediction value used adjust model parameters due complexity model interrelations predicted variables process prediction using trained model training often computationally infeasible approximate inference learning methods used example problem translating natural language sentence syntactic representation parse tree seen structured prediction problem structured output domain set possible parse trees structured prediction also used wide variety application domains including bioinformatics natural language processing speech recognition computer vision sequence tagging class problems prevalent natural language processing input data often sequences sequence tagging problem appears several guises e g part speech tagging named entity recognition pos tagging example word sequence must receive tag expresses type word main challenge problem resolve ambiguity word sentence also verb english tagged problem solved simply performing classification individual tokens approach take account empirical fact tags occur independently instead tag displays strong conditional dependence tag previous word fact exploited sequence model hidden markov model conditional random field predicts entire tag sequence sentence rather individual tags means viterbi algorithm probabilistic graphical models form large class structured prediction models particular bayesian networks random fields popular algorithms models structured prediction include inductive logic programming case based reasoning structured svms markov logic networks constrained conditional models main techniques one easiest ways understand algorithms general structured prediction structured perceptron collins algorithm combines perceptron algorithm learning linear classifiers inference algorithm described abstractly follows first define joint feature function maps training sample x candidate prediction vector length n let gen function generates candidate predictions practice finding argmax g e n displaystyle gen done using algorithm viterbi algorithm max sum rather exhaustive search exponentially large set candidates idea learning similar multiclass perceptron
https://en.wikipedia.org/wiki/Feature_engineering,feature engineering process using domain knowledge extract features raw data via data mining techniques features used improve performance machine learning algorithms feature engineering considered applied machine learning feature attribute property shared independent units analysis prediction done attribute could feature long useful model purpose feature attribute would much easier understand context problem feature characteristic might help solving problem features important predictive models influence results asserted feature engineering plays important part kaggle competitions machine learning project success failure feature engineering process feature could strongly relevant relevant weakly relevant irrelevant even features irrelevant many better missing important feature selection used prevent overfitting feature explosion caused feature combination feature templates leading quick growth total number features feature explosion limited via techniques regularization kernel method feature selection automation feature engineering research topic dates back least late academic literature topic roughly separated two strings first multi relational decision tree learning uses supervised algorithm similar decision tree second recent approaches like deep feature synthesis use simpler methods citation needed multi relational decision tree learning generates features form sql queries successively adding new clauses queries instance algorithm might start query successively refined adding conditions charge however academic studies mrdtl use implementations based existing relational databases results many redundant operations redundancies reduced using tricks tuple id propagation recently demonstrated efficiency increased using incremental updates completely eliminates redundancies researchers mit presented deep feature synthesis algorithm demonstrated effectiveness online data science competitions beat human teams deep feature synthesis available open source library called featuretools work followed researchers including ibm onebm berkeley explorekit researchers ibm stated feature engineering automation helps data scientists reduce data exploration time allowing try error many ideas short time hand enables non experts familiar data science quickly extract value data little effort time cost citation needed
https://en.wikipedia.org/wiki/Feature_learning,machine learning feature learning representation learning set techniques allows system automatically discover representations needed feature detection classification raw data replaces manual feature engineering allows machine learn features use perform specific task feature learning motivated fact machine learning tasks classification often require input mathematically computationally convenient process however real world data images video sensor data yielded attempts algorithmically define specific features alternative discover features representations examination without relying explicit algorithms feature learning either supervised unsupervised supervised feature learning learning features labeled data data label allows system compute error term degree system fails produce label used feedback correct learning process approaches include dictionary learning develops set representative elements input data data point represented weighted sum representative elements dictionary elements weights may found minimizing average representation error together l regularization weights enable sparsity supervised dictionary learning exploits structure underlying input data labels optimizing dictionary elements example supervised dictionary learning technique applied dictionary learning classification problems jointly optimizing dictionary elements weights representing data points parameters classifier based input data particular minimization problem formulated objective function consists classification error representation error l regularization representing weights data point l regularization parameters classifier neural networks family learning algorithms use network consisting multiple layers inter connected nodes inspired animal nervous system nodes viewed neurons edges viewed synapses edge associated weight network defines computational rules passing input data network input layer output layer network function associated neural network characterizes relationship input output layers parameterized weights appropriately defined network functions various learning tasks performed minimizing cost function network function multilayer neural networks used perform feature learning since learn representation input hidden layer subsequently used classification regression output layer popular network architecture type siamese networks unsupervised feature learning learning features unlabeled data goal unsupervised feature learning often discover low dimensional features captures structure underlying high dimensional input data feature learning performed unsupervised way enables form semisupervised learning features learned unlabeled dataset employed improve performance supervised setting labeled data several approaches introduced following k means clustering approach vector quantization particular given set n vectors k means clustering groups k clusters way vector belongs cluster closest mean problem computationally np hard although suboptimal greedy algorithms developed k means clustering used group unlabeled set inputs k clusters use centroids clusters produce features features produced several ways simplest add k binary features sample feature j value one iff jth centroid learned k means closest sample consideration also possible use distances clusters features perhaps transforming radial basis function coates ng note certain variants k means behave similarly sparse coding algorithms comparative evaluation unsupervised feature learning methods coates lee ng found k means clustering appropriate transformation outperforms recently invented auto encoders rbms image classification task k means also improves performance domain nlp specifically named entity recognition competes brown clustering well distributed word representations principal component analysis often used dimension reduction given unlabeled set n input data vectors pca generates p right singular vectors corresponding p largest singular values data matrix kth row data matrix kth input data vector shifted sample mean input equivalently singular vectors eigenvectors corresponding p largest eigenvalues sample covariance matrix input vectors p singular vectors feature vectors learned input data represent directions along data largest variations pca linear feature learning approach since p singular vectors linear functions data matrix singular vectors generated via simple algorithm p iterations ith iteration projection data matrix th eigenvector subtracted ith singular vector found right singular vector corresponding largest singular residual data matrix pca several limitations first assumes directions large variance interest may case pca relies orthogonal transformations original data exploits first second order moments data may well characterize data distribution furthermore pca effectively reduce dimension input data vectors correlated local linear embedding nonlinear learning approach generating low dimensional neighbor preserving representations high dimension input approach proposed roweis saul general idea lle reconstruct original high dimensional data using lower dimensional points maintaining geometric properties neighborhoods original data set lle consists two major steps first step neighbor preserving input data point xi reconstructed weighted sum k nearest neighbor data points optimal weights found minimizing average squared reconstruction error constraint weights associated point sum one second step dimension reduction looking vectors lower dimensional space minimizes representation error using optimized weights first step note first step weights optimized fixed data solved least squares problem second step lower dimensional points optimized fixed weights solved via sparse eigenvalue decomposition reconstruction weights obtained first step capture intrinsic geometric properties neighborhood input data assumed original data lie smooth lower dimensional manifold intrinsic geometric properties captured weights original data also expected manifold weights used second step lle compared pca lle powerful exploiting underlying data structure independent component analysis technique forming data representation using weighted sum independent non gaussian components assumption non gaussian imposed since weights cannot uniquely determined components follow gaussian distribution unsupervised dictionary learning utilize data labels exploits structure underlying data optimizing dictionary elements example unsupervised dictionary learning sparse coding aims learn basis functions data representation unlabeled input data sparse coding applied learn overcomplete dictionaries number dictionary elements larger dimension input data aharon et al proposed algorithm k svd learning dictionary elements enables sparse representation hierarchical architecture biological neural system inspires deep learning architectures feature learning stacking multiple layers learning nodes architectures often designed based assumption distributed representation observed data generated interactions many different factors multiple levels deep learning architecture output intermediate layer viewed representation original input data level uses representation produced previous level input produces new representations output fed higher levels input bottom layer raw data output final layer final low dimensional feature representation restricted boltzmann machines often used building block multilayer learning architectures rbm represented undirected bipartite graph consisting group binary hidden variables group visible variables edges connecting hidden visible nodes special case general boltzmann machines constraint intra node connections edge rbm associated weight weights together connections define energy function based joint distribution visible hidden nodes devised based topology rbm hidden variables independent conditioned visible variables clarification needed conditional independence facilitates computations rbm viewed single layer architecture unsupervised feature learning particular visible variables correspond input data hidden variables correspond feature detectors weights trained maximizing probability visible variables using hinton contrastive divergence algorithm general training rbm solving maximization problem tends result non sparse representations sparse rbm proposed enable sparse representations idea add regularization term objective function data likelihood penalizes deviation expected hidden variables small constant p displaystyle p autoencoder consisting encoder decoder paradigm deep learning architectures example provided hinton salakhutdinov encoder uses raw data input produces feature representation output decoder uses extracted feature encoder input reconstructs original input raw data output encoder decoder constructed stacking multiple layers rbms parameters involved architecture originally trained greedy layer layer manner one layer feature detectors learned fed visible variables training corresponding rbm current approaches typically apply end end training stochastic gradient descent methods training repeated stopping criteria satisfied
https://en.wikipedia.org/wiki/Online_machine_learning,computer science online machine learning method machine learning data becomes available sequential order used update best predictor future data step opposed batch learning techniques generate best predictor learning entire training data set online learning common technique used areas machine learning computationally infeasible train entire dataset requiring need core algorithms also used situations necessary algorithm dynamically adapt new patterns data data generated function time e g stock price prediction online learning algorithms may prone catastrophic interference problem addressed incremental learning approaches setting supervised learning function f x displaystyle f x learned x displaystyle x thought space inputs displaystyle space outputs predicts well instances drawn joint probability distribution p displaystyle p x displaystyle x times reality learner never knows true distribution p displaystyle p instances instead learner usually access training set examples displaystyle ldots setting loss function given v r displaystyle v times mathbb r v displaystyle v measures difference predicted value f displaystyle f true value displaystyle ideal goal select function f h displaystyle f mathcal h h displaystyle mathcal h space functions called hypothesis space notion total loss minimised depending type model one devise different notions loss lead different learning algorithms statistical learning models training sample displaystyle assumed drawn true distribution p displaystyle p objective minimize expected risk common paradigm situation estimate function f displaystyle hat f empirical risk minimization regularized empirical risk minimization choice loss function gives rise several well known learning algorithms regularized least squares support vector machines purely online model category would learn based new input displaystyle current best predictor f displaystyle f extra stored information many formulations example nonlinear kernel methods true online learning possible though form hybrid online learning recursive algorithms used f displaystyle f permitted depend f displaystyle f previous data points displaystyle ldots case space requirements longer guaranteed constant since requires storing previous data points solution may take less time compute addition new data point compared batch learning techniques common strategy overcome issues learn using mini batches process small batch b displaystyle b geq data points time considered pseudo online learning b displaystyle b much smaller total number training points mini batch techniques used repeated passing training data obtain optimized core clarification needed versions machine learning algorithms example stochastic gradient descent combined backpropagation currently de facto training method training artificial neural networks simple example linear least squares used explain variety ideas online learning ideas general enough applied settings example convex loss functions consider setting supervised learning f displaystyle f linear function learned x j r displaystyle x j mathbb r vector inputs w r displaystyle w mathbb r linear filter vector goal compute filter vector w displaystyle w end square loss function used compute vector w displaystyle w minimizes empirical loss let x displaystyle x displaystyle times data matrix r displaystyle mathbb r column vector target values arrival first displaystyle data points assuming covariance matrix x x displaystyle sigma x x invertible best solution f w x displaystyle f langle w x rangle linear least squares problem given calculating covariance matrix j x j x j displaystyle sigma sum j x j x j takes time displaystyle inverting displaystyle times matrix takes time displaystyle rest multiplication takes time displaystyle giving total time displaystyle n displaystyle n total points dataset recompute solution arrival every datapoint n displaystyle ldots n naive approach total complexity displaystyle note storing matrix displaystyle sigma updating step needs adding x x displaystyle x x takes displaystyle time reducing total time displaystyle additional storage space displaystyle store displaystyle sigma recursive least squares algorithm considers online approach least squares problem shown initialising w r displaystyle textstyle w mathbb r r displaystyle textstyle gamma mathbb r times solution linear least squares problem given previous section computed following iteration iteration algorithm proved using induction displaystyle proof also shows displaystyle gamma sigma one look rls also context adaptive filters complexity n displaystyle n steps algorithm displaystyle order magnitude faster corresponding batch learning complexity storage requirements every step displaystyle store matrix displaystyle gamma constant displaystyle case displaystyle sigma invertible consider regularised version problem loss function j n w displaystyle sum j n lambda w easy show algorithm works displaystyle gamma iterations proceed give displaystyle gamma replaced r displaystyle gamma mathbb r times r displaystyle gamma mathbb r becomes stochastic gradient descent algorithm case complexity n displaystyle n steps algorithm reduces displaystyle storage requirements every step displaystyle constant displaystyle however stepsize displaystyle gamma needs chosen carefully solve expected risk minimization problem detailed choosing decaying step size displaystyle gamma approx frac sqrt one prove convergence average iterate w n n n w displaystyle overline w n frac n sum n w setting special case stochastic optimization well known problem optimization practice one perform multiple stochastic gradient passes data algorithm thus obtained called incremental gradient method corresponds iteration main difference stochastic gradient method sequence displaystyle chosen decide training point visited displaystyle th step sequence stochastic deterministic number iterations decoupled number points incremental gradient method shown provide minimizer empirical risk incremental techniques advantageous considering objective functions made sum many terms e g empirical error corresponding large dataset kernels used extend algorithms non parametric models corresponding procedure longer truly online instead involve storing data points still faster brute force method discussion restricted case square loss though extended convex loss shown easy induction x displaystyle x data matrix w displaystyle w output displaystyle steps sgd algorithm c r displaystyle textstyle c mathbb r sequence c displaystyle c satisfies recursion notice x j x displaystyle langle x j x rangle standard kernel r displaystyle mathbb r predictor form general kernel k displaystyle k introduced instead let predictor proof also show predictor minimising least squares loss obtained changing recursion expression requires storing data updating c displaystyle c total time complexity recursion evaluating n displaystyle n th datapoint displaystyle k displaystyle k cost evaluating kernel single pair points thus use kernel allowed movement finite dimensional parameter space w r displaystyle textstyle w mathbb r possibly infinite dimensional feature represented kernel k displaystyle k instead performing recursion space parameters c r displaystyle textstyle c mathbb r whose dimension size training dataset general consequence representer theorem online convex optimization general framework decision making leverages convex optimization allow efficient algorithms framework repeated game playing follows displaystyle goal minimize regret difference cumulative loss loss best fixed point u displaystyle u hindsight example consider case online least squares linear regression weight vectors come convex set r displaystyle mathbb r nature sends back convex loss function v displaystyle v note displaystyle implicitly sent v displaystyle v online prediction problems however cannot fit framework oco example online classification prediction domain loss functions convex scenarios two simple techniques convexification used randomisation surrogate loss functions citation needed simple online convex optimisation algorithms simplest learning rule try select hypothesis least loss past rounds algorithm called follow leader simply given round displaystyle method thus looked greedy algorithm case online quadratic optimization w x displaystyle v w x one show regret bound grows log displaystyle log however similar bounds cannot obtained ftl algorithm important families models like online linear optimization one modifies ftl adding regularisation natural modification ftl used stabilise ftl solutions obtain better regret bounds regularisation function r r displaystyle r rightarrow mathbb r chosen learning performed round follows special example consider case online linear optimisation e nature sends back loss functions form v w z displaystyle v langle w z rangle also let r displaystyle mathbb r suppose regularisation function r w displaystyle r frac eta w chosen positive number displaystyle eta one show regret minimising iteration becomes note rewritten w w v displaystyle w w eta nabla v looks exactly like online gradient descent instead convex subspace r displaystyle mathbb r would need projected onto leading modified update rule algorithm known lazy projection vector displaystyle theta accumulates gradients also known nesterov dual averaging algorithm scenario linear loss functions quadratic regularisation regret bounded displaystyle thus average regret goes desired proved regret bound linear loss functions v w z displaystyle v langle w z rangle generalise algorithm convex loss function subgradient v displaystyle partial v v displaystyle v used linear approximation v displaystyle v near w displaystyle w leading online subgradient descent algorithm initialise parameter w displaystyle eta w displaystyle one use osd algorithm derive displaystyle regret bounds online version svm classification use hinge loss v max displaystyle v max quadratically regularised ftrl algorithms lead lazily projected gradient algorithms described use arbitrary convex functions regularisers one uses online mirror descent optimal regularization hindsight derived linear loss functions leads adagrad algorithm euclidean regularisation one show regret bound displaystyle improved displaystyle strongly convex exp concave loss functions paradigm online learning different interpretations depending choice learning model distinct implications predictive quality sequence functions f f f n displaystyle f f ldots f n prototypical stochastic gradient descent algorithm used discussion noted recursion given first interpretation consider stochastic gradient descent method applied problem minimizing expected risk w displaystyle w defined indeed case infinite stream data since examples displaystyle ldots assumed drawn distribution p displaystyle p sequence gradients v displaystyle v iteration sample stochastic estimates gradient expected risk w displaystyle w therefore one apply complexity results stochastic gradient descent method bound deviation w w displaystyle w w ast w displaystyle w ast minimizer w displaystyle w interpretation also valid case finite training set although multiple passes data gradients longer independent still complexity results obtained special cases second interpretation applies case finite training set considers sgd algorithm instance incremental gradient descent method case one instead looks empirical risk since gradients v displaystyle v incremental gradient descent iterations also stochastic estimates gradient n w displaystyle n w interpretation also related stochastic gradient descent method applied minimize empirical risk opposed expected risk since interpretation concerns empirical risk expected risk multiple passes data readily allowed actually lead tighter bounds deviations n w n w n displaystyle n w n w n ast w n displaystyle w n ast minimizer n w displaystyle n w learning paradigms general algorithms learning models
https://en.wikipedia.org/wiki/Semi-supervised_learning,semi supervised learning approach machine learning combines small amount labeled data large amount unlabeled data training semi supervised learning falls unsupervised learning supervised learning unlabeled data used conjunction small amount labeled data produce considerable improvement learning accuracy acquisition labeled data learning problem often requires skilled human agent physical experiment cost associated labeling process thus may render large fully labeled training sets infeasible whereas acquisition unlabeled data relatively inexpensive situations semi supervised learning great practical value semi supervised learning also theoretical interest machine learning model human learning set l displaystyle l independently identically distributed examples x x l x displaystyle x dots x l x corresponding labels l displaystyle dots l u displaystyle u unlabeled examples x l x l u x displaystyle x l dots x l u x processed semi supervised learning combines information surpass classification performance obtained either discarding unlabeled data supervised learning discarding labels unsupervised learning semi supervised learning may refer either transductive learning inductive learning goal transductive learning infer correct labels given unlabeled data x l x l u displaystyle x l dots x l u goal inductive learning infer correct mapping x displaystyle x displaystyle intuitively learning problem seen exam labeled data sample problems teacher solves class aid solving another set problems transductive setting unsolved problems act exam questions inductive setting become practice problems sort make exam unnecessary perform transductive learning way inferring classification rule entire input space however practice algorithms formally designed transduction induction often used interchangeably order make use unlabeled data relationship underlying distribution data must exist semi supervised learning algorithms make use least one following assumptions points close likely share label also generally assumed supervised learning yields preference geometrically simple decision boundaries case semi supervised learning smoothness assumption additionally yields preference decision boundaries low density regions points close different classes data tend form discrete clusters points cluster likely share label special case smoothness assumption gives rise feature learning clustering algorithms data lie approximately manifold much lower dimension input space case learning manifold using labeled unlabeled data avoid curse dimensionality learning proceed using distances densities defined manifold manifold assumption practical high dimensional data generated process may hard model directly degrees freedom instance human voice controlled vocal folds images various facial expressions controlled muscles cases distances smoothness natural space generating problem superior considering space possible acoustic waves images respectively heuristic approach self training historically oldest approach semi supervised learning examples applications starting transductive learning framework formally introduced vladimir vapnik interest inductive learning using generative models also began probably approximately correct learning bound semi supervised learning gaussian mixture demonstrated ratsaby venkatesh semi supervised learning recently become popular practically relevant due variety problems vast quantities unlabeled data available e g text websites protein sequences images generative approaches statistical learning first seek estimate p displaystyle p disputed discuss distribution data points belonging class probability p displaystyle p given point x displaystyle x label displaystyle proportional p p displaystyle pp bayes rule semi supervised learning generative models viewed either extension supervised learning displaystyle p extension unsupervised learning generative models assume distributions take particular form p displaystyle p parameterized vector displaystyle theta assumptions incorrect unlabeled data may actually decrease accuracy solution relative would obtained labeled data alone however assumptions correct unlabeled data necessarily improves performance unlabeled data distributed according mixture individual class distributions order learn mixture distribution unlabeled data must identifiable different parameters must yield different summed distributions gaussian mixture distributions identifiable commonly used generative models parameterized joint distribution written p p p displaystyle p pp using chain rule parameter vector displaystyle theta associated decision function f argmax p displaystyle f theta underset operatorname argmax p parameter chosen based fit labeled unlabeled data weighted displaystyle lambda another major class methods attempts place boundaries regions data points one commonly used algorithms transductive support vector machine tsvm whereas support vector machines supervised learning seek decision boundary maximal margin labeled data goal tsvm labeling unlabeled data decision boundary maximal margin data addition standard hinge loss displaystyle labeled data loss function displaystyle introduced unlabeled data letting sign f displaystyle operatorname sign f tsvm selects f h b displaystyle f h b reproducing kernel hilbert space h displaystyle mathcal h minimizing regularized empirical risk exact solution intractable due non convex term displaystyle research focuses useful approximations approaches implement low density separation include gaussian process models information regularization entropy minimization graph based methods semi supervised learning use graph representation data node labeled unlabeled example graph may constructed using domain knowledge similarity examples two common methods connect data point k displaystyle k nearest neighbors examples within distance displaystyle epsilon weight w j displaystyle w ij edge x displaystyle x x j displaystyle x j set e x x j displaystyle e frac x x j epsilon within framework manifold regularization graph serves proxy manifold term added standard tikhonov regularization problem enforce smoothness solution relative manifold well relative ambient input space minimization problem becomes h displaystyle mathcal h reproducing kernel hilbert space displaystyle mathcal manifold data lie regularization parameters displaystyle lambda displaystyle lambda control smoothness ambient intrinsic spaces respectively graph used approximate intrinsic regularization term defining graph laplacian l w displaystyle l w j l u w j displaystyle ii sum j l u w ij f displaystyle mathbf f vector f f displaystyle f dots f laplacian also used extend supervised learning algorithms regularized least squares support vector machines semi supervised versions laplacian regularized least squares laplacian svm methods semi supervised learning intrinsically geared learning unlabeled labeled data instead make use unlabeled data within supervised learning framework instance labeled unlabeled examples x x l u displaystyle x dots x l u may inform choice representation distance metric kernel data unsupervised first step supervised learning proceeds labeled examples self training wrapper method semi supervised learning first supervised learning algorithm trained based labeled data classifier applied unlabeled data generate labeled examples input supervised learning algorithm generally labels classifier confident added step co training extension self training multiple classifiers trained different sets features generate labeled examples one another human responses formal semi supervised learning problems yielded varying conclusions degree influence unlabeled data natural learning problems may also viewed instances semi supervised learning much human concept learning involves small amount direct instruction combined large amounts unlabeled experience human infants sensitive structure unlabeled natural categories images dogs cats male female faces infants children take account unlabeled examples sampling process labeled examples arise
https://en.wikipedia.org/wiki/Unsupervised_learning,unsupervised learning type machine learning looks previously undetected patterns data set pre existing labels minimum human supervision contrast supervised learning usually makes use human labeled data unsupervised learning also known self organization allows modeling probability densities inputs forms one three main categories machine learning along supervised reinforcement learning semi supervised learning related variant makes use supervised unsupervised techniques two main methods used unsupervised learning principal component cluster analysis cluster analysis used unsupervised learning group segment datasets shared attributes order extrapolate algorithmic relationships cluster analysis branch machine learning groups data labelled classified categorized instead responding feedback cluster analysis identifies commonalities data reacts based presence absence commonalities new piece data approach helps detect anomalous data points fit either group requirement called unsupervised learning strategy learn new feature space captures characteristics original space maximizing objective function minimising loss function therefore generating covariance matrix unsupervised learning taking eigenvectors covariance matrix eigendecomposition linear algebra operation maximizes variance similarly taking log transform dataset unsupervised learning passing input data multiple sigmoid functions minimising distance function generated resulting data known autoencoder central application unsupervised learning field density estimation statistics though unsupervised learning encompasses many domains involving summarizing explaining data features could contrasted supervised learning saying whereas supervised learning intends infer conditional probability distribution p x textstyle p x conditioned label textstyle input data unsupervised learning intends infer priori probability distribution p x textstyle p x generative adversarial networks also used supervised learning though also applied unsupervised reinforcement techniques common algorithms used unsupervised learning include clustering anomaly detection neural networks approaches learning latent variable models approach uses several methods follows classical example unsupervised learning study neural networks donald hebb principle neurons fire together wire together hebbian learning connection reinforced irrespective error exclusively function coincidence action potentials two neurons similar version modifies synaptic weights takes account time action potentials hebbian learning hypothesized underlie range cognitive functions pattern recognition experiential learning among neural network models self organizing map adaptive resonance theory commonly used unsupervised learning algorithms som topographic organization nearby locations map represent inputs similar properties art model allows number clusters vary problem size lets user control degree similarity members clusters means user defined constant called vigilance parameter art networks used many pattern recognition tasks automatic target recognition seismic signal processing one statistical approaches unsupervised learning method moments method moments unknown parameters model related moments one random variables thus unknown parameters estimated given moments moments usually estimated samples empirically basic moments first second order moments random vector first order moment mean vector second order moment covariance matrix higher order moments usually represented using tensors generalization matrices higher orders multi dimensional arrays particular method moments shown effective learning parameters latent variable models latent variable models statistical models addition observed variables set latent variables also exists observed highly practical example latent variable models machine learning topic modeling statistical model generating words document based topic document topic modeling words document generated according different statistical parameters topic document changed shown method moments consistently recover parameters large class latent variable models assumptions expectation maximization algorithm also one practical methods learning latent variable models however get stuck local optima guaranteed algorithm converge true unknown parameters model contrast method moments global convergence guaranteed conditions
https://en.wikipedia.org/wiki/Learning_to_rank,learning rank machine learned ranking application machine learning typically supervised semi supervised reinforcement learning construction ranking models information retrieval systems training data consists lists items partial order specified items list order typically induced giving numerical ordinal score binary judgment item ranking model purposes rank e producing permutation items new unseen lists similar way rankings training data ranking central part many information retrieval problems document retrieval collaborative filtering sentiment analysis online advertising possible architecture machine learned search engine shown accompanying figure training data consists queries documents matching together relevance degree match may prepared manually human assessors check results queries determine relevance result feasible check relevance documents typically technique called pooling used top documents retrieved existing ranking models checked alternatively training data may derived automatically analyzing clickthrough logs query chains search engines features google searchwiki training data used learning algorithm produce ranking model computes relevance documents actual queries typically users expect search query complete short time makes impossible evaluate complex ranking model document corpus two phase scheme used first small number potentially relevant documents identified using simpler retrieval models permit fast query evaluation vector space model boolean model weighted bm phase called top k displaystyle k document retrieval many heuristics proposed literature accelerate using document static quality score tiered indexes second phase accurate computationally expensive machine learned model used rank documents learning rank algorithms applied areas information retrieval convenience mlr algorithms query document pairs usually represented numerical vectors called feature vectors approach sometimes called bag features analogous bag words model vector space model used information retrieval representation documents components vectors called features factors ranking signals may divided three groups examples features used well known letor dataset selecting designing good features important area machine learning called feature engineering several measures commonly used judge well algorithm training data compare performance different mlr algorithms often learning rank problem reformulated optimization problem respect one metrics examples ranking quality measures dcg normalized variant ndcg usually preferred academic research multiple levels relevance used metrics map mrr precision defined binary judgments recently proposed several new evaluation metrics claim model user satisfaction search results better dcg metric metrics based assumption user likely stop looking search results examining relevant document less relevant document tie yan liu microsoft research asia analyzed existing algorithms learning rank problems paper learning rank information retrieval categorized three groups input representation loss function pointwise pairwise listwise approach practice listwise approaches often outperform pairwise approaches pointwise approaches statement supported large scale experiment performance different learning rank methods large collection benchmark data sets case assumed query document pair training data numerical ordinal score learning rank problem approximated regression problem given single query document pair predict score number existing supervised machine learning algorithms readily used purpose ordinal regression classification algorithms also used pointwise approach used predict score single query document pair takes small finite number values case learning rank problem approximated classification problem learning binary classifier tell document better given pair documents goal minimize average number inversions ranking algorithms try directly optimize value one evaluation measures averaged queries training data difficult evaluation measures continuous functions respect ranking model parameters continuous approximations bounds evaluation measures used partial list published learning rank algorithms shown years first publication method regularized least squares based ranking work extended learning rank general preference graphs note supervised learning algorithms applied pointwise case methods specifically designed ranking mind shown norbert fuhr introduced general idea mlr describing learning approaches information retrieval generalization parameter estimation specific variant approach published three years earlier bill cooper proposed logistic regression purpose used berkeley research group train successful ranking function trec manning et al suggest early works achieved limited results time due little available training data poor machine learning techniques several conferences nips sigir icml workshops devoted learning rank problem since mid commercial web search engines began using machine learned ranking systems since one first search engines start using altavista launched gradient boosting trained ranking function april bing search said powered ranknet algorithm invented microsoft research november russian search engine yandex announced significantly increased search quality due deployment new proprietary matrixnet algorithm variant gradient boosting method uses oblivious decision trees recently also sponsored machine learned ranking competition internet mathematics based search engine production data yahoo announced similar competition google peter norvig denied search engine exclusively relies machine learned ranking cuil ceo tom costello suggests prefer hand built models outperform machine learned models measured metrics like click rate time landing page machine learned models learn people say like people actually like january technology included open source search engine apache solr thus making machine learned search rank widely accessible also enterprise search similar recognition applications computer vision recent neural network based ranking algorithms also found susceptible covert adversarial attacks candidates queries small perturbations imperceptible human beings ranking order could arbitrarily altered addition model agnostic transferable adversarial examples found possible enables black box adversarial attacks deep ranking systems without requiring access underlying implementations conversely robustness ranking systems improved via adversarial defenses madry defense
https://en.wikipedia.org/wiki/Grammar_induction,grammar induction process machine learning learning formal grammar set observations thus constructing model accounts characteristics observed objects generally grammatical inference branch machine learning instance space consists discrete combinatorial objects strings trees graphs grammatical inference often focused problem learning finite state machines various types since efficient algorithms problem since since beginning century approaches extended problem inference context free grammars richer formalisms multiple context free grammars parallel multiple context free grammars classes grammars grammatical inference studied combinatory categorial grammars stochastic context free grammars contextual grammars pattern languages simplest form learning learning algorithm merely receives set examples drawn language question aim learn language examples however learning models studied one frequently studied alternative case learner ask membership queries exact query learning model minimally adequate teacher model introduced angluin wide variety methods grammatical inference two classic sources fu fu duda hart stork also devote brief section problem cite number references basic trial error method present discussed approaches infer subclasses regular languages particular see induction regular languages recent textbook de la higuera covers theory grammatical inference regular languages finite state automata ulizia ferri grifoni provide survey explores grammatical inference methods natural languages method proposed section duda hart stork suggests successively guessing grammar rules testing positive negative observations rule set expanded able generate positive example given rule set also generates negative example must discarded particular approach characterized hypothesis testing bears similarity mitchel version space algorithm duda hart stork text provide simple example nicely illustrates process feasibility unguided trial error approach substantial problems dubious grammatical induction using evolutionary algorithms process evolving representation grammar target language evolutionary process formal grammars easily represented tree structures production rules subjected evolutionary operators algorithms sort stem genetic programming paradigm pioneered john koza citation needed early work simple formal languages used binary string representation genetic algorithms inherently hierarchical structure grammars couched ebnf language made trees flexible approach koza represented lisp programs trees able find analogues genetic operators within standard set tree operators example swapping sub trees equivalent corresponding process genetic crossover sub strings genetic code transplanted individual next generation fitness measured scoring output functions lisp code similar analogues tree structured lisp representation representation grammars trees made application genetic programming techniques possible grammar induction case grammar induction transplantation sub trees corresponds swapping production rules enable parsing phrases language fitness operator grammar based upon measure well performed parsing group sentences target language tree representation grammar terminal symbol production rule corresponds leaf node tree parent nodes corresponds non terminal symbol rule set ultimately root node might correspond sentence non terminal like greedy algorithms greedy grammar inference algorithms make iterative manner decisions seem best stage decisions made usually deal things like creation new rules removal existing rules choice rule applied merging existing rules several ways define stage best also several greedy grammar inference algorithms context free grammar generating algorithms make decision every read symbol context free grammar generating algorithms first read whole given symbol sequence start make decisions recent approach based distributional learning algorithms using approaches applied learning context free grammars mildly context sensitive languages proven correct efficient large subclasses grammars angluin defines pattern string constant symbols variable symbols disjoint set language pattern set nonempty ground instances e strings resulting consistent replacement variable symbols nonempty strings constant symbols note pattern called descriptive finite input set strings language minimal among pattern languages subsuming input set angluin gives polynomial algorithm compute given input string set descriptive patterns one variable x note end builds automaton representing possibly relevant patterns using sophisticated arguments word lengths rely x variable state count drastically reduced erlebach et al give efficient version angluin pattern learning algorithm well parallelized version arimura et al show language class obtained limited unions patterns learned polynomial time pattern theory formulated ulf grenander mathematical formalism describe knowledge world patterns differs approaches artificial intelligence begin prescribing algorithms machinery recognize classify patterns rather prescribes vocabulary articulate recast pattern concepts precise language addition new algebraic vocabulary statistical approach novel aim broad mathematical coverage pattern theory spans algebra statistics well local topological global entropic properties principle grammar induction applied aspects natural language processing applied semantic parsing natural language understanding example based translation morpheme analysis place name derivations citation needed grammar induction also used lossless data compression statistical inference via minimum message length minimum description length principles citation needed grammar induction also used probabilistic models language acquisition
https://en.wikipedia.org/wiki/Supervised_learning,supervised learning machine learning task learning function maps input output based example input output pairs infers function labeled training data consisting set training examples supervised learning example pair consisting input object desired output value supervised learning algorithm analyzes training data produces inferred function used mapping new examples optimal scenario allow algorithm correctly determine class labels unseen instances requires learning algorithm generalize training data unseen situations reasonable way parallel task human animal psychology often referred concept learning order solve given problem supervised learning one perform following steps wide range supervised learning algorithms available strengths weaknesses single learning algorithm works best supervised learning problems four major issues consider supervised learning first issue tradeoff bias variance imagine available several different equally good training data sets learning algorithm biased particular input x displaystyle x trained data sets systematically incorrect predicting correct output x displaystyle x learning algorithm high variance particular input x displaystyle x predicts different output values trained different training sets prediction error learned classifier related sum bias variance learning algorithm generally tradeoff bias variance learning algorithm low bias must flexible fit data well learning algorithm flexible fit training data set differently hence high variance key aspect many supervised learning methods able adjust tradeoff bias variance second issue amount training data available relative complexity true function true function simple inflexible learning algorithm high bias low variance able learn small amount data true function highly complex function able learn large amount training data using flexible learning algorithm low bias high variance clear demarcation input desired output third issue dimensionality input space input feature vectors high dimension learning problem difficult even true function depends small number features many extra dimensions confuse learning algorithm cause high variance hence high input dimensional typically requires tuning classifier low variance high bias practice engineer manually remove irrelevant features input data likely improve accuracy learned function addition many algorithms feature selection seek identify relevant features discard irrelevant ones instance general strategy dimensionality reduction seeks map input data lower dimensional space prior running supervised learning algorithm fourth issue degree noise desired output values desired output values often incorrect learning algorithm attempt find function exactly matches training examples attempting fit data carefully leads overfitting overfit even measurement errors function trying learn complex learning model situation part target function cannot modeled corrupts training data phenomenon called deterministic noise either type noise present better go higher bias lower variance estimator practice several approaches alleviate noise output values early stopping prevent overfitting well detecting removing noisy training examples prior training supervised learning algorithm several algorithms identify noisy training examples removing suspected noisy training examples prior training decreased generalization error statistical significance factors consider choosing applying learning algorithm include following considering new application engineer compare multiple learning algorithms experimentally determine one works best problem hand tuning performance learning algorithm time consuming given fixed resources often better spend time collecting additional training data informative features spend extra time tuning learning algorithms widely used learning algorithms given set n displaystyle n training examples form displaystyle x displaystyle x feature vector th example displaystyle label learning algorithm seeks function g x displaystyle g x x displaystyle x input space displaystyle output space function g displaystyle g element space possible functions g displaystyle g usually called hypothesis space sometimes convenient represent g displaystyle g using scoring function f x r displaystyle f x times mathbb r g displaystyle g defined returning displaystyle value gives highest score g arg max f displaystyle g underset arg max f let f displaystyle f denote space scoring functions although g displaystyle g f displaystyle f space functions many learning algorithms probabilistic models g displaystyle g takes form conditional probability model g p displaystyle g p f displaystyle f takes form joint probability model f p displaystyle f p example naive bayes linear discriminant analysis joint probability models whereas logistic regression conditional probability model two basic approaches choosing f displaystyle f g displaystyle g empirical risk minimization structural risk minimization empirical risk minimization seeks function best fits training data structural risk minimization includes penalty function controls bias variance tradeoff cases assumed training set consists sample independent identically distributed pairs displaystyle order measure well function fits training data loss function l r displaystyle l times mathbb r geq defined training example displaystyle loss predicting value displaystyle hat l displaystyle l risk r displaystyle r function g displaystyle g defined expected loss g displaystyle g estimated training data empirical risk minimization supervised learning algorithm seeks function g displaystyle g minimizes r displaystyle r hence supervised learning algorithm constructed applying optimization algorithm find g displaystyle g g displaystyle g conditional probability distribution p displaystyle p loss function negative log likelihood l log p displaystyle l log p empirical risk minimization equivalent maximum likelihood estimation g displaystyle g contains many candidate functions training set sufficiently large empirical risk minimization leads high variance poor generalization learning algorithm able memorize training examples without generalizing well called overfitting structural risk minimization seeks prevent overfitting incorporating regularization penalty optimization regularization penalty viewed implementing form occam razor prefers simpler functions complex ones wide variety penalties employed correspond different definitions complexity example consider case function g displaystyle g linear function form popular regularization penalty j j displaystyle sum j beta j squared euclidean norm weights also known l displaystyle l norm norms include l displaystyle l norm j j displaystyle sum j beta j l displaystyle l norm number non zero j displaystyle beta j penalty denoted c displaystyle c supervised learning optimization problem find function g displaystyle g minimizes parameter displaystyle lambda controls bias variance tradeoff displaystyle lambda gives empirical risk minimization low bias high variance displaystyle lambda large learning algorithm high bias low variance value displaystyle lambda chosen empirically via cross validation complexity penalty bayesian interpretation negative log prior probability g displaystyle g log p displaystyle log p case j displaystyle j posterior probabability g displaystyle g training methods described discriminative training methods seek find function g displaystyle g discriminates well different output values special case f p displaystyle f p joint probability distribution loss function negative log likelihood log p displaystyle sum log p risk minimization algorithm said perform generative training f displaystyle f regarded generative model explains data generated generative training algorithms often simpler computationally efficient discriminative training algorithms cases solution computed closed form naive bayes linear discriminant analysis several ways standard supervised learning problem generalized
https://en.wikipedia.org/wiki/Statistical_classification,statistics classification problem identifying set categories new observation belongs basis training set data containing observations whose category membership known examples assigning given email spam non spam class assigning diagnosis given patient based observed characteristics patient classification example pattern recognition terminology machine learning classification considered instance supervised learning e learning training set correctly identified observations available corresponding unsupervised procedure known clustering involves grouping data categories based measure inherent similarity distance often individual observations analyzed set quantifiable properties known variously explanatory variables features properties may variously categorical ordinal integer valued real valued classifiers work comparing observations previous observations means similarity distance function algorithm implements classification especially concrete implementation known classifier term classifier sometimes also refers mathematical function implemented classification algorithm maps input data category terminology across fields quite varied statistics classification often done logistic regression similar procedure properties observations termed explanatory variables categories predicted known outcomes considered possible values dependent variable machine learning observations often known instances explanatory variables termed features possible categories predicted classes fields may use different terminology e g community ecology term classification normally refers cluster analysis e type unsupervised learning rather supervised learning described article classification clustering examples general problem pattern recognition assignment sort output value given input value examples regression assigns real valued output input sequence labeling assigns class member sequence values parsing assigns parse tree input sentence describing syntactic structure sentence etc common subclass classification probabilistic classification algorithms nature use statistical inference find best class given instance unlike algorithms simply output best class probabilistic algorithms output probability instance member possible classes best class normally selected one highest probability however algorithm numerous advantages non probabilistic classifiers early work statistical classification undertaken fisher context two group problems leading fisher linear discriminant function rule assigning group new observation early work assumed data values within two groups multivariate normal distribution extension context two groups also considered restriction imposed classification rule linear later work multivariate normal distribution allowed classifier nonlinear several classification rules derived based different adjustments mahalanobis distance new observation assigned group whose centre lowest adjusted distance observation unlike frequentist procedures bayesian classification procedures provide natural way taking account available information relative sizes different groups within overall population bayesian procedures tend computationally expensive days markov chain monte carlo computations developed approximations bayesian clustering rules devised bayesian procedures involve calculation group membership probabilities viewed providing informative outcome data analysis simple attribution single group label new observation classification thought two separate problems binary classification multiclass classification binary classification better understood task two classes involved whereas multiclass classification involves assigning object one several classes since many classification methods developed specifically binary classification multiclass classification often requires combined use multiple binary classifiers algorithms describe individual instance whose category predicted using feature vector individual measurable properties instance property termed feature also known statistics explanatory variable features may variously binary categorical ordinal integer valued real valued instance image feature values might correspond pixels image instance piece text feature values might occurrence frequencies different words algorithms work terms discrete data require real valued integer valued data discretized groups large number algorithms classification phrased terms linear function assigns score possible category k combining feature vector instance vector weights using dot product predicted category one highest score type score function known linear predictor function following general form xi feature vector instance k vector weights corresponding category k score score associated assigning instance category k discrete choice theory instances represent people categories represent choices score considered utility associated person choosing category k algorithms basic setup known linear classifiers distinguishes procedure determining optimal weights coefficients way score interpreted examples algorithms unsupervised learning classifiers form backbone cluster analysis supervised semi supervised learning classifiers system characterizes evaluates unlabeled data cases though classifiers specific set dynamic rules includes interpretation procedure handle vague unknown values tailored type inputs examined since single form classification appropriate data sets large toolkit classification algorithms developed commonly used include classifier performance depends greatly characteristics data classified single classifier works best given problems various empirical tests performed compare classifier performance find characteristics data determine classifier performance determining suitable classifier given problem however still art science measures precision recall popular metrics used evaluate quality classification system recently receiver operating characteristic curves used evaluate tradeoff true false positive rates classification algorithms performance metric uncertainty coefficient advantage simple accuracy affected relative sizes different classes penalize algorithm simply rearranging classes classification many applications employed data mining procedure others detailed statistical modeling undertaken
https://en.wikipedia.org/wiki/Regression_analysis,statistical modeling regression analysis set statistical processes estimating relationships dependent variable one independent variables common form regression analysis linear regression researcher finds line closely fits data according specific mathematical criterion example method ordinary least squares computes unique line minimizes sum squared distances true data line specific mathematical reasons allows researcher estimate conditional expectation dependent variable independent variables take given set values less common forms regression use slightly different procedures estimate alternative location parameters estimate conditional expectation across broader collection non linear models regression analysis primarily used two conceptually distinct purposes first regression analysis widely used prediction forecasting use substantial overlap field machine learning second situations regression analysis used infer causal relationships independent dependent variables importantly regressions reveal relationships dependent variable collection independent variables fixed dataset use regressions prediction infer causal relationships respectively researcher must carefully justify existing relationships predictive power new context relationship two variables causal interpretation latter especially important researchers hope estimate causal relationships using observational data earliest form regression method least squares published legendre gauss legendre gauss applied method problem determining astronomical observations orbits bodies sun gauss published development theory least squares including version gauss markov theorem term regression coined francis galton nineteenth century describe biological phenomenon phenomenon heights descendants tall ancestors tend regress towards normal average galton regression biological meaning work later extended udny yule karl pearson general statistical context work yule pearson joint distribution response explanatory variables assumed gaussian assumption weakened r fisher works fisher assumed conditional distribution response variable gaussian joint distribution need respect fisher assumption closer gauss formulation economists used electromechanical desk calculators calculate regressions sometimes took hours receive result one regression regression methods continue area active research recent decades new methods developed robust regression regression involving correlated responses time series growth curves regression predictor response variables curves images graphs complex data objects regression methods accommodating various types missing data nonparametric regression bayesian methods regression regression predictor variables measured error regression predictor variables observations causal inference regression practice researchers first select model would like estimate use chosen method estimate parameters model regression models involve following components various fields application different terminologies used place dependent independent variables regression models propose displaystyle function x displaystyle x displaystyle beta e displaystyle e representing additive error term may stand un modeled determinants displaystyle random statistical noise researchers goal estimate function f displaystyle f closely fits data carry regression analysis form function f displaystyle f must specified sometimes form function based knowledge relationship displaystyle x displaystyle x rely data knowledge available flexible convenient form f displaystyle f chosen example simple univariate regression may propose f x displaystyle f beta beta x suggesting researcher believes x e displaystyle beta beta x e reasonable approximation statistical process generating data researchers determine preferred statistical model different forms regression analysis provide tools estimate parameters displaystyle beta example least squares finds value displaystyle beta minimizes sum squared errors displaystyle sum given regression method ultimately provide estimate displaystyle beta usually denoted displaystyle hat beta distinguish estimate true parameter value generated data using estimate researcher use fitted value f displaystyle hat f prediction assess accuracy model explaining data whether researcher intrinsically interested estimate displaystyle hat beta predicted value displaystyle hat depend context goals described ordinary least squares least squares widely used estimated function f displaystyle f approximates conditional expectation e displaystyle e however alternative variants useful researchers want model functions f displaystyle f important note must sufficient data estimate regression model example suppose researcher access n displaystyle n rows data one dependent two independent variables displaystyle suppose researcher wants estimate bivariate linear model via least squares x x e displaystyle beta beta x beta x e researcher access n displaystyle n data points could find infinitely many combinations displaystyle explain data equally well combination chosen satisfies x x displaystyle hat hat beta hat beta x hat beta x lead e displaystyle sum hat e sum therefore valid solutions minimize sum squared residuals understand infinitely many options note system n displaystyle n equations solved unknowns makes system underdetermined alternatively one visualize infinitely many dimensional planes go n displaystyle n fixed points generally estimate least squares model k displaystyle k distinct parameters one must n k displaystyle n geq k distinct data points n k displaystyle n k generally exist set parameters perfectly fit data quantity n k displaystyle n k appears often regression analysis referred degrees freedom model moreover estimate least squares model independent variables displaystyle must linearly independent one must able reconstruct independent variables adding multiplying remaining independent variables discussed ordinary least squares condition ensures x x displaystyle x x invertible matrix therefore unique solution displaystyle hat beta exists regression simply calculation using data order interpret output regression meaningful statistical quantity measures real world relationships researchers often rely number classical assumptions often include handful conditions sufficient least squares estimator possess desirable properties particular gauss markov assumptions imply parameter estimates unbiased consistent efficient class linear unbiased estimators practitioners developed variety methods maintain desirable properties real world settings classical assumptions unlikely hold exactly example modeling errors variables lead reasonable estimates independent variables measured errors heteroscedasticity consistent standard errors allow variance e displaystyle e change across values x displaystyle x correlated errors exist within subsets data follow specific patterns handled using clustered standard errors geographic weighted regression newey west standard errors among techniques rows data correspond locations space choice model e displaystyle e within geographic units important consequences subfield econometrics largely focused developing techniques allow researchers make reasonable real world conclusions real world settings classical assumptions hold exactly linear regression model specification dependent variable displaystyle linear combination parameters example simple linear regression modeling n displaystyle n data points one independent variable x displaystyle x two parameters displaystyle beta displaystyle beta multiple linear regression several independent variables functions independent variables adding term x displaystyle x preceding regression gives still linear regression although expression right hand side quadratic independent variable x displaystyle x linear parameters displaystyle beta displaystyle beta displaystyle beta cases displaystyle varepsilon error term subscript displaystyle indexes particular observation returning attention straight line case given random sample population estimate population parameters obtain sample linear regression model residual e displaystyle e widehat difference value dependent variable predicted model displaystyle widehat true value dependent variable displaystyle one method estimation ordinary least squares method obtains parameter estimates minimize sum squared residuals ssr minimization function results set normal equations set simultaneous linear equations parameters solved yield parameter estimators displaystyle widehat beta widehat beta case simple regression formulas least squares estimates x displaystyle bar x mean x displaystyle x values displaystyle bar mean displaystyle values assumption population error term constant variance estimate variance given called mean square error regression denominator sample size reduced number model parameters estimated data displaystyle p displaystyle p regressors displaystyle intercept used case p displaystyle p denominator n displaystyle n standard errors parameter estimates given assumption population error term normally distributed researcher use estimated standard errors create confidence intervals conduct hypothesis tests population parameters general multiple regression model p displaystyle p independent variables x j displaystyle x ij displaystyle th observation j displaystyle j th independent variable first independent variable takes value displaystyle x displaystyle x displaystyle beta called regression intercept least squares parameter estimates obtained p displaystyle p normal equations residual written normal equations matrix notation normal equations written j displaystyle ij element x displaystyle mathbf x x j displaystyle x ij displaystyle element column vector displaystyle displaystyle j displaystyle j element displaystyle hat boldsymbol beta j displaystyle hat beta j thus x displaystyle mathbf x n p displaystyle n times p displaystyle n displaystyle n times displaystyle hat boldsymbol beta p displaystyle p times solution regression model constructed may important confirm goodness fit model statistical significance estimated parameters commonly used checks goodness fit include r squared analyses pattern residuals hypothesis testing statistical significance checked f test overall fit followed tests individual parameters interpretations diagnostic tests rest heavily model assumptions although examination residuals used invalidate model results test f test sometimes difficult interpret model assumptions violated example error term normal distribution small samples estimated parameters follow normal distributions complicate inference relatively large samples however central limit theorem invoked hypothesis testing may proceed using asymptotic approximations limited dependent variables response variables categorical variables variables constrained fall certain range often arise econometrics response variable may non continuous binary variables analysis proceeds least squares linear regression model called linear probability model nonlinear models binary dependent variables include probit logit model multivariate probit model standard method estimating joint relationship several binary dependent variables independent variables categorical variables two values multinomial logit ordinal variables two values ordered logit ordered probit models censored regression models may used dependent variable sometimes observed heckman correction type models may used sample randomly selected population interest alternative procedures linear regression based polychoric correlation categorical variables procedures differ assumptions made distribution variables population variable positive low values represents repetition occurrence event count models like poisson regression negative binomial model may used model function linear parameters sum squares must minimized iterative procedure introduces many complications summarized differences linear non linear least squares regression models predict value variable given known values x variables prediction within range values dataset used model fitting known informally interpolation prediction outside range data known extrapolation performing extrapolation relies strongly regression assumptions extrapolation goes outside data room model fail due differences assumptions sample data true values generally advised citation needed performing extrapolation one accompany estimated value dependent variable prediction interval represents uncertainty intervals tend expand rapidly values independent variable moved outside range covered observed data reasons others tend say might unwise undertake extrapolation however cover full set modeling errors may made particular assumption particular form relation x properly conducted regression analysis include assessment well assumed form matched observed data within range values independent variables actually available means extrapolation particularly reliant assumptions made structural form regression relationship best practice advice citation needed linear variables linear parameters relationship chosen simply computational convenience available knowledge deployed constructing regression model knowledge includes fact dependent variable cannot go outside certain range values made use selecting model even observed dataset values particularly near bounds implications step choosing appropriate functional form regression great extrapolation considered minimum ensure extrapolation arising fitted model realistic generally agreed methods relating number observations versus number independent variables model one rule thumb conjectured good hardin n n displaystyle n n n displaystyle n sample size n displaystyle n number independent variables displaystyle number observations needed reach desired precision model one independent variable example researcher building linear regression model using dataset contains patients researcher decides five observations needed precisely define straight line maximum number independent variables model support although parameters regression model usually estimated using method least squares methods used include major statistical software packages perform least squares regression analysis inference simple linear regression multiple regression using least squares done spreadsheet applications calculators many statistical software packages perform various types nonparametric robust regression methods less standardized different software packages implement different methods method given name may implemented differently different packages specialized regression software developed use fields survey analysis neuroimaging
https://en.wikipedia.org/wiki/Decision_tree_learning,decision tree learning one predictive modelling approaches used statistics data mining machine learning uses decision tree go observations item conclusions item target value tree models target variable take discrete set values called classification trees tree structures leaves represent class labels branches represent conjunctions features lead class labels decision trees target variable take continuous values called regression trees decision trees among popular machine learning algorithms given intelligibility simplicity decision analysis decision tree used visually explicitly represent decisions decision making data mining decision tree describes data page deals decision trees data mining decision tree learning method commonly used data mining goal create model predicts value target variable based several input variables decision tree simple representation classifying examples section assume input features finite discrete domains single target feature called classification element domain classification called class decision tree classification tree tree internal node labeled input feature arcs coming node labeled input feature labeled possible values target feature arc leads subordinate decision node different input feature leaf tree labeled class probability distribution classes signifying data set classified tree either specific class particular probability distribution tree built splitting source set constituting root node tree subsets constitute successor children splitting based set splitting rules based classification features process repeated derived subset recursive manner called recursive partitioning recursion completed subset node values target variable splitting longer adds value predictions process top induction decision trees example greedy algorithm far common strategy learning decision trees data citation needed data mining decision trees described also combination mathematical computational techniques aid description categorization generalization given set data data comes records form dependent variable displaystyle target variable trying understand classify generalize vector x displaystyle textbf x composed features x x x displaystyle x x x etc used task decision trees used data mining two main types term classification regression tree analysis umbrella term used refer procedures first introduced breiman et al trees used regression trees used classification similarities also differences procedure used determine split techniques often called ensemble methods construct one decision tree special case decision tree decision list one sided decision tree every internal node exactly leaf node exactly internal node child less expressive decision lists arguably easier understand general decision trees due added sparsity permit non greedy learning methods monotonic constraints imposed notable decision tree algorithms include id cart invented independently around time citation needed yet follow similar approach learning decision tree training tuples also proposed leverage concepts fuzzy set theory definition special version decision tree known fuzzy decision tree type fuzzy classification generally input vector x displaystyle textbf x associated multiple classes different confidence value boosted ensembles fdts recently investigated well shown performances comparable efficient fuzzy classifiers algorithms constructing decision trees usually work top choosing variable step best splits set items different algorithms use different metrics measuring best generally measure homogeneity target variable within subsets examples given metrics applied candidate subset resulting values combined provide measure quality split used cart algorithm classification trees gini impurity measure often randomly chosen element set would incorrectly labeled randomly labeled according distribution labels subset gini impurity computed summing probability p displaystyle p item label displaystyle chosen times probability k p k p displaystyle sum k neq p k p mistake categorizing item reaches minimum cases node fall single target category gini impurity also information theoretic measure corresponds tsallis entropy deformation coefficient q displaystyle q physics associated lack information equilibrium non extensive dissipative quantum systems limit q displaystyle q one recovers usual boltzmann gibbs shannon entropy sense gini impurity variation usual entropy measure decision trees compute gini impurity set items j displaystyle j classes suppose j displaystyle j let p displaystyle p fraction items labeled class displaystyle set used id c c tree generation algorithms information gain based concept entropy information content information theory entropy defined p p displaystyle p p fractions add represent percentage class present child node results split tree averaging possible values displaystyle expected information gain mutual information meaning average reduction entropy mutual information information gain used decide feature split step building tree simplicity best want keep tree small step choose split results purest daughter nodes commonly used measure purity called information measured bits node tree information value represents expected amount information would needed specify whether new instance classified yes given example reached node consider example data set four attributes outlook temperature humidity windy binary target variable play data points construct decision tree data need compare information gain four trees split one four features split highest information gain taken first split process continue children nodes pure information gain find information gain split using windy must first calculate information data split original data contained nine yes five split using feature windy results two children nodes one windy value true one windy value false data set six data points true windy value three play value yes three play value eight remaining data points windy value false contain two six yes information windy true node calculated using entropy equation since equal number yes node node windy false eight data points six yes two thus find information split take weighted average two numbers based many observations fell node calculate information gain achieved splitting windy feature build tree information gain possible first split would need calculated best first split one provides information gain process repeated impure node tree complete example adapted example appearing witten et al introduced cart variance reduction often employed cases target variable continuous meaning use many metrics would first require discretization applied variance reduction node n defined total reduction variance target variable x due split node displaystyle displaystyle f displaystyle f set presplit sample indices set sample indices split test true set sample indices split test false respectively summands indeed variance estimates though written form without directly referring mean amongst data mining methods decision trees various advantages many data mining software packages provide implementations one decision tree algorithms examples include salford systems cart ibm spss modeler rapidminer sas enterprise miner matlab r weka orange knime microsoft sql server scikit learn decision tree paths root node leaf node proceed way conjunction decision graph possible use disjunctions join two paths together using minimum message length decision graphs extended allow previously unstated new attributes learnt dynamically used different places within graph general coding scheme results better predictive accuracy log loss probabilistic scoring citation needed general decision graphs infer models fewer leaves decision trees evolutionary algorithms used avoid local optimal decisions search decision tree space little priori bias also possible tree sampled using mcmc tree searched bottom fashion
https://en.wikipedia.org/wiki/Ensemble_learning,statistics machine learning ensemble methods use multiple learning algorithms obtain better predictive performance could obtained constituent learning algorithms alone unlike statistical ensemble statistical mechanics usually infinite machine learning ensemble consists concrete finite set alternative models typically allows much flexible structure exist among alternatives supervised learning algorithms perform task searching hypothesis space find suitable hypothesis make good predictions particular problem even hypothesis space contains hypotheses well suited particular problem may difficult find good one ensembles combine multiple hypotheses form better hypothesis term ensemble usually reserved methods generate multiple hypotheses using base learner according broader term multiple classifier systems also covers hybridization hypotheses induced base learner citation needed evaluating prediction ensemble typically requires computation evaluating prediction single model one sense ensemble learning may thought way compensate poor learning algorithms performing lot extra computation hand alternative lot learning one non ensemble system ensemble system may efficient improving overall accuracy increase compute storage communication resources using increase two methods would improved increasing resource use single method fast algorithms decision trees commonly used ensemble methods although slower algorithms benefit ensemble techniques well analogy ensemble techniques used also unsupervised learning scenarios example consensus clustering anomaly detection ensemble supervised learning algorithm trained used make predictions trained ensemble therefore represents single hypothesis hypothesis however necessarily contained within hypothesis space models built thus ensembles shown flexibility functions represent flexibility theory enable fit training data single model would practice ensemble techniques tend reduce problems related fitting training data citation needed empirically ensembles tend yield better results significant diversity among models many ensemble methods therefore seek promote diversity among models combine although perhaps non intuitive random algorithms used produce stronger ensemble deliberate algorithms using variety strong learning algorithms however shown effective using techniques attempt dumb models order promote diversity number component classifiers ensemble great impact accuracy prediction limited number studies addressing problem priori determining ensemble size volume velocity big data streams make even crucial online ensemble classifiers mostly statistical tests used determining proper number components recently theoretical framework suggested ideal number component classifiers ensemble less number classifiers would deteriorate accuracy called law diminishing returns ensemble construction theoretical framework shows using number independent component classifiers class labels gives highest accuracy bayes optimal classifier classification technique ensemble hypotheses hypothesis space average ensemble outperform naive bayes optimal classifier version assumes data conditionally independent class makes computation feasible hypothesis given vote proportional likelihood training dataset would sampled system hypothesis true facilitate training data finite size vote hypothesis also multiplied prior probability hypothesis bayes optimal classifier expressed following equation displaystyle predicted class c displaystyle c set possible classes h displaystyle h hypothesis space p displaystyle p refers probability displaystyle training data ensemble bayes optimal classifier represents hypothesis necessarily h displaystyle h hypothesis represented bayes optimal classifier however optimal hypothesis ensemble space formula restated using bayes theorem says posterior proportional likelihood times prior hence bootstrap aggregating often abbreviated bagging involves model ensemble vote equal weight order promote model variance bagging trains model ensemble using randomly drawn subset training set example random forest algorithm combines random decision trees bagging achieve high classification accuracy bagging samples generated way samples different however replacement allowed replacement means instance occur multiple samples multiple times appear samples samples given multiple learners results learner combined form voting boosting involves incrementally building ensemble training new model instance emphasize training instances previous models mis classified cases boosting shown yield better accuracy bagging also tends likely fit training data far common implementation boosting adaboost although newer algorithms reported achieve better results citation needed boosting equal weight given sample training data starting round data given base learner mis classified instances l assigned weight higher correctly classified instances keeping mind total probability distribution equal boosted data given second base learner results combined form voting bayesian model averaging makes predictions using average several models weights given posterior probability model given data bma known generally give better answers single model obtained e g via stepwise regression especially different models nearly identical performance training set may otherwise perform quite differently obvious question technique uses bayes theorem prior e specification probability model best use given purpose conceptually bma used prior ensemblebma bma packages r use prior implied bayesian information criterion following raftery bas package r supports use priors implied akaike information criterion criteria alternative models well priors coefficients difference bic aic strength preference parsimony penalty model complexity ln k displaystyle lnk bic k displaystyle k aic large sample asymptotic theory established best model increasing sample sizes bic strongly consistent e almost certainly find aic may aic may continue place excessive posterior probability models complicated need hand concerned efficiency e minimum mean square prediction error asymptotically aic aicc efficient bic burnham anderson contributed greatly introducing wider audience basic ideas bayesian model averaging popularizing methodology availability software including free open source packages r beyond mentioned helped make methods accessible wider audience haussler et al showed bma used classification expected error twice expected error bayes optimal classifier bayesian model combination algorithmic correction bayesian model averaging instead sampling model ensemble individually samples space possible ensembles modification overcomes tendency bma converge toward giving weight single model although bmc somewhat computationally expensive bma tends yield dramatically better results results bmc shown better average bma bagging use bayes law compute model weights necessitates computing probability data given model typically none models ensemble exactly distribution training data generated correctly receive value close zero term would work well ensemble big enough sample entire model space rarely possible consequently pattern training data cause ensemble weight shift toward model ensemble closest distribution training data essentially reduces unnecessarily complex method model selection possible weightings ensemble visualized lying simplex vertex simplex weight given single model ensemble bma converges toward vertex closest distribution training data contrast bmc converges toward point distribution projects onto simplex words instead selecting one model closest generating distribution seeks combination models closest generating distribution results bma often approximated using cross validation select best model bucket models likewise results bmc may approximated using cross validation select best ensemble combination random sampling possible weightings bucket models ensemble technique model selection algorithm used choose best model problem tested one problem bucket models produce better results best model set evaluated across many problems typically produce much better results average model set common approach used model selection cross validation selection described following pseudo code cross validation selection summed try training set pick one works best gating generalization cross validation selection involves training another learning model decide models bucket best suited solve problem often perceptron used gating model used pick best model used give linear weight predictions model bucket bucket models used large set problems may desirable avoid training models take long time train landmark learning meta learning approach seeks solve problem involves training fast algorithms bucket using performance algorithms help determine slow algorithm likely best stacking involves training learning algorithm combine predictions several learning algorithms first algorithms trained using available data combiner algorithm trained make final prediction using predictions algorithms additional inputs arbitrary combiner algorithm used stacking theoretically represent ensemble techniques described article although practice logistic regression model often used combiner stacking typically yields performance better single one trained models successfully used supervised learning tasks unsupervised learning also used estimate bagging error rate reported perform bayesian model averaging two top performers netflix competition utilized blending may considered form stacking recent years due growing computational power allows training large ensemble learning reasonable time frame number applications grown increasingly applications ensemble classifiers include land cover mapping one major applications earth observation satellite sensors using remote sensing geospatial data identify materials objects located surface target areas generally classes target materials include roads buildings rivers lakes vegetation different ensemble learning approaches based artificial neural networks kernel principal component analysis decision trees boosting random forest automatic design multiple classifier systems proposed efficiently identify land cover objects change detection image analysis problem consisting identification places land cover changed time change detection widely used fields urban growth forest vegetation dynamics land use disaster monitoring earliest applications ensemble classifiers change detection designed majority voting bayesian average maximum posterior probability distributed denial service one threatening cyber attacks may happen internet service provider combining output single classifiers ensemble classifiers reduce total error detecting discriminating attacks legitimate flash crowds classification malware codes computer viruses computer worms trojans ransomware spywares usage machine learning techniques inspired document categorization problem ensemble learning systems shown proper efficacy area intrusion detection system monitors computer network computer systems identify intruder codes like anomaly detection process ensemble learning successfully aids monitoring systems reduce total error face recognition recently become one popular research areas pattern recognition copes identification verification person digital images hierarchical ensembles based gabor fisher classifier independent component analysis preprocessing techniques earliest ensembles employed field speech recognition mainly based deep learning industry players field like google microsoft ibm reveal core technology speech recognition based approach speech based emotion recognition also satisfactory performance ensemble learning also successfully used facial emotion recognition fraud detection deals identification bank fraud money laundering credit card fraud telecommunication fraud vast domains research applications machine learning ensemble learning improves robustness normal behavior modelling proposed efficient technique detect fraudulent cases activities banking credit card systems accuracy prediction business failure crucial issue financial decision making therefore different ensemble classifiers proposed predict financial crises financial distress also trade based manipulation problem traders attempt manipulate stock prices buying selling activities ensemble classifiers required analyze changes stock market data detect suspicious symptom stock price manipulation ensemble classifiers successfully applied neuroscience proteomics medical diagnosis like neuro cognitive disorder detection based mri datasets
https://en.wikipedia.org/wiki/Bootstrap_aggregating,bootstrap aggregating also called bagging machine learning ensemble meta algorithm designed improve stability accuracy machine learning algorithms used statistical classification regression also reduces variance helps avoid overfitting although usually applied decision tree methods used type method bagging special case model averaging approach given standard training set displaystyle size n bagging generates new training sets displaystyle size n sampling uniformly replacement sampling replacement observations may repeated displaystyle n n large n set displaystyle expected fraction unique examples rest duplicates kind sample known bootstrap sample models fitted using bootstrap samples combined averaging output voting bagging leads improvements unstable procedures include example artificial neural networks classification regression trees subset selection linear regression bagging shown improve preimage learning hand mildly degrade performance stable methods k nearest neighbors illustrate basic principles bagging analysis relationship ozone temperature analysis done r relationship temperature ozone data set apparently non linear based scatter plot mathematically describe relationship loess smoothers used instead building single smoother complete data set bootstrap samples data drawn sample different original data set yet resembles distribution variability bootstrap sample loess smoother fit predictions smoothers made across range data first predicted smooth fits appear grey lines figure lines clearly wiggly overfit data result bandwidth small taking average smoothers fitted subset original data set arrive one bagged predictor clearly mean stable less overfit bagging proposed leo breiman improve classification combining classifications randomly generated training sets
https://en.wikipedia.org/wiki/Boosting_(machine_learning),machine learning boosting ensemble meta algorithm primarily reducing bias also variance supervised learning family machine learning algorithms convert weak learners strong ones boosting based question posed kearns valiant set weak learners create single strong learner weak learner defined classifier slightly correlated true classification contrast strong learner classifier arbitrarily well correlated true classification robert schapire affirmative answer paper question kearns valiant significant ramifications machine learning statistics notably leading development boosting first introduced hypothesis boosting problem simply referred process turning weak learner strong learner informally hypothesis boosting problem asks whether efficient learning algorithm outputs hypothesis whose performance slightly better random guessing e weak learner implies existence efficient algorithm outputs hypothesis arbitrary accuracy e strong learner algorithms achieve hypothesis boosting quickly became simply known boosting freund schapire arcing general technique less synonymous boosting boosting algorithmically constrained boosting algorithms consist iteratively learning weak classifiers respect distribution adding final strong classifier added weighted way related weak learners accuracy weak learner added data weights readjusted known weighting misclassified input data gain higher weight examples classified correctly lose weight note thus future weak learners focus examples previous weak learners misclassified many boosting algorithms original ones proposed robert schapire yoav freund adaptive could take full advantage weak learners schapire freund developed adaboost adaptive boosting algorithm prestigious g del prize algorithms provable boosting algorithms probably approximately correct learning formulation accurately called boosting algorithms algorithms similar spirit clarification needed boosting algorithms sometimes called leveraging algorithms although also sometimes incorrectly called boosting algorithms main variation many boosting algorithms method weighting training data points hypotheses adaboost popular significant historically first algorithm could adapt weak learners often basis introductory coverage boosting university machine learning courses many recent algorithms lpboost totalboost brownboost xgboost madaboost logitboost others many boosting algorithms fit anyboost framework shows boosting performs gradient descent function space using convex cost function given images containing various known objects world classifier learned automatically classify objects future images simple classifiers built based image feature object tend weak categorization performance using boosting methods object categorization way unify weak classifiers special way boost overall ability categorization object categorization typical task computer vision involves determining whether image contains specific category object idea closely related recognition identification detection appearance based object categorization typically contains feature extraction learning classifier applying classifier new examples many ways represent category objects e g shape analysis bag words models local descriptors sift etc examples supervised classifiers naive bayes classifiers support vector machines mixtures gaussians neural networks however research shown object categories locations images discovered unsupervised manner well recognition object categories images challenging problem computer vision especially number categories large due high intra class variability need generalization across variations objects within category objects within one category may look quite different even object may appear unalike different viewpoint scale illumination background clutter partial occlusion add difficulties recognition well humans able recognize thousands object types whereas existing object recognition systems trained recognize quantify e g human faces cars simple objects etc needs update research active dealing categories enabling incremental additions new categories although general problem remains unsolved several multi category objects detectors developed one means feature sharing boosting adaboost used face detection example binary categorization two categories faces versus background general algorithm follows boosting classifier constructed features could yield detection rate displaystyle false positive rate another application boosting binary categorization system detects pedestrians using patterns motion appearance work first combine motion information appearance information features detect walking person takes similar approach viola jones object detection framework compared binary categorization multi class categorization looks common features shared across categories time turn generic edge like features learning detectors category trained jointly compared training separately generalizes better needs less training data requires fewer features achieve performance main flow algorithm similar binary case different measure joint training error shall defined advance iteration algorithm chooses classifier single feature done via converting multi class classification binary one introducing penalty error categories feature classifier paper sharing visual features multiclass multiview object detection torralba et al used gentleboost boosting showed training data limited learning via sharing features much better job sharing given boosting rounds also given performance level total number features required feature sharing detectors observed scale approximately logarithmically number class e slower linear growth non sharing case similar results shown paper incremental learning object detectors using visual shape alphabet yet authors used adaboost boosting boosting algorithms based convex non convex optimization algorithms convex algorithms adaboost logitboost defeated random noise learn basic learnable combinations weak hypotheses limitation pointed long servedio however multiple authors demonstrated boosting algorithms based non convex optimization brownboost learn noisy datasets specifically learn underlying classifier long servedio dataset
https://en.wikipedia.org/wiki/Random_forest,random forests random decision forests ensemble learning method classification regression tasks operate constructing multitude decision trees training time outputting class mode classes mean average prediction individual trees random decision forests correct decision trees habit overfitting training set random forests generally outperform decision trees accuracy lower gradient boosted trees however data characteristics affect performance first algorithm random decision forests created tin kam ho using random subspace method ho formulation way implement stochastic discrimination approach classification proposed eugene kleinberg extension algorithm developed leo breiman adele cutler registered random forests trademark extension combines breiman bagging idea random selection features introduced first ho later independently amit geman order construct collection decision trees controlled variance random forests frequently used blackbox models businesses generate reasonable predictions across wide range data requiring little configuration packages scikit learn general method random decision forests first proposed ho ho established forests trees splitting oblique hyperplanes gain accuracy grow without suffering overtraining long forests randomly restricted sensitive selected feature dimensions subsequent work along lines concluded splitting methods behave similarly long randomly forced insensitive feature dimensions note observation complex classifier getting accurate nearly monotonically sharp contrast common belief complexity classifier grow certain level accuracy hurt overfitting explanation forest method resistance overtraining found kleinberg theory stochastic discrimination early development breiman notion random forests influenced work amit geman introduced idea searching random subset available decisions splitting node context growing single tree idea random subspace selection ho also influential design random forests method forest trees grown variation among trees introduced projecting training data randomly chosen subspace fitting tree node finally idea randomized node optimization decision node selected randomized procedure rather deterministic optimization first introduced dietterich introduction random forests proper first made paper leo breiman paper describes method building forest uncorrelated trees using cart like procedure combined randomized node optimization bagging addition paper combines several ingredients previously known novel form basis modern practice random forests particular report also offers first theoretical result random forests form bound generalization error depends strength trees forest correlation decision trees popular method various machine learning tasks tree learning come closest meeting requirements serving shelf procedure data mining say hastie et al invariant scaling various transformations feature values robust inclusion irrelevant features produces inspectable models however seldom accurate particular trees grown deep tend learn highly irregular patterns overfit training sets e low bias high variance random forests way averaging multiple deep decision trees trained different parts training set goal reducing variance comes expense small increase bias loss interpretability generally greatly boosts performance final model forests like pulling together decision tree algorithm efforts taking teamwork many trees thus improving performance single random tree though quite similar forests give effects k fold cross validation training algorithm random forests applies general technique bootstrap aggregating bagging tree learners given training set x x xn responses yn bagging repeatedly selects random sample replacement training set fits trees samples training predictions unseen samples x made averaging predictions individual regression trees x taking majority vote case classification trees bootstrapping procedure leads better model performance decreases variance model without increasing bias means predictions single tree highly sensitive noise training set average many trees long trees correlated simply training many trees single training set would give strongly correlated trees bootstrap sampling way de correlating trees showing different training sets additionally estimate uncertainty prediction made standard deviation predictions individual regression trees x number samples trees b free parameter typically hundred several thousand trees used depending size nature training set optimal number trees b found using cross validation observing bag error mean prediction error training sample x using trees x bootstrap sample training test error tend level number trees fit procedure describes original bagging algorithm trees random forests differ one way general scheme use modified tree learning algorithm selects candidate split learning process random subset features process sometimes called feature bagging reason correlation trees ordinary bootstrap sample one features strong predictors response variable features selected many b trees causing become correlated analysis bagging random subspace projection contribute accuracy gains different conditions given ho typically classification problem p features p features used split regression problems inventors recommend p minimum node size default practice best values parameters depend problem treated tuning parameters adding one step randomization yields extremely randomized trees extratrees similar ordinary random forests ensemble individual trees two main differences first tree trained using whole learning sample second top splitting tree learner randomized instead computing locally optimal cut point feature consideration random cut point selected value selected uniform distribution within feature empirical range randomly generated splits split yields highest score chosen split node similar ordinary random forests number randomly selected features considered node specified default values parameter p displaystyle sqrt p classification p displaystyle p regression p displaystyle p number features model random forests used rank importance variables regression classification problem natural way following technique described breiman original paper implemented r package randomforest first step measuring variable importance data set n n displaystyle mathcal n n fit random forest data fitting process bag error data point recorded averaged forest measure importance j displaystyle j th feature training values j displaystyle j th feature permuted among training data bag error computed perturbed data set importance score j displaystyle j th feature computed averaging difference bag error permutation trees score normalized standard deviation differences features produce large values score ranked important features produce small values statistical definition variable importance measure given analyzed zhu et al method determining variable importance drawbacks data including categorical variables different number levels random forests biased favor attributes levels methods partial permutations growing unbiased trees used solve problem data contain groups correlated features similar relevance output smaller groups favored larger groups relationship random forests k nearest neighbor algorithm pointed lin jeon turns viewed called weighted neighborhoods schemes models built training set n displaystyle n make predictions displaystyle hat new points x looking neighborhood point formalized weight function w w displaystyle w non negative weight th training point relative new point x tree particular x weights points x displaystyle x must sum one weight functions given follows since forest averages predictions set trees individual weight functions w j displaystyle w j predictions shows whole forest weighted neighborhood scheme weights average individual trees neighbors x interpretation points x displaystyle x sharing leaf tree j displaystyle j way neighborhood x depends complex way structure trees thus structure training set lin jeon show shape neighborhood used random forest adapts local importance feature part construction random forest predictors naturally lead dissimilarity measure among observations one also define random forest dissimilarity measure unlabeled data idea construct random forest predictor distinguishes observed data suitably generated synthetic data observed data original unlabeled data synthetic data drawn reference distribution random forest dissimilarity attractive handles mixed variable types well invariant monotonic transformations input variables robust outlying observations random forest dissimilarity easily deals large number semi continuous variables due intrinsic variable selection example addcl random forest dissimilarity weighs contribution variable according dependent variables random forest dissimilarity used variety applications e g find clusters patients based tissue marker data instead decision trees linear models proposed evaluated base estimators random forests particular multinomial logistic regression naive bayes classifiers machine learning kernel random forests establish connection random forests kernel methods slightly modifying definition random forests rewritten kernel methods interpretable easier analyze leo breiman first person notice link random forest kernel methods pointed random forests grown using random vectors tree construction equivalent kernel acting true margin lin jeon established connection random forests adaptive nearest neighbor implying random forests seen adaptive kernel estimates davies ghahramani proposed random forest kernel show empirically outperform state art kernel methods scornet first defined kerf estimates gave explicit link kerf estimates random forest also gave explicit expressions kernels based centered random forest uniform random forest two simplified models random forest named two kerfs centered kerf uniform kerf proved upper bounds rates consistency centered forest simplified model breiman original random forest uniformly selects attribute among attributes performs splits center cell along pre chosen attribute algorithm stops fully binary tree level k displaystyle k built k n displaystyle k mathbb n parameter algorithm uniform forest another simplified model breiman original random forest uniformly selects feature among features performs splits point uniformly drawn side cell along preselected feature given training sample n n displaystyle mathcal n n p r displaystyle p times mathbb r valued independent random variables distributed independent prototype pair displaystyle e displaystyle operatorname e displaystyle c n displaystyle n e n c c c n displaystyle mathbb e tilde n cc leq c n providing k displaystyle k rightarrow infty n k displaystyle n k rightarrow infty exists constant c displaystyle c e n u f c n displaystyle mathbb e tilde n uf leq cn
https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm,pattern recognition k nearest neighbors algorithm non parametric method proposed thomas cover used classification regression cases input consists k closest training examples feature space output depends whether k nn used classification regression k nn type instance based learning lazy learning function approximated locally computation deferred function evaluation since algorithm relies distance classification normalizing training data improve accuracy dramatically classification regression useful technique assign weights contributions neighbors nearer neighbors contribute average distant ones example common weighting scheme consists giving neighbor weight distance neighbor neighbors taken set objects class object property value known thought training set algorithm though explicit training step required peculiarity k nn algorithm sensitive local structure data suppose pairs displaystyle dots taking values r displaystyle mathbb r times class label x x r p r displaystyle x r sim p r r displaystyle r given norm displaystyle cdot r displaystyle mathbb r point x r displaystyle x mathbb r let displaystyle dots reordering training data x x x x displaystyle x x leq dots leq x x training examples vectors multidimensional feature space class label training phase algorithm consists storing feature vectors class labels training samples classification phase k user defined constant unlabeled vector classified assigning label frequent among k training samples nearest query point commonly used distance metric continuous variables euclidean distance discrete variables text classification another metric used overlap metric context gene expression microarray data example k nn employed correlation coefficients pearson spearman metric often classification accuracy k nn improved significantly distance metric learned specialized algorithms large margin nearest neighbor neighbourhood components analysis drawback basic majority voting classification occurs class distribution skewed examples frequent class tend dominate prediction new example tend common among k nearest neighbors due large number one way overcome problem weight classification taking account distance test point k nearest neighbors class k nearest points multiplied weight proportional inverse distance point test point another way overcome skew abstraction data representation example self organizing map node representative cluster similar points regardless density original training data k nn applied som best choice k depends upon data generally larger values k reduces effect noise classification make boundaries classes less distinct good k selected various heuristic techniques special case class predicted class closest training sample called nearest neighbor algorithm accuracy k nn algorithm severely degraded presence noisy irrelevant features feature scales consistent importance much research effort put selecting scaling features improve classification particularly popular citation needed approach use evolutionary algorithms optimize feature scaling another popular approach scale features mutual information training data training classes citation needed binary classification problems helpful choose k odd number avoids tied votes one popular way choosing empirically optimal k setting via bootstrap method intuitive nearest neighbour type classifier one nearest neighbour classifier assigns point x class closest neighbour feature space c n n n displaystyle c n nn size training data set approaches infinity one nearest neighbour classifier guarantees error rate worse twice bayes error rate k nearest neighbour classifier viewed assigning k nearest neighbours weight k displaystyle k others weight generalised weighted nearest neighbour classifiers ith nearest neighbour assigned weight w n displaystyle w ni n w n displaystyle sum n w ni analogous result strong consistency weighted nearest neighbour classifiers also holds let c n w n n displaystyle c n wnn denote weighted nearest classifier weights w n n displaystyle w ni n subject regularity conditions explanation needed class distributions excess risk following asymptotic expansion constants b displaystyle b b displaystyle b n n w n displaystyle n sum n w ni n n n w n displaystyle n n sum n w ni optimal weighting scheme w n n displaystyle w ni n balances two terms display given follows set k b n displaystyle k lfloor bn frac rfloor optimal weights dominant term asymptotic expansion excess risk displaystyle mathcal similar results true using bagged nearest neighbour classifier k nn special case variable bandwidth kernel density balloon estimator uniform kernel naive version algorithm easy implement computing distances test example stored examples computationally intensive large training sets using approximate nearest neighbor search algorithm makes k nn computationally tractable even large data sets many nearest neighbor search algorithms proposed years generally seek reduce number distance evaluations actually performed k nn strong consistency results amount data approaches infinity two class k nn algorithm guaranteed yield error rate worse twice bayes error rate various improvements k nn speed possible using proximity graphs multi class k nn classification cover hart prove upper bound error rate r displaystyle r bayes error rate r k n n displaystyle r knn k nn error rate number classes problem displaystyle bayesian error rate r displaystyle r approaches zero limit reduces twice bayesian error rate many results error rate k nearest neighbour classifiers k nearest neighbour classifier strongly displaystyle consistent provided k k n displaystyle k k n diverges k n n displaystyle k n n converges zero n displaystyle n infty let c n k n n displaystyle c n knn denote k nearest neighbour classifier based training set size n certain regularity conditions excess risk yields following asymptotic expansion constants b displaystyle b b displaystyle b choice k b n displaystyle k lfloor bn frac rfloor offers trade two terms display k displaystyle k nearest neighbour error converges bayes error optimal rate displaystyle mathcal k nearest neighbor classification performance often significantly improved metric learning popular algorithms neighbourhood components analysis large margin nearest neighbor supervised metric learning algorithms use label information learn new metric pseudo metric input data algorithm large processed suspected redundant input data transformed reduced representation set features transforming input data set features called feature extraction features extracted carefully chosen expected features set extract relevant information input data order perform desired task using reduced representation instead full size input feature extraction performed raw data prior applying k nn algorithm transformed data feature space example typical computer vision computation pipeline face recognition using k nn including feature extraction dimension reduction pre processing steps high dimensional data dimension reduction usually performed prior applying k nn algorithm order avoid effects curse dimensionality curse dimensionality k nn context basically means euclidean distance unhelpful high dimensions vectors almost equidistant search query vector feature extraction dimension reduction combined one step using principal component analysis linear discriminant analysis canonical correlation analysis techniques pre processing step followed clustering k nn feature vectors reduced dimension space machine learning process also called low dimensional embedding high dimensional datasets running fast approximate k nn search using locality sensitive hashing random projections sketches high dimensional similarity search techniques vldb toolbox might feasible option nearest neighbor rules effect implicitly compute decision boundary also possible compute decision boundary explicitly efficiently computational complexity function boundary complexity data reduction one important problems work huge data sets usually data points needed accurate classification data called prototypes found follows training example surrounded examples classes called class outlier causes class outliers include class outliers k nn produce noise detected separated future analysis given two natural numbers k r training example called nn class outlier k nearest neighbors include r examples classes condensed nearest neighbor algorithm designed reduce data set k nn classification selects set prototypes u training data nn u classify examples almost accurately nn whole data set given training set x cnn works iteratively use u instead x classification examples prototypes called absorbed points efficient scan training examples order decreasing border ratio border ratio training example x defined x distance closest example different color x x distance closest example x label x border ratio interval x never exceeds x ordering gives preference borders classes inclusion set prototypes u point different label x called external x calculation border ratio illustrated figure right data points labeled colors initial point x label red external points blue green closest x external point closest red point x border ratio x x attribute initial point x illustration cnn series figures three classes fig initially points class fig shows nn classification map pixel classified nn using data fig shows nn classification map white areas correspond unclassified regions nn voting tied fig shows reduced data set crosses class outliers selected nn rule squares prototypes empty circles absorbed points left bottom corner shows numbers class outliers prototypes absorbed points three classes number prototypes varies different classes example fig shows nn classification map prototypes similar initial data set figures produced using mirkes applet fig dataset fig nn classification map fig nn classification map fig cnn reduced dataset fig nn classification map based cnn extracted prototypes k nn regression k nn algorithm citation needed used estimating continuous variables one algorithm uses weighted average k nearest neighbors weighted inverse distance algorithm works follows distance kth nearest neighbor also seen local density estimate thus also popular outlier score anomaly detection larger distance k nn lower local density likely query point outlier although quite simple outlier model along another classic data mining method local outlier factor works quite well also comparison recent complex approaches according large scale experimental analysis confusion matrix matching matrix often used tool validate accuracy k nn classification robust statistical methods likelihood ratio test also applied
https://en.wikipedia.org/wiki/Linear_regression,statistics linear regression linear approach modeling relationship scalar response one explanatory variables case one explanatory variable called simple linear regression one explanatory variable process called multiple linear regression term distinct multivariate linear regression multiple correlated dependent variables predicted rather single scalar variable linear regression relationships modeled using linear predictor functions whose unknown model parameters estimated data models called linear models commonly conditional mean response given values explanatory variables assumed affine function values less commonly conditional median quantile used like forms regression analysis linear regression focuses conditional probability distribution response given values predictors rather joint probability distribution variables domain multivariate analysis linear regression first type regression analysis studied rigorously used extensively practical applications models depend linearly unknown parameters easier fit models non linearly related parameters statistical properties resulting estimators easier determine linear regression many practical uses applications fall one following two broad categories linear regression models often fitted using least squares approach may also fitted ways minimizing lack fit norm minimizing penalized version least squares cost function ridge regression lasso conversely least squares approach used fit models linear models thus although terms least squares linear model closely linked synonymous given data set x x p n displaystyle x ldots x ip n n statistical units linear regression model assumes relationship dependent variable p vector regressors x linear relationship modeled disturbance term error variable unobserved random variable adds noise linear relationship dependent variable regressors thus model takes form denotes transpose xit inner product vectors xi often n equations stacked together written matrix notation remarks notation terminology fitting linear model given data set usually requires estimating regression coefficients displaystyle boldsymbol beta error term x displaystyle boldsymbol varepsilon mathbf x boldsymbol beta minimized example common use sum squared errors displaystyle boldsymbol varepsilon quality fit example consider situation small ball tossed air measure heights ascent hi various moments time ti physics tells us ignoring drag relationship modeled determines initial velocity ball proportional standard gravity due measurement errors linear regression used estimate values measured data model non linear time variable linear parameters take regressors xi model takes standard form standard linear regression models standard estimation techniques make number assumptions predictor variables response variables relationship numerous extensions developed allow assumptions relaxed cases eliminated entirely generally extensions make estimation procedure complex time consuming may also require data order produce equally precise model following major assumptions made standard linear regression models standard estimation techniques beyond assumptions several statistical properties data strongly influence performance different estimation methods fitted linear regression model used identify relationship single predictor variable xj response variable predictor variables model held fixed specifically interpretation j expected change one unit change xj covariates held fixed expected value partial derivative respect xj sometimes called unique effect xj contrast marginal effect xj assessed using correlation coefficient simple linear regression model relating xj effect total derivative respect xj care must taken interpreting regression results regressors may allow marginal changes others cannot held fixed possible unique effect nearly zero even marginal effect large may imply covariate captures information xj variable model contribution xj variation conversely unique effect xj large marginal effect nearly zero would happen covariates explained great deal variation mainly explain variation way complementary captured xj case including variables model reduces part variability unrelated xj thereby strengthening apparent relationship xj meaning expression held fixed may depend values predictor variables arise experimenter directly sets values predictor variables according study design comparisons interest may literally correspond comparisons among units whose predictor variables held fixed experimenter alternatively expression held fixed refer selection takes place context data analysis case hold variable fixed restricting attention subsets data happen common value given predictor variable interpretation held fixed used observational study notion unique effect appealing studying complex system multiple interrelated components influence response variable cases literally interpreted causal effect intervention linked value predictor variable however argued many cases multiple regression analysis fails clarify relationships predictor variables response variable predictors correlated assigned following study design commonality analysis may helpful disentangling shared unique impacts correlated independent variables numerous extensions linear regression developed allow assumptions underlying basic model relaxed simplest case single scalar predictor variable x single scalar response variable known simple linear regression extension multiple vector valued predictor variables known multiple linear regression also known multivariable linear regression multiple linear regression generalization simple linear regression case one independent variable special case general linear models restricted one dependent variable basic model multiple linear regression observation n formula consider n observations one dependent variable p independent variables thus yi ith observation dependent variable xij ith observation jth independent variable j p values j represent parameters estimated ith independent identically distributed normal error general multivariate linear regression one equation form dependent variables share set explanatory variables hence estimated simultaneously observations indexed n dependent variables indexed j nearly real world regression models involve multiple predictors basic descriptions linear regression often phrased terms multiple regression model note however cases response variable still scalar another term multivariate linear regression refers cases vector e general linear regression general linear model considers situation response variable scalar vector yi conditional linearity e x b displaystyle e mathbf x mathsf b still assumed matrix b replacing vector classical linear regression model multivariate analogues ordinary least squares generalized least squares developed general linear models also called multivariate linear models multivariable linear models various models created allow heteroscedasticity e errors different response variables may different variances example weighted least squares method estimating linear regression models response variables may different error variances possibly correlated errors heteroscedasticity consistent standard errors improved method use uncorrelated potentially heteroscedastic errors generalized linear models framework modeling response variables bounded discrete used example generalized linear models allow arbitrary link function g relates mean response variable predictors e g displaystyle e g link function often related distribution response particular typically effect transforming displaystyle range linear predictor range response variable common examples glms single index models clarification needed allow degree nonlinearity relationship x preserving central role linear predictor x classical linear regression model certain conditions simply applying ols data single index model consistently estimate proportionality constant hierarchical linear models organizes data hierarchy regressions example regressed b b regressed c often used variables interest natural hierarchical structure educational statistics students nested classrooms classrooms nested schools schools nested administrative grouping school district response variable might measure student achievement test score different covariates would collected classroom school school district levels errors variables models extend traditional linear regression model allow predictor variables x observed error error causes standard estimators become biased generally form bias attenuation meaning effects biased toward zero large number procedures developed parameter estimation inference linear regression methods differ computational simplicity algorithms presence closed form solution robustness respect heavy tailed distributions theoretical assumptions needed validate desirable statistical properties consistency asymptotic efficiency common estimation techniques linear regression summarized assuming independent variable x x x x displaystyle vec x x x dots x model parameters displaystyle vec beta beta beta cdots beta model prediction would j j x j displaystyle approx beta sum j beta j times x j x displaystyle vec x extended x x x x displaystyle vec x x x dots x displaystyle would become dot product parameter independent variable e j j x j x displaystyle approx sum j beta j times x j vec beta vec x least squares setting optimum parameter defined minimizes sum mean squared loss arg min l arg min n displaystyle vec hat beta underset vec beta mbox arg min l underset vec beta mbox arg min sum n putting independent dependent variables matrices x displaystyle x displaystyle respectively loss function rewritten l x x x x x displaystyle begin aligned l x vec beta x vec beta vec beta x vec beta x x vec beta end aligned loss convex optimum solution lies gradient zero gradient loss function l x x x displaystyle begin aligned frac partial l partial vec beta frac partial left partial vec beta x vec beta x x end aligned setting gradient zero produces optimum parameter x x x x x x x x x x displaystyle begin aligned x vec beta x x rightarrow x vec beta x x rightarrow x x x vec beta rightarrow vec hat beta x end aligned note prove displaystyle hat beta obtained indeed local minimum one needs differentiate obtain hessian matrix show positive definite provided gauss markov theorem linear least squares methods include mainly linear regression widely used biological behavioral social sciences describe possible relationships variables ranks one important tools used disciplines trend line represents trend long term movement time series data components accounted tells whether particular data set increased decreased period time trend line could simply drawn eye set data points properly position slope calculated using statistical techniques like linear regression trend lines typically straight lines although variations use higher degree polynomials depending degree curvature desired line trend lines sometimes used business analytics show changes data time advantage simple trend lines often used argue particular action event caused observed changes point time simple technique require control group experimental design sophisticated analysis technique however suffers lack scientific validity cases potential changes affect data early evidence relating tobacco smoking mortality morbidity came observational studies employing regression analysis order reduce spurious correlations analyzing observational data researchers usually include several variables regression models addition variable primary interest example regression model cigarette smoking independent variable primary interest dependent variable lifespan measured years researchers might include education income additional independent variables ensure observed effect smoking lifespan due socio economic factors however never possible include possible confounding variables empirical analysis example hypothetical gene might increase mortality also cause people smoke reason randomized controlled trials often able generate compelling evidence causal relationships obtained using regression analyses observational data controlled experiments feasible variants regression analysis instrumental variables regression may used attempt estimate causal relationships observational data capital asset pricing model uses linear regression well concept beta analyzing quantifying systematic risk investment comes directly beta coefficient linear regression model relates return investment return risky assets linear regression predominant empirical tool economics example used predict consumption spending fixed investment spending inventory investment purchases country exports spending imports demand hold liquid assets labor demand labor supply linear regression finds application wide range environmental science applications canada environmental effects monitoring program uses statistical analyses fish benthic surveys measure effects pulp mill metal mine effluent aquatic ecosystem linear regression plays important role field artificial intelligence machine learning linear regression algorithm one fundamental supervised machine learning algorithms due relative simplicity well known properties least squares linear regression means finding good rough linear fit set points performed legendre gauss prediction planetary movement quetelet responsible making procedure well known using extensively social sciences
https://en.wikipedia.org/wiki/Naive_Bayes_classifier,statistics naive bayes classifiers family simple probabilistic classifiers based applying bayes theorem strong independence assumptions features among simplest bayesian network models could coupled kernel density estimation achieve higher accuracy levels na bayes classifiers highly scalable requiring number parameters linear number variables learning problem maximum likelihood training done evaluating closed form expression takes linear time rather expensive iterative approximation used many types classifiers statistics computer science literature naive bayes models known variety names including simple bayes independence bayes names reference use bayes theorem classifier decision rule na bayes bayesian method naive bayes simple technique constructing classifiers models assign class labels problem instances represented vectors feature values class labels drawn finite set single algorithm training classifiers family algorithms based common principle naive bayes classifiers assume value particular feature independent value feature given class variable example fruit may considered apple red round cm diameter naive bayes classifier considers features contribute independently probability fruit apple regardless possible correlations color roundness diameter features types probability models naive bayes classifiers trained efficiently supervised learning setting many practical applications parameter estimation naive bayes models uses method maximum likelihood words one work naive bayes model without accepting bayesian probability using bayesian methods despite naive design apparently oversimplified assumptions naive bayes classifiers worked quite well many complex real world situations analysis bayesian classification problem showed sound theoretical reasons apparently implausible efficacy naive bayes classifiers still comprehensive comparison classification algorithms showed bayes classification outperformed approaches boosted trees random forests advantage naive bayes requires small number training data estimate parameters necessary classification citation needed abstractly na bayes conditional probability model given problem instance classified represented vector x displaystyle mathbf x representing n features assigns instance probabilities k possible outcomes classes c k displaystyle c k problem formulation number features n large feature take large number values basing model probability tables infeasible therefore reformulate model make tractable using bayes theorem conditional probability decomposed plain english using bayesian probability terminology equation written practice interest numerator fraction denominator depend c displaystyle c values features x displaystyle x given denominator effectively constant numerator equivalent joint probability model rewritten follows using chain rule repeated applications definition conditional probability na conditional independence assumptions come play assume features x displaystyle mathbf x mutually independent conditional category c k displaystyle c k assumption thus joint model expressed displaystyle varpropto denotes proportionality means independence assumptions conditional distribution class variable c displaystyle c evidence z p k p p displaystyle z p sum k p p scaling factor dependent x x n displaystyle x ldots x n constant values feature variables known discussion far derived independent feature model na bayes probability model na bayes classifier combines model decision rule one common rule pick hypothesis probable known maximum posteriori map decision rule corresponding classifier bayes classifier function assigns class label c k displaystyle hat c k k follows class prior may calculated assuming equiprobable classes k displaystyle p k calculating estimate class probability training set estimate parameters feature distribution one must assume distribution generate nonparametric models features training set assumptions distributions features called event model na bayes classifier discrete features like ones encountered document classification multinomial bernoulli distributions popular assumptions lead two distinct models often confused dealing continuous data typical assumption continuous values associated class distributed according normal distribution example suppose training data contains continuous attribute x displaystyle x first segment data class compute mean variance x displaystyle x class let k displaystyle mu k mean values x displaystyle x associated class ck let k displaystyle sigma k bessel corrected variance values x displaystyle x associated class ck suppose collected observation value v displaystyle v probability distribution v displaystyle v given class c k displaystyle c k p displaystyle p computed plugging v displaystyle v equation normal distribution parameterized k displaystyle mu k k displaystyle sigma k another common technique handling continuous values use binning discretize feature values obtain new set bernoulli distributed features literature fact suggests necessary apply naive bayes discretization may throw away discriminative information sometimes distribution class conditional marginal densities far normal cases kernel density estimation used realistic estimate marginal densities class method introduced john langley boost accuracy classifier considerably multinomial event model samples represent frequencies certain events generated multinomial displaystyle p displaystyle p probability event occurs feature vector x displaystyle mathbf x histogram x displaystyle x counting number times event observed particular instance event model typically used document classification events representing occurrence word single document likelihood observing histogram x given multinomial na bayes classifier becomes linear classifier expressed log space b log p displaystyle b log p w k log p k displaystyle w ki log p ki given class feature value never occur together training data frequency based probability estimate zero probability estimate directly proportional number occurrences feature value problematic wipe information probabilities multiplied therefore often desirable incorporate small sample correction called pseudocount probability estimates probability ever set exactly zero way regularizing naive bayes called laplace smoothing pseudocount one lidstone smoothing general case rennie et al discuss problems multinomial assumption context document classification possible ways alleviate problems including use tf idf weights instead raw term frequencies document length normalization produce naive bayes classifier competitive support vector machines multivariate bernoulli event model features independent booleans describing inputs like multinomial model model popular document classification tasks binary term occurrence features used rather term frequencies x displaystyle x boolean expressing occurrence absence th term vocabulary likelihood document given class c k displaystyle c k given p k displaystyle p ki probability class c k displaystyle c k generating term x displaystyle x event model especially popular classifying short texts benefit explicitly modelling absence terms note naive bayes classifier bernoulli event model multinomial nb classifier frequency counts truncated one given way train na bayes classifier labeled data possible construct semi supervised training algorithm learn combination labeled unlabeled data running supervised learning algorithm loop convergence determined based improvement model likelihood p displaystyle p displaystyle theta denotes parameters na bayes model training algorithm instance general expectation maximization algorithm prediction step inside loop e step em training na bayes step algorithm formally justified assumption data generated mixture model components mixture model exactly classes classification problem despite fact far reaching independence assumptions often inaccurate naive bayes classifier several properties make surprisingly useful practice particular decoupling class conditional feature distributions means distribution independently estimated one dimensional distribution helps alleviate problems stemming curse dimensionality need data sets scale exponentially number features naive bayes often fails produce good estimate correct class probabilities may requirement many applications example naive bayes classifier make correct map decision rule classification long correct class probable class true regardless whether probability estimate slightly even grossly inaccurate manner overall classifier robust enough ignore serious deficiencies underlying naive probability model reasons observed success naive bayes classifier discussed literature cited case discrete inputs naive bayes classifiers form generative discriminative pair logistic regression classifiers naive bayes classifier considered way fitting probability model optimizes joint likelihood p displaystyle p logistic regression fits probability model optimize conditional p displaystyle p link two seen observing decision function naive bayes rewritten predict class c displaystyle c odds p displaystyle p exceed p displaystyle p expressing log space gives left hand side equation log odds logit quantity predicted linear model underlies logistic regression since naive bayes also linear model two discrete event models reparametrised linear function b w x displaystyle b mathbf w top x obtaining probabilities matter applying logistic function b w x displaystyle b mathbf w top x multiclass case softmax function discriminative classifiers lower asymptotic error generative ones however research ng jordan shown practical cases naive bayes outperform logistic regression reaches asymptotic error faster problem classify whether given person male female based measured features features include height weight foot size example training set classifier created training set using gaussian distribution assumption would let say equiprobable classes p p prior probability distribution might based knowledge frequencies larger population frequency training set sample classified male female wish determine posterior greater male female classification male posterior given classification female posterior given evidence may calculated however given sample evidence constant thus scales posteriors equally therefore affect classification ignored determine probability distribution sex sample displaystyle mu displaystyle sigma cdot parameters normal distribution previously determined training set note value greater ok probability density rather probability height continuous variable since posterior numerator greater female case predict sample female worked example naive bayesian classification document classification problem consider problem classifying documents content example spam non spam e mails imagine documents drawn number classes documents modeled sets words probability th word given document occurs document class c written probability given document contains words w displaystyle w given class c question desire answer probability given document belongs given class c words p displaystyle p definition bayes theorem manipulates statement probability terms likelihood assume moment two mutually exclusive classes every element either one using bayesian result write dividing one gives factored thus probability ratio p p expressed terms series likelihood ratios actual probability p easily computed log p based observation p p taking logarithm ratios conversion log likelihood ratio probability takes form sigmoid curve see logit details finally document classified follows spam p p displaystyle p p p displaystyle ln p p otherwise spam
https://en.wikipedia.org/wiki/Artificial_neural_network,collective intelligence collective action self organized criticality herd mentality phase transition agent based modelling synchronization ant colony optimization particle swarm optimization swarm behaviour social network analysis small world networks centrality motifs graph theory scaling robustness systems biology dynamic networks evolutionary computation genetic algorithms genetic programming artificial life machine learning evolutionary developmental biology artificial intelligence evolutionary robotics reaction diffusion systems partial differential equations dissipative structures percolation cellular automata spatial ecology self replication operationalization feedback self reference goal oriented system dynamics sensemaking entropy cybernetics autopoiesis information theory ordinary differential equations phase space attractors population dynamics chaos multistability bifurcation rational choice theory bounded rationality artificial neural networks usually simply called neural networks computing systems vaguely inspired biological neural networks constitute animal brains ann based collection connected units nodes called artificial neurons loosely model neurons biological brain connection like synapses biological brain transmit signal neurons artificial neuron receives signal processes signal neurons connected signal connection real number output neuron computed non linear function sum inputs connections called edges neurons edges typically weight adjusts learning proceeds weight increases decreases strength signal connection neurons may threshold signal sent aggregate signal crosses threshold typically neurons aggregated layers different layers may perform different transformations inputs signals travel first layer last layer possibly traversing layers multiple times mw parser output toclimit toclevel ul mw parser output toclimit toclevel ul mw parser output toclimit toclevel ul mw parser output toclimit toclevel ul mw parser output toclimit toclevel ul mw parser output toclimit toclevel ul display none neural networks learn processing examples contains known input result forming probability weighted associations two stored within data structure net training neural network given example usually conducted determining difference processed output network target output error network adjusts weighted associations according learning rule using error value successive adjustments cause neural network produce output increasingly similar target output sufficient number adjustments training terminated based upon certain criteria known supervised learning systems learn perform tasks considering examples generally without programmed task specific rules example image recognition might learn identify images contain cats analyzing example images manually labeled cat cat using results identify cats images without prior knowledge cats example fur tails whiskers cat like faces instead automatically generate identifying characteristics examples process warren mcculloch walter pitts opened subject creating computational model neural networks late hebb created learning hypothesis based mechanism neural plasticity became known hebbian learning farley wesley clark first used computational machines called calculators simulate hebbian network rosenblatt created perceptron first functional networks many layers published ivakhnenko lapa group method data handling basics continuous backpropagation derived context control theory kelley bryson using principles dynamic programming seppo linnainmaa published general method automatic differentiation discrete connected networks nested differentiable functions dreyfus used backpropagation adapt parameters controllers proportion error gradients werbos backpropagation algorithm enabled practical training multi layer networks applied linnainmaa ad method neural networks way became widely used thereafter research stagnated following minsky papert discovered basic perceptrons incapable processing exclusive circuit computers lacked sufficient power process useful neural networks increasing transistor count digital electronics provided processing power enabled development practical artificial neural networks max pooling introduced help least shift invariance tolerance deformation aid object recognition schmidhuber adopted multi level hierarchy networks pre trained one level time unsupervised learning fine tuned backpropagation geoffrey hinton et al proposed learning high level representation using successive layers binary real valued latent variables restricted boltzmann machine model layer ng dean created network learned recognize higher level concepts cats watching unlabeled images unsupervised pre training increased computing power gpus distributed computing allowed use larger networks particularly image visual recognition problems became known deep learning ciresan colleagues showed despite vanishing gradient problem gpus make backpropagation feasible many layered feedforward neural networks anns began winning prizes ann contests approaching human level performance various tasks initially pattern recognition machine learning example bi directional multi dimensional long short term memory graves et al three competitions connected handwriting recognition without prior knowledge three languages learned ciresan colleagues built first pattern recognizers achieve human competitive superhuman performance benchmarks traffic sign recognition anns began attempt exploit architecture human brain perform tasks conventional algorithms little success soon reoriented towards improving empirical results mostly abandoning attempts remain true biological precursors neurons connected various patterns allow output neurons become input others network forms directed weighted graph artificial neural network consists collection simulated neurons neuron node connected nodes via links correspond biological axon synapse dendrite connections link weight determines strength one node influence another anns composed artificial neurons conceptually derived biological neurons artificial neuron inputs produce single output sent multiple neurons inputs feature values sample external data images documents outputs neurons outputs final output neurons neural net accomplish task recognizing object image find output neuron first take weighted sum inputs weighted weights connections inputs neuron add bias term sum weighted sum sometimes called activation weighted sum passed activation function produce output initial inputs external data images documents ultimate outputs accomplish task recognizing object image network consists connections connection providing output one neuron input another neuron connection assigned weight represents relative importance given neuron multiple input output connections propagation function computes input neuron outputs predecessor neurons connections weighted sum bias term added result propagation neurons typically organized multiple layers especially deep learning neurons one layer connect neurons immediately preceding immediately following layers layer receives external data input layer layer produces ultimate result output layer zero hidden layers single layer unlayered networks also used two layers multiple connection patterns possible fully connected every neuron one layer connecting every neuron next layer pooling group neurons one layer connect single neuron next layer thereby reducing number neurons layer neurons connections form directed acyclic graph known feedforward networks alternatively networks allow connections neurons previous layers known recurrent networks hyperparameter constant parameter whose value set learning process begins values parameters derived via learning examples hyperparameters include learning rate number hidden layers batch size values hyperparameters dependent hyperparameters example size layers depend overall number layers learning adaptation network better handle task considering sample observations learning involves adjusting weights network improve accuracy result done minimizing observed errors learning complete examining additional observations usefully reduce error rate even learning error rate typically reach learning error rate high network typically must redesigned practically done defining cost function evaluated periodically learning long output continues decline learning continues cost frequently defined statistic whose value approximated outputs actually numbers error low difference output correct answer small learning attempts reduce total differences across observations learning models viewed straightforward application optimization theory statistical estimation learning rate defines size corrective steps model takes adjust errors observation high learning rate shortens training time lower ultimate accuracy lower learning rate takes longer potential greater accuracy optimizations quickprop primarily aimed speeding error minimization improvements mainly try increase reliability order avoid oscillation inside network alternating connection weights improve rate convergence refinements use adaptive learning rate increases decreases appropriate concept momentum allows balance gradient previous change weighted weight adjustment depends degree previous change momentum close emphasizes gradient value close emphasizes last change possible define cost function ad hoc frequently choice determined function desirable properties arises model backpropagation method adjust connection weights compensate error found learning error amount effectively divided among connections technically backprop calculates gradient cost function associated given state respect weights weight updates done via stochastic gradient descent methods extreme learning machines prop networks training without backtracking weightless networks non connectionist neural networks three major learning paradigms supervised learning unsupervised learning reinforcement learning correspond particular learning task supervised learning uses set paired inputs desired outputs learning task produce desired output input case cost function related eliminating incorrect deductions commonly used cost mean squared error tries minimize average squared error network output desired output tasks suited supervised learning pattern recognition regression supervised learning also applicable sequential data thought learning teacher form function provides continuous feedback quality solutions obtained thus far unsupervised learning input data given along cost function function data x displaystyle textstyle x network output cost function dependent task priori assumptions trivial example consider model f displaystyle textstyle f displaystyle textstyle constant cost c e displaystyle textstyle c e minimizing cost produces value displaystyle textstyle equal mean data cost function much complicated form depends application example compression could related mutual information x displaystyle textstyle x f displaystyle textstyle f whereas statistical modeling could related posterior probability model given data tasks fall within paradigm unsupervised learning general estimation problems applications include clustering estimation statistical distributions compression filtering applications playing video games actor takes string actions receiving generally unpredictable response environment one goal win game e generate positive responses reinforcement learning aim weight network perform actions minimize long term cost point time agent performs action environment generates observation instantaneous cost according rules rules long term cost usually estimated juncture agent decides whether explore new actions uncover costs exploit prior learning proceed quickly formally environment modeled markov decision process states n displaystyle textstyle n actions displaystyle textstyle state transitions known probability distributions used instead instantaneous cost distribution p displaystyle textstyle p observation distribution p displaystyle textstyle p transition distribution p displaystyle textstyle p policy defined conditional distribution actions given observations taken together two define markov chain aim discover lowest cost mc anns serve learning component applications dynamic programming coupled anns applied problems involved vehicle routing video games natural resource management medicine anns ability mitigate losses accuracy even reducing discretization grid density numerically approximating solution control problems tasks fall within paradigm reinforcement learning control problems games sequential decision making tasks self learning neural networks introduced along neural network capable self learning named crossbar adaptive array system one input situation one output action neither external advice input external reinforcement input environment caa computes crossbar fashion decisions actions emotions encountered situations system driven interaction cognition emotion given memory matrix w w crossbar self learning algorithm iteration performs following computation backpropagated value emotion toward consequence situation caa exists two environments one behavioral environment behaves genetic environment initially receives initial emotions encountered situations behavioral environment received genome vector genetic environment caa learn goal seeking behavior behavioral environment contains desirable undesirable situations bayesian framework distribution set allowed models chosen minimize cost evolutionary methods gene expression programming simulated annealing expectation maximization non parametric methods particle swarm optimization learning algorithms convergent recursion learning algorithm cerebellar model articulation controller neural networks two modes learning available stochastic batch stochastic learning input creates weight adjustment batch learning weights adjusted based batch inputs accumulating errors batch stochastic learning introduces noise process using local gradient calculated one data point reduces chance network getting stuck local minima however batch learning typically yields faster stable descent local minimum since update performed direction batch average error common compromise use mini batches small batches samples batch selected stochastically entire data set anns evolved broad family techniques advanced state art across multiple domains simplest types one static components including number units number layers unit weights topology dynamic types allow one evolve via learning latter much complicated shorten learning periods produce better results types allow require learning supervised operator others operate independently types operate purely hardware others purely software run general purpose computers main breakthroughs include convolutional neural networks proven particularly successful processing visual two dimensional data long short term memory avoid vanishing gradient problem handle signals mix low high frequency components aiding large vocabulary speech recognition text speech synthesis photo real talking heads competitive networks generative adversarial networks multiple networks compete tasks winning game deceiving opponent authenticity input neural architecture search uses machine learning automate ann design various approaches nas designed networks compare well hand designed systems basic search algorithm propose candidate model evaluate dataset use results feedback teach nas network available systems include automl autokeras design issues include deciding number type connectedness network layers well size connection type hyperparameters must also defined part design governing matters many neurons layer learning rate step stride depth receptive field padding etc using artificial neural networks requires understanding characteristics ann capabilities fall within following broad categories citation needed ability reproduce model nonlinear processes artificial neural networks found applications many disciplines application areas include system identification control quantum chemistry general game playing pattern recognition sequence recognition medical diagnosis finance data mining visualization machine translation social network filtering e mail spam filtering anns used diagnose cancers including lung cancer prostate cancer colorectal cancer distinguish highly invasive cancer cell lines less invasive lines using cell shape information anns used accelerate reliability analysis infrastructures subject natural disasters predict foundation settlements anns also used building black box models geoscience hydrology ocean modelling coastal engineering geomorphology anns employed cybersecurity objective discriminate legitimate activities malicious ones example machine learning used classifying android malware identifying domains belonging threat actors detecting urls posing security risk research underway ann systems designed penetration testing detecting botnets credit cards frauds network intrusions anns proposed tool simulate properties many body open quantum systems brain research anns studied short term behavior individual neurons dynamics neural circuitry arise interactions individual neurons behavior arise abstract neural modules represent complete subsystems studies considered long short term plasticity neural systems relation learning memory individual neuron system level multilayer perceptron universal function approximator proven universal approximation theorem however proof constructive regarding number neurons required network topology weights learning parameters specific recurrent architecture rational valued weights power universal turing machine using finite number neurons standard linear connections use irrational values weights results machine super turing power model capacity property corresponds ability model given function related amount information stored network notion complexity two notions capacity known community information capacity vc dimension information capacity perceptron intensively discussed sir david mackay book summarizes work thomas cover capacity network standard neurons derived four rules derive understanding neuron electrical element information capacity captures functions modelable network given data input second notion vc dimension vc dimension uses principles measure theory finds maximum capacity best possible circumstances given input data specific form noted vc dimension arbitrary inputs half information capacity perceptron vc dimension arbitrary points sometimes referred memory capacity models may consistently converge single solution firstly local minima may exist depending cost function model secondly optimization method used might guarantee converge begins far local minimum thirdly sufficiently large data parameters methods become impractical convergence behavior certain types ann architectures understood others width network approaches infinity ann well described first order taylor expansion throughout training inherits convergence behavior affine models another example parameters small observed anns often fits target functions low high frequencies phenomenon opposite behavior well studied iterative numerical schemes jacobi method applications whose goal create system generalizes well unseen examples face possibility training arises convoluted specified systems network capacity significantly exceeds needed free parameters two approaches address training first use cross validation similar techniques check presence training select hyperparameters minimize generalization error second use form regularization concept emerges probabilistic framework regularization performed selecting larger prior probability simpler models also statistical learning theory goal minimize two quantities empirical risk structural risk roughly corresponds error training set predicted error unseen data due overfitting supervised neural networks use mean squared error cost function use formal statistical methods determine confidence trained model mse validation set used estimate variance value used calculate confidence interval network output assuming normal distribution confidence analysis made way statistically valid long output probability distribution stays network modified assigning softmax activation function generalization logistic function output layer neural network categorical target variables outputs interpreted posterior probabilities useful classification gives certainty measure classifications softmax activation function common criticism neural networks particularly robotics require much training real world operation citation needed potential solutions include randomly shuffling training examples using numerical optimization algorithm take large steps changing network connections following example grouping examples called mini batches introducing recursive least squares algorithm cmac fundamental objection anns sufficiently reflect neuronal function backpropagation critical step although mechanism exists biological neural networks information coded real neurons known sensor neurons fire action potentials frequently sensor activation muscle cells pull strongly associated motor neurons receive action potentials frequently case relaying information sensor neuron motor neuron almost nothing principles information handled biological neural networks known central claim anns embody new powerful general principles processing information unfortunately principles ill defined often claimed emergent network allows simple statistical association described learning recognition alexander dewdney commented result artificial neural networks something nothing quality one imparts peculiar aura laziness distinct lack curiosity good computing systems human hand intervenes solutions found magic one seems learned anything one response dewdney neural networks handle many complex diverse tasks ranging autonomously flying aircraft detecting credit card fraud mastering game go technology writer roger bridgman commented neural networks instance dock hyped high heaven also could create successful net without understanding worked bunch numbers captures behaviour would probability opaque unreadable table valueless scientific resource spite emphatic declaration science technology dewdney seems pillory neural nets bad science devising trying good engineers unreadable table useful machine could read would still well worth biological brains use shallow deep circuits reported brain anatomy displaying wide variety invariance weng argued brain self wires largely according signal statistics therefore serial cascade cannot catch major statistical dependencies large effective neural networks require considerable computing resources brain hardware tailored task processing signals graph neurons simulating even simplified neuron von neumann architecture may consume vast amounts memory storage furthermore designer often needs transmit signals many connections associated neurons require enormous cpu power time schmidhuber noted resurgence neural networks twenty first century largely attributable advances hardware computing power especially delivered gpgpus increased around million fold making standard backpropagation algorithm feasible training networks several layers deeper use accelerators fpgas gpus reduce training times months days neuromorphic engineering addresses hardware difficulty directly constructing non von neumann chips directly implement neural networks circuitry another type chip optimized neural network processing called tensor processing unit tpu analyzing learned ann much easier analyze learned biological neural network furthermore researchers involved exploring learning algorithms neural networks gradually uncovering general principles allow learning machine successful example local vs non local learning shallow vs deep architecture advocates hybrid models claim mixture better capture mechanisms human mind single layer feedforward artificial neural network arrows originating x displaystyle scriptstyle x omitted clarity p inputs network q outputs system value qth output q displaystyle scriptstyle q would calculated q k b q displaystyle scriptstyle q k b q two layer feedforward artificial neural network artificial neural network ann dependency graph single layer feedforward artificial neural network inputs hidden outputs given position state direction outputs wheel based control values two layer feedforward artificial neural network inputs x hidden outputs given position state direction environment values outputs thruster based control values parallel pipeline structure cmac neural network learning algorithm converge one step
https://en.wikipedia.org/wiki/Logistic_regression,statistics logistic model used model probability certain class event existing pass fail win lose alive dead healthy sick extended model several classes events determining whether image contains cat dog lion etc object detected image would assigned probability sum one logistic regression statistical model basic form uses logistic function model binary dependent variable although many complex extensions exist regression analysis logistic regression estimating parameters logistic model mathematically binary logistic model dependent variable two possible values pass fail represented indicator variable two values labeled logistic model log odds value labeled linear combination one independent variables independent variables binary variable continuous variable corresponding probability value labeled vary hence labeling function converts log odds probability logistic function hence name unit measurement log odds scale called logit logistic unit hence alternative names analogous models different sigmoid function instead logistic function also used probit model defining characteristic logistic model increasing one independent variables multiplicatively scales odds given outcome constant rate independent variable parameter binary dependent variable generalizes odds ratio binary logistic regression model dependent variable two levels outputs two values modeled multinomial logistic regression multiple categories ordered ordinal logistic regression logistic regression model simply models probability output terms input perform statistical classification though used make classifier instance choosing cutoff value classifying inputs probability greater cutoff one class cutoff common way make binary classifier coefficients generally computed closed form expression unlike linear least squares see model fitting logistic regression general statistical model originally developed popularized primarily joseph berkson beginning berkson harvtxt error target citerefberkson coined logit see history logistic regression used various fields including machine learning medical fields social sciences example trauma injury severity score widely used predict mortality injured patients originally developed boyd et al using logistic regression many medical scales used assess severity patient developed using logistic regression logistic regression may used predict risk developing given disease based observed characteristics patient another example might predict whether nepalese voter vote nepali congress communist party nepal party based age income sex race state residence votes previous elections etc technique also used engineering especially predicting probability failure given process system product also used marketing applications prediction customer propensity purchase product halt subscription etc economics used predict likelihood person choosing labor force business application would predict likelihood homeowner defaulting mortgage conditional random fields extension logistic regression sequential data used natural language processing let us try understand logistic regression considering logistic model given parameters seeing coefficients estimated data consider model two predictors x displaystyle x x displaystyle x one binary response variable displaystyle denote p p displaystyle p p assume linear relationship predictor variables log odds event displaystyle linear relationship written following mathematical form recover odds exponentiating log odds simple algebraic manipulation probability displaystyle formula shows displaystyle beta fixed easily compute either log odds displaystyle given observation probability displaystyle given observation main use case logistic model given observation displaystyle estimate probability p displaystyle p displaystyle applications base b displaystyle b logarithm usually taken e however cases easier communicate results working base base consider example b displaystyle b coefficients displaystyle beta displaystyle beta displaystyle beta concrete model p displaystyle p probability event displaystyle interpreted follows order estimate parameters displaystyle beta data one must logistic regression answer following question group students spends hours studying exam number hours spent studying affect probability student passing exam reason using logistic regression problem values dependent variable pass fail represented cardinal numbers problem changed pass fail replaced grade simple regression analysis could used table shows number hours student spent studying whether passed failed graph shows probability passing exam versus number hours studying logistic regression curve fitted data logistic regression analysis gives following output output indicates hours studying significantly associated probability passing exam output also provides coefficients intercept displaystyle text intercept hours displaystyle text hours coefficients entered logistic regression equation estimate odds passing exam one additional hour study estimated increase log odds passing multiplying odds passing exp displaystyle exp approx form x intercept shows estimates even odds student studies hours example student studies hours entering value hours displaystyle text hours equation gives estimated probability passing exam similarly student studies hours estimated probability passing exam table shows probability passing exam several values hours studying output logistic regression analysis gives p value p displaystyle p based wald z score rather wald method recommended method citation needed calculate p value logistic regression likelihood ratio test data gives p displaystyle p logistic regression binomial ordinal multinomial binomial binary logistic regression deals situations observed outcome dependent variable two possible types multinomial logistic regression deals situations outcome three possible types ordered ordinal logistic regression deals dependent variables ordered binary logistic regression outcome usually coded leads straightforward interpretation particular observed outcome dependent variable noteworthy possible outcome usually coded contrary outcome binary logistic regression used predict odds case based values independent variables odds defined probability particular outcome case divided probability noninstance like forms regression analysis logistic regression makes use one predictor variables may either continuous categorical unlike ordinary linear regression however logistic regression used predicting dependent variables take membership one limited number categories rather continuous outcome given difference assumptions linear regression violated particular residuals cannot normally distributed addition linear regression may make nonsensical predictions binary dependent variable needed way convert binary variable continuous one take real value binomial logistic regression first calculates odds event happening different levels independent variable takes logarithm create continuous criterion transformed version dependent variable logarithm odds logit probability logit defined follows although dependent variable logistic regression bernoulli logit unrestricted scale logit function link function kind generalized linear model e bernoulli distributed response variable x predictor variable values linear parameters logit probability success fitted predictors predicted value logit converted back predicted odds via inverse natural logarithm exponential function thus although observed dependent variable binary logistic regression variable logistic regression estimates odds continuous variable dependent variable success applications odds needed others specific yes prediction needed whether dependent variable success categorical prediction based computed odds success predicted odds chosen cutoff value translated prediction success assumption linear predictor effects easily relaxed using techniques spline functions logistic regression measures relationship categorical dependent variable one independent variables estimating probabilities using logistic function cumulative distribution function logistic distribution thus treats set problems probit regression using similar techniques latter using cumulative normal distribution curve instead equivalently latent variable interpretations two methods first assumes standard logistic distribution errors second standard normal distribution errors logistic regression seen special case generalized linear model thus analogous linear regression model logistic regression however based quite different assumptions linear regression particular key differences two models seen following two features logistic regression first conditional distribution x displaystyle mid x bernoulli distribution rather gaussian distribution dependent variable binary second predicted values probabilities therefore restricted logistic distribution function logistic regression predicts probability particular outcomes rather outcomes logistic regression alternative fisher method linear discriminant analysis assumptions linear discriminant analysis hold conditioning reversed produce logistic regression converse true however logistic regression require multivariate normal assumption discriminant analysis logistic regression understood simply finding displaystyle beta parameters best fit displaystyle varepsilon error distributed standard logistic distribution associated latent variable x displaystyle beta beta x varepsilon error term displaystyle varepsilon observed displaystyle also unobservable hence termed latent unlike ordinary regression however displaystyle beta parameters cannot expressed direct formula displaystyle x displaystyle x values observed data instead found iterative search process usually implemented software program finds maximum complicated likelihood expression function observed displaystyle x displaystyle x values estimation approach explained explanation logistic regression begin explanation standard logistic function logistic function sigmoid function takes real input displaystyle outputs value zero one logit interpreted taking input log odds output probability standard logistic function r displaystyle sigma mathbb r rightarrow defined follows graph logistic function interval shown figure let us assume displaystyle linear function single explanatory variable x displaystyle x express displaystyle follows general logistic function p r displaystyle p mathbb r rightarrow written logistic model p displaystyle p interpreted probability dependent variable displaystyle equaling success case rather failure non case clear response variables displaystyle identically distributed p displaystyle p differs one data point x displaystyle x another though independent given design matrix x displaystyle x shared parameters displaystyle beta define logit function inverse g displaystyle g sigma standard logistic function easy see satisfies equivalently exponentiating sides odds equations terms follows odds dependent variable equaling case equivalent exponential function linear regression expression illustrates logit serves link function probability linear regression expression given logit ranges negative positive infinity provides adequate criterion upon conduct linear regression logit easily converted back odds define odds dependent variable equaling case follows continuous independent variable odds ratio defined exponential relationship provides interpretation displaystyle beta odds multiply e displaystyle e beta every unit increase x binary independent variable odds ratio defined b c displaystyle frac ad bc b c cells contingency table multiple explanatory variables expression x displaystyle beta beta x revised x x x x displaystyle beta beta x beta x cdots beta x beta sum beta x used equation relating log odds success values predictors linear regression multiple regression explanators parameters j displaystyle beta j j estimated traditional equations usually b e displaystyle b e logistic regression important machine learning algorithm goal model probability random variable displaystyle given experimental data consider generalized linear model function parameterized displaystyle theta therefore since displaystyle see pr displaystyle pr given pr h displaystyle pr h theta calculate likelihood function assuming observations sample independently bernoulli distributed typically log likelihood maximized maximized using optimization techniques gradient descent assuming displaystyle pairs drawn uniformly underlying distribution limit large n h displaystyle h conditional entropy kl displaystyle text kl kullback leibler divergence leads intuition maximizing log likelihood model minimizing kl divergence model maximal entropy distribution intuitively searching model makes fewest assumptions parameters widely used rule thumb one ten rule states logistic regression models give stable values explanatory variables based minimum events per explanatory variable event denotes cases belonging less frequent category dependent variable thus study designed use k displaystyle k explanatory variables event expected occur proportion p displaystyle p participants study require total k p displaystyle k p participants however considerable debate reliability rule based simulation studies lacks secure theoretical underpinning according authors rule overly conservative circumstances authors stating regard confidence interval coverage less percent type error greater percent relative bias greater percent problematic results indicate problems fairly frequent epv uncommon epv still observed epv worst instances problem severe epv usually comparable epv others found results consistent using different criteria useful criterion whether fitted model expected achieve predictive discrimination new sample appeared achieve model development sample criterion events per candidate variable may required also one argue observations needed estimate model intercept precisely enough margin error predicted probabilities confidence level regression coefficients usually estimated using maximum likelihood estimation unlike linear regression normally distributed residuals possible find closed form expression coefficient values maximize likelihood function iterative process must used instead example newton method process begins tentative solution revises slightly see improved repeats revision improvement made point process said converged instances model may reach convergence non convergence model indicates coefficients meaningful iterative process unable find appropriate solutions failure converge may occur number reasons large ratio predictors cases multicollinearity sparseness complete separation machine learning applications logistic regression used binary classification mle minimises cross entropy loss function binary logistic regression example calculated using iteratively reweighted least squares equivalent maximizing log likelihood bernoulli distributed process using newton method problem written vector matrix form parameters w displaystyle mathbf w beta beta beta ldots explanatory variables x x x displaystyle mathbf x x x ldots expected value bernoulli distribution e w x displaystyle mu frac e mathbf w mathbf x parameters w displaystyle mathbf w found using following iterative algorithm diag displaystyle mathbf operatorname diag diagonal weighting matrix displaystyle boldsymbol mu mu mu ldots vector expected values regressor matrix displaystyle mathbf ldots vector response variables details found literature goodness fit linear regression models generally measured using r since direct analog logistic regression various methods ch including following used instead linear regression analysis one concerned partitioning variance via sum squares calculations variance criterion essentially divided variance accounted predictors residual variance logistic regression analysis deviance used lieu sum squares calculations deviance analogous sum squares calculations linear regression measure lack fit data logistic regression model saturated model available deviance calculated comparing given model saturated model computation gives likelihood ratio test equation represents deviance ln represents natural logarithm log likelihood ratio produce negative value hence need negative sign shown follow approximate chi squared distribution smaller values indicate better fit fitted model deviates less saturated model assessed upon chi square distribution nonsignificant chi square values indicate little unexplained variance thus good model fit conversely significant chi square value indicates significant amount variance unexplained saturated model available deviance calculated simply reference saturated model log likelihood removed follows without harm two measures deviance particularly important logistic regression null deviance model deviance null deviance represents difference model intercept saturated model model deviance represents difference model least one predictor saturated model respect null model provides baseline upon compare predictor models given deviance measure difference given model saturated model smaller values indicate better fit thus assess contribution predictor set predictors one subtract model deviance null deviance assess difference p displaystyle chi p chi square distribution degrees freedom equal difference number parameters estimated let difference model deviance significantly smaller null deviance one conclude predictor set predictors significantly improved model fit analogous f test used linear regression analysis assess significance prediction linear regression squared multiple correlation r used assess goodness fit represents proportion variance criterion explained predictors logistic regression analysis agreed upon analogous measure several competing measures limitations four commonly used indices one less commonly used one examined page r l given cohen analogous index squared multiple correlations linear regression represents proportional reduction deviance wherein deviance treated measure variation analogous identical variance linear regression analysis one limitation likelihood ratio r monotonically related odds ratio meaning necessarily increase odds ratio increases necessarily decrease odds ratio decreases r cs alternative index goodness fit related r value linear regression given lm mvar l likelihoods model fitted null model respectively cox snell index problematic maximum value l n displaystyle l n highest upper bound easily low marginal proportion cases small r n provides correction cox snell r maximum value equal nevertheless cox snell likelihood ratio r show greater agreement either nagelkerke r course might case values exceeding cox snell index capped value likelihood ratio r often preferred alternatives analogous r linear regression independent base rate varies r mcf defined preferred r cs allison two expressions r mcf r cs related respectively however allison prefers r relatively new measure developed tjur calculated two steps word caution order interpreting pseudo r statistics reason indices fit referred pseudo r represent proportionate reduction error r linear regression linear regression assumes homoscedasticity error variance values criterion logistic regression always heteroscedastic error variances differ value predicted score value predicted score would different value proportionate reduction error therefore inappropriate think r proportionate reduction error universal sense logistic regression hosmer lemeshow test uses test statistic asymptotically follows displaystyle chi distribution assess whether observed event rates match expected event rates subgroups model population test considered obsolete statisticians dependence arbitrary binning predicted probabilities relative low power fitting model likely researchers want examine contribution individual predictors want examine regression coefficients linear regression regression coefficients represent change criterion unit change predictor logistic regression however regression coefficients represent change logit unit change predictor given logit intuitive researchers likely focus predictor effect exponential function regression coefficient odds ratio linear regression significance regression coefficient assessed computing test logistic regression several different tests designed assess significance individual predictor notably likelihood ratio test wald statistic likelihood ratio test discussed assess model fit also recommended procedure assess contribution individual predictors given model case single predictor model one simply compares deviance predictor model null model chi square distribution single degree freedom predictor model significantly smaller deviance one conclude significant association predictor outcome although common statistical packages provide likelihood ratio test statistics without computationally intensive test would difficult assess contribution individual predictors multiple logistic regression case citation needed assess contribution individual predictors one enter predictors hierarchically comparing new model previous determine contribution predictor debate among statisticians appropriateness called stepwise procedures weasel words fear may preserve nominal statistical properties may become misleading alternatively assessing contribution individual predictors given model one may examine significance wald statistic wald statistic analogous test linear regression used assess significance coefficients wald statistic ratio square regression coefficient square standard error coefficient asymptotically distributed chi square distribution although several statistical packages report wald statistic assess contribution individual predictors wald statistic limitations regression coefficient large standard error regression coefficient also tends larger increasing probability type ii error wald statistic also tends biased data sparse suppose cases rare might wish sample frequently prevalence population example suppose disease affects person collect data need complete physical may expensive thousands physicals healthy people order obtain data diseased individuals thus may evaluate diseased individuals perhaps rare outcomes also retrospective sampling equivalently called unbalanced data rule thumb sampling controls rate five times number cases produce sufficient control data logistic regression unique may estimated unbalanced data rather randomly sampled data still yield correct coefficient estimates effects independent variable outcome say form logistic model data model correct general population j displaystyle beta j parameters correct except displaystyle beta correct displaystyle beta know true prevalence follows displaystyle pi true prevalence displaystyle tilde pi prevalence sample various equivalent specifications logistic regression fit different types general models different specifications allow different sorts useful generalizations basic setup logistic regression follows given dataset containing n points point consists set input variables x xm binary outcome variable yi e assume two possible values goal logistic regression use dataset create predictive model outcome variable examples linear regression outcome variables yi assumed depend explanatory variables x xm shown examples explanatory variables may type real valued binary categorical etc main distinction continuous variables discrete variables discrete variables referring two possible choices typically coded using dummy variables separate explanatory variables taking value created possible value discrete variable meaning variable given value meaning variable value example four way discrete variable blood type possible values b ab converted four separate two way dummy variables b ab one value rest value allows separate regression coefficients matched possible value discrete variable formally outcomes yi described bernoulli distributed data outcome determined unobserved probability pi specific outcome hand related explanatory variables expressed following equivalent forms meanings four lines basic idea logistic regression use mechanism already developed linear regression modeling probability pi using linear predictor function e linear combination explanatory variables set regression coefficients specific model hand trials linear predictor function f displaystyle f particular data point written displaystyle beta ldots beta regression coefficients indicating relative effect particular explanatory variable outcome model usually put compact form follows makes possible write linear predictor function follows using notation dot product two vectors particular model used logistic regression distinguishes standard linear regression types regression analysis used binary valued outcomes way probability particular outcome linked linear predictor function written using compact notation described formulation expresses logistic regression type generalized linear model predicts variables various types probability distributions fitting linear predictor function form sort arbitrary transformation expected value variable intuition transforming using logit function explained also practical effect converting probability variable ranges displaystyle thereby matching potential range linear prediction function right side equation note probabilities pi regression coefficients unobserved means determining part model typically determined sort optimization procedure e g maximum likelihood estimation finds values best fit observed data usually subject regularization conditions seek exclude unlikely values e g extremely large values regression coefficients use regularization condition equivalent maximum posteriori estimation extension maximum likelihood whether regularization used usually possible find closed form solution instead iterative numerical method must used iteratively reweighted least squares commonly days quasi newton method l bfgs method interpretation j parameter estimates additive effect log odds unit change j explanatory variable case dichotomous explanatory variable instance gender e displaystyle e beta estimate odds outcome say males compared females equivalent formula uses inverse logit function logistic function e formula also written probability distribution model equivalent formulation latent variable model formulation common theory discrete choice models makes easier extend certain complicated models multiple correlated choices well compare logistic regression closely related probit model imagine trial continuous latent variable yi distributed follows e latent variable written directly terms linear predictor function additive random error variable distributed according standard logistic distribution yi viewed indicator whether latent variable positive choice modeling error variable specifically standard logistic distribution rather general logistic distribution location scale set arbitrary values seems restrictive fact must kept mind choose regression coefficients often use offset changes parameters error variable distribution example logistic error variable distribution non zero location parameter equivalent distribution zero location parameter added intercept coefficient situations produce value yi regardless settings explanatory variables similarly arbitrary scale parameter equivalent setting scale parameter dividing regression coefficients latter case resulting value yi smaller factor former case sets explanatory variables critically always remain side hence lead yi choice turns formulation exactly equivalent preceding one phrased terms generalized linear model without latent variables shown follows using fact cumulative distribution function standard logistic distribution logistic function inverse logit function e formulation standard discrete choice models makes clear relationship logistic regression probit model uses error variable distributed according standard normal distribution instead standard logistic distribution logistic normal distributions symmetric basic unimodal bell curve shape difference logistic distribution somewhat heavier tails means less sensitive outlying data yet another formulation uses two separate latent variables ev standard type extreme value distribution e model separate latent variable separate set regression coefficients possible outcome dependent variable reason separation makes easy extend logistic regression multi outcome categorical variables multinomial logit model model natural model possible outcome using different set regression coefficients also possible motivate separate latent variables theoretical utility associated making associated choice thus motivate logistic regression terms utility theory approach taken economists formulating discrete choice models provides theoretically strong foundation facilitates intuitions model turn makes easy consider various sorts extensions choice type extreme value distribution seems fairly arbitrary makes mathematics work may possible justify use rational choice theory turns model equivalent previous model although seems non obvious since two sets regression coefficients error variables error variables different distribution fact model reduces directly previous one following substitutions intuition comes fact since choose based maximum two values difference matters exact values effectively removes one degree freedom another critical fact difference two type extreme value distributed variables logistic distribution e logistic displaystyle varepsilon varepsilon varepsilon sim operatorname logistic demonstrate equivalent follows example consider province level election choice right center party left center party secessionist party would use three latent variables one choice accordance utility theory interpret latent variables expressing utility results making choices also interpret regression coefficients indicating strength associated factor contributing utility correctly amount unit change explanatory variable changes utility given choice voter might expect right center party would lower taxes especially rich people would give low income people benefit e change utility would cause moderate benefit middle incoming people would cause significant benefits high income people hand left center party might expected raise taxes offset increased welfare assistance lower middle classes would cause significant positive benefit low income people perhaps weak benefit middle income people significant negative benefit high income people finally secessionist party would take direct actions economy simply secede low income middle income voter might expect basically clear utility gain loss high income voter might expect negative utility since likely companies harder time business environment probably lose money intuitions expressed follows clearly shows yet another formulation combines two way latent variable formulation original formulation higher without latent variables process provides link one standard formulations multinomial logit instead writing logit probabilities pi linear predictor separate linear predictor two one two outcomes note two separate sets regression coefficients introduced two way latent variable model two equations appear form writes logarithm associated probability linear predictor extra term ln z displaystyle ln z end term turns serves normalizing factor ensuring result distribution seen exponentiating sides form clear purpose z ensure resulting distribution yi fact probability distribution e sums means z simply sum un normalized probabilities dividing probability z probabilities become normalized resulting equations generally shows clearly generalize formulation two outcomes multinomial logit note general formulation exactly softmax function order prove equivalent previous model note model overspecified pr displaystyle pr pr displaystyle pr cannot independently specified rather pr pr displaystyle pr pr knowing one automatically determines result model nonidentifiable multiple combinations produce probabilities possible explanatory variables fact seen adding constant vector produce probabilities result simplify matters restore identifiability picking arbitrary value one two vectors choose set displaystyle boldsymbol beta mathbf shows formulation indeed equivalent previous formulation note treatments multinomial logit model start either extending log linear formulation presented two way latent variable formulation presented since clearly show way model could extended multi way outcomes general presentation latent variables common econometrics political science discrete choice models utility theory reign log linear formulation common computer science e g machine learning natural language processing model equivalent formulation functional form commonly called single layer perceptron single layer artificial neural network single layer neural network computes continuous output instead step function derivative pi respect x computed general form f analytic function x choice single layer neural network identical logistic regression model function continuous derivative allows used backpropagation function also preferred derivative easily calculated closely related model assumes associated single bernoulli trial ni independent identically distributed trials observation yi number successes observed hence follows binomial distribution example distribution fraction seeds germinate ni planted terms expected values model expressed follows equivalently model fit using sorts methods basic model bayesian statistics context prior distributions normally placed regression coefficients usually form gaussian distributions conjugate prior likelihood function logistic regression bayesian inference performed analytically made posterior distribution difficult calculate except low dimensions though automatic software openbugs jags pymc stan allows posteriors computed using simulation lack conjugacy concern however sample size number parameters large full bayesian simulation slow people often use approximate methods variational bayesian methods expectation propagation detailed history logistic regression given cramer logistic function developed model population growth named logistic pierre fran ois verhulst guidance adolphe quetelet see logistic function history details earliest paper verhulst specify fit curves data detailed paper verhulst determined three parameters model making curve pass three observed points yielded poor predictions logistic function independently developed chemistry model autocatalysis autocatalytic reaction one one products catalyst reaction supply one reactants fixed naturally gives rise logistic equation reason population growth reaction self reinforcing constrained logistic function independently rediscovered model population growth raymond pearl lowell reed published pearl reed harvtxt error target citerefpearlreed led use modern statistics initially unaware verhulst work presumably learned l gustave du pasquier gave little credit adopt terminology verhulst priority acknowledged term logistic revived udny yule followed since pearl reed first applied model population united states also initially fitted curve making pass three points verhulst yielded poor results probit model developed systematized chester ittner bliss coined term probit bliss harvtxt error target citerefbliss john gaddum gaddum harvtxt error target citerefgaddum model fit maximum likelihood estimation ronald fisher fisher harvtxt error target citereffisher addendum bliss work probit model principally used bioassay preceded earlier work dating see probit model history probit model influenced subsequent development logit model models competed logistic model likely first used alternative probit model bioassay edwin bidwell wilson student jane worcester wilson worcester however development logistic model general alternative probit model principally due work joseph berkson many decades beginning berkson harvtxt error target citerefberkson coined logit analogy probit continuing berkson harvtxt error target citerefberkson following years logit model initially dismissed inferior probit model gradually achieved equal footing logit particularly logit model achieved parity probit model use statistics journals thereafter surpassed relative popularity due adoption logit outside bioassay rather displacing probit within bioassay informal use practice logit popularity credited logit model computational simplicity mathematical properties generality allowing use varied fields various refinements occurred time notably david cox cox multinomial logit model introduced independently cox thiel greatly increased scope application popularity logit model daniel mcfadden linked multinomial logit theory discrete choice specifically luce choice axiom showing multinomial logit followed assumption independence irrelevant alternatives interpreting odds alternatives relative preferences gave theoretical foundation logistic regression large numbers extensions statistical software binary logistic regression notably microsoft excel statistics extension package include
https://en.wikipedia.org/wiki/Perceptron,machine learning perceptron algorithm supervised learning binary classifiers binary classifier function decide whether input represented vector numbers belongs specific class type linear classifier e classification algorithm makes predictions based linear predictor function combining set weights feature vector perceptron algorithm invented cornell aeronautical laboratory frank rosenblatt funded united states office naval research perceptron intended machine rather program first implementation software ibm subsequently implemented custom built hardware mark perceptron machine designed image recognition array photocells randomly connected neurons weights encoded potentiometers weight updates learning performed electric motors press conference organized us navy rosenblatt made statements perceptron caused heated controversy among fledgling ai community based rosenblatt statements new york times reported perceptron embryo electronic computer navy expects able walk talk see write reproduce conscious existence although perceptron initially seemed promising quickly proved perceptrons could trained recognise many classes patterns caused field neural network research stagnate many years recognised feedforward neural network two layers greater processing power perceptrons one layer single layer perceptrons capable learning linearly separable patterns classification task step activation function single node single line dividing data points forming patterns nodes create dividing lines lines must somehow combined form complex classifications second layer perceptrons even linear nodes sufficient solve lot otherwise non separable problems famous book entitled perceptrons marvin minsky seymour papert showed impossible classes network learn xor function often believed also conjectured similar result would hold multi layer perceptron network however true minsky papert already knew multi layer perceptrons capable producing xor function information nevertheless often miscited minsky papert text caused significant decline interest funding neural network research took ten years neural network research experienced resurgence text reprinted perceptrons expanded edition errors original text shown corrected kernel perceptron algorithm already introduced aizerman et al margin bounds guarantees given perceptron algorithm general non separable case first freund schapire recently mohri rostamizadeh extend previous results give new l bounds perceptron simplified model biological neuron complexity biological neuron models often required fully understand neural behavior research suggests perceptron like linear model produce behavior seen real neurons modern sense perceptron algorithm learning binary classifier called threshold function function maps input x displaystyle mathbf x output value f displaystyle f w displaystyle mathbf w vector real valued weights w x displaystyle mathbf w cdot mathbf x dot product w x displaystyle sum w x number inputs perceptron b bias bias shifts decision boundary away origin depend input value value f displaystyle f used classify x displaystyle mathbf x either positive negative instance case binary classification problem b negative weighted combination inputs must produce positive value greater b displaystyle b order push classifier neuron threshold spatially bias alters position decision boundary perceptron learning algorithm terminate learning set linearly separable vectors linearly separable learning never reach point vectors classified properly famous example perceptron inability solve problems linearly nonseparable vectors boolean exclusive problem solution spaces decision boundaries binary functions learning behaviors studied reference context neural networks perceptron artificial neuron using heaviside step function activation function perceptron algorithm also termed single layer perceptron distinguish multilayer perceptron misnomer complicated neural network linear classifier single layer perceptron simplest feedforward neural network example learning algorithm single layer perceptron multilayer perceptrons hidden layer exists sophisticated algorithms backpropagation must used activation function underlying process modeled perceptron nonlinear alternative learning algorithms delta rule used long activation function differentiable nonetheless learning algorithm described steps often work even multilayer perceptrons nonlinear activation functions multiple perceptrons combined artificial neural network output neuron operates independently others thus learning output considered isolation first define variables show values features follows represent weights show time dependence w displaystyle mathbf w use algorithm updates weights steps b weights immediately applied pair training set subsequently updated rather waiting pairs training set undergone steps perceptron linear classifier therefore never get state input vectors classified correctly training set linearly separable e positive examples cannot separated negative examples hyperplane case approximate solution gradually approached standard learning algorithm instead learning fail completely hence linear separability training set known priori one training variants used training set linearly separable perceptron guaranteed converge furthermore upper bound number times perceptron adjust weights training suppose input vectors two classes separated hyperplane margin displaystyle gamma e exists weight vector w w displaystyle mathbf w mathbf w bias term b w x j displaystyle mathbf w cdot mathbf x j gamma j displaystyle j j displaystyle j w x j displaystyle mathbf w cdot mathbf x j
https://en.wikipedia.org/wiki/Relevance_vector_machine,mathematics relevance vector machine machine learning technique uses bayesian inference obtain parsimonious solutions regression probabilistic classification rvm identical functional form support vector machine provides probabilistic classification actually equivalent gaussian process model covariance function displaystyle varphi kernel function j displaystyle alpha j variances prior weight vector w n displaystyle w sim n x x n displaystyle mathbf x ldots mathbf x n input vectors training set compared support vector machines bayesian formulation rvm avoids set free parameters svm however rvms use expectation maximization like learning method therefore risk local minima unlike standard sequential minimal optimization based algorithms employed svms guaranteed find global optimum relevance vector machine patented united states microsoft
https://en.wikipedia.org/wiki/Support-vector_machine,machine learning support vector machines supervised learning models associated learning algorithms analyze data used classification regression analysis developed bell laboratories vapnik colleagues presents one robust prediction methods based statistical learning framework vc theory proposed vapnik chervonenkis vapnik given set training examples marked belonging one two categories svm training algorithm builds model assigns new examples one category making non probabilistic binary linear classifier svm model representation examples points space mapped examples separate categories divided clear gap wide possible new examples mapped space predicted belong category based side gap fall addition performing linear classification svms efficiently perform non linear classification using called kernel trick implicitly mapping inputs high dimensional feature spaces data unlabelled supervised learning possible unsupervised learning approach required attempts find natural clustering data groups map new data formed groups support vector clustering algorithm created hava siegelmann vladimir vapnik applies statistics support vectors developed support vector machines algorithm categorize unlabeled data one widely used clustering algorithms industrial applications citation needed classifying data common task machine learning suppose given data points belong one two classes goal decide class new data point case support vector machines data point viewed p displaystyle p dimensional vector want know whether separate points displaystyle dimensional hyperplane called linear classifier many hyperplanes might classify data one reasonable choice best hyperplane one represents largest separation margin two classes choose hyperplane distance nearest data point side maximized hyperplane exists known maximum margin hyperplane linear classifier defines known maximum margin classifier equivalently perceptron optimal stability citation needed formally support vector machine constructs hyperplane set hyperplanes high infinite dimensional space used classification regression tasks like outliers detection intuitively good separation achieved hyperplane largest distance nearest training data point class since general larger margin lower generalization error classifier whereas original problem may stated finite dimensional space often happens sets discriminate linearly separable space reason proposed original finite dimensional space mapped much higher dimensional space presumably making separation easier space keep computational load reasonable mappings used svm schemes designed ensure dot products pairs input data vectors may computed easily terms variables original space defining terms kernel function k displaystyle k selected suit problem hyperplanes higher dimensional space defined set points whose dot product vector space constant set vectors orthogonal set vectors defines hyperplane vectors defining hyperplanes chosen linear combinations parameters displaystyle alpha images feature vectors x displaystyle x occur data base choice hyperplane points x displaystyle x feature space mapped hyperplane defined relation k constant displaystyle textstyle sum alpha k text constant note k displaystyle k becomes small displaystyle grows away x displaystyle x term sum measures degree closeness test point x displaystyle x corresponding data base point x displaystyle x way sum kernels used measure relative nearness test point data points originating one sets discriminated note fact set points x displaystyle x mapped hyperplane quite convoluted result allowing much complex discrimination sets convex original space svms used solve various real world problems original svm algorithm invented vladimir n vapnik alexey ya chervonenkis bernhard boser isabelle guyon vladimir vapnik suggested way create nonlinear classifiers applying kernel trick maximum margin hyperplanes current standard according incarnation proposed corinna cortes vapnik published given training dataset n displaystyle n points form displaystyle either indicating class point x displaystyle vec x belongs x displaystyle vec x p displaystyle p dimensional real vector want find maximum margin hyperplane divides group points x displaystyle vec x displaystyle group points displaystyle defined distance hyperplane nearest point x displaystyle vec x either group maximized hyperplane written set points x displaystyle vec x satisfying w displaystyle vec w normal vector hyperplane much like hesse normal form except w displaystyle vec w necessarily unit vector parameter b w displaystyle tfrac b vec w determines offset hyperplane origin along normal vector w displaystyle vec w training data linearly separable select two parallel hyperplanes separate two classes data distance large possible region bounded two hyperplanes called margin maximum margin hyperplane hyperplane lies halfway normalized standardized dataset hyperplanes described equations geometrically distance two hyperplanes w displaystyle tfrac vec w maximize distance planes want minimize w displaystyle vec w distance computed using distance point plane equation also prevent data points falling margin add following constraint displaystyle either constraints state data point must lie correct side margin rewritten put together get optimization problem w displaystyle vec w b displaystyle b solve problem determine classifier x sgn displaystyle vec x mapsto operatorname sgn sgn displaystyle operatorname sgn sign function important consequence geometric description max margin hyperplane completely determined x displaystyle vec x lie nearest x displaystyle vec x called support vectors extend svm cases data linearly separable hinge loss function helpful note displaystyle th target w x b displaystyle vec w cdot vec x b th output function zero constraint satisfied words x displaystyle vec x lies correct side margin data wrong side margin function value proportional distance margin goal optimization minimize parameter displaystyle lambda determines trade increasing margin size ensuring x displaystyle vec x lie correct side margin thus sufficiently small values displaystyle lambda second term loss function become negligible hence behave similar hard margin svm input data linearly classifiable still learn classification rule viable original maximum margin hyperplane algorithm proposed vapnik constructed linear classifier however bernhard boser isabelle guyon vladimir vapnik suggested way create nonlinear classifiers applying kernel trick maximum margin hyperplanes resulting algorithm formally similar except every dot product replaced nonlinear kernel function allows algorithm fit maximum margin hyperplane transformed feature space transformation may nonlinear transformed space high dimensional although classifier hyperplane transformed feature space may nonlinear original input space noteworthy working higher dimensional feature space increases generalization error support vector machines although given enough samples algorithm still performs well common kernels include kernel related transform displaystyle varphi equation k displaystyle k varphi cdot varphi value w also transformed space w displaystyle textstyle vec w sum alpha varphi dot products w classification computed kernel trick e w k displaystyle textstyle vec w cdot varphi sum alpha k computing svm classifier amounts minimizing expression form focus soft margin classifier since noted choosing sufficiently small value displaystyle lambda yields hard margin classifier linearly classifiable input data classical approach involves reducing quadratic programming problem detailed recent approaches sub gradient descent coordinate descent discussed minimizing rewritten constrained optimization problem differentiable objective function following way n displaystyle ldots n introduce variable max displaystyle zeta max left right note displaystyle zeta smallest nonnegative number satisfying displaystyle geq zeta thus rewrite optimization problem follows called primal problem solving lagrangian dual problem one obtains simplified problem called dual problem since dual maximization problem quadratic function c displaystyle c subject linear constraints efficiently solvable quadratic programming algorithms variables c displaystyle c defined moreover c displaystyle c exactly x displaystyle vec x lies correct side margin c displaystyle
https://en.wikipedia.org/wiki/Cluster_analysis,cluster analysis clustering task grouping set objects way objects group similar groups main task exploratory data mining common technique statistical data analysis used many fields including pattern recognition image analysis information retrieval bioinformatics data compression computer graphics machine learning cluster analysis one specific algorithm general task solved achieved various algorithms differ significantly understanding constitutes cluster efficiently find popular notions clusters include groups small distances cluster members dense areas data space intervals particular statistical distributions clustering therefore formulated multi objective optimization problem appropriate clustering algorithm parameter settings depend individual data set intended use results cluster analysis automatic task iterative process knowledge discovery interactive multi objective optimization involves trial failure often necessary modify data preprocessing model parameters result achieves desired properties besides term clustering number terms similar meanings including automatic classification numerical taxonomy botryology typological analysis community detection subtle differences often use results data mining resulting groups matter interest automatic classification resulting discriminative power interest cluster analysis originated anthropology driver kroeber introduced psychology joseph zubin robert tryon famously used cattell beginning trait theory classification personality psychology notion cluster cannot precisely defined one reasons many clustering algorithms common denominator group data objects however different researchers employ different cluster models cluster models different algorithms given notion cluster found different algorithms varies significantly properties understanding cluster models key understanding differences various algorithms typical cluster models include clustering essentially set clusters usually containing objects data set additionally may specify relationship clusters example hierarchy clusters embedded clusterings roughly distinguished also finer distinctions possible example listed clustering algorithms categorized based cluster model following overview list prominent examples clustering algorithms possibly published clustering algorithms provide models clusters thus easily categorized overview algorithms explained wikipedia found list statistics algorithms objectively correct clustering algorithm noted clustering eye beholder appropriate clustering algorithm particular problem often needs chosen experimentally unless mathematical reason prefer one cluster model another algorithm designed one kind model generally fail data set contains radically different kind model example k means cannot find non convex clusters connectivity based clustering also known hierarchical clustering based core idea objects related nearby objects objects farther away algorithms connect objects form clusters based distance cluster described largely maximum distance needed connect parts cluster different distances different clusters form represented using dendrogram explains common name hierarchical clustering comes algorithms provide single partitioning data set instead provide extensive hierarchy clusters merge certain distances dendrogram axis marks distance clusters merge objects placed along x axis clusters mix connectivity based clustering whole family methods differ way distances computed apart usual choice distance functions user also needs decide linkage criterion use popular choices known single linkage clustering complete linkage clustering upgma wpgma furthermore hierarchical clustering agglomerative divisive methods produce unique partitioning data set hierarchy user still needs choose appropriate clusters robust towards outliers either show additional clusters even cause clusters merge general case complexity displaystyle mathcal agglomerative clustering displaystyle mathcal divisive clustering makes slow large data sets special cases optimal efficient methods displaystyle mathcal known slink single linkage clink complete linkage clustering data mining community methods recognized theoretical foundation cluster analysis often considered obsolete citation needed however provide inspiration many later methods density based clustering single linkage gaussian data clusters biggest cluster starts fragmenting smaller parts still connected second largest due single link effect single linkage density based clusters clusters extracted contain single elements since linkage clustering notion noise centroid based clustering clusters represented central vector may necessarily member data set number clusters fixed k k means clustering gives formal definition optimization problem find k cluster centers assign objects nearest cluster center squared distances cluster minimized optimization problem known np hard thus common approach search approximate solutions particularly well known approximate method lloyd algorithm often referred k means algorithm however find local optimum commonly run multiple times different random initializations variations k means often include optimizations choosing best multiple runs also restricting centroids members data set choosing medians choosing initial centers less randomly allowing fuzzy cluster assignment k means type algorithms require number clusters k specified advance considered one biggest drawbacks algorithms furthermore algorithms prefer clusters approximately similar size always assign object nearest centroid often leads incorrectly cut borders clusters k means number interesting theoretical properties first partitions data space structure known voronoi diagram second conceptually close nearest neighbor classification popular machine learning third seen variation model based clustering lloyd algorithm variation expectation maximization algorithm model discussed k means separates data voronoi cells assumes equal sized clusters k means cannot represent density based clusters centroid based clustering problems k means k medoids special cases uncapacitated metric facility location problem canonical problem operations research computational geometry communities basic facility location problem task find best warehouse locations optimally service given set consumers one may view warehouses cluster centroids consumer locations data clustered makes possible apply well developed algorithmic solutions facility location literature presently considered centroid based clustering problem clustering model closely related statistics based distribution models clusters easily defined objects belonging likely distribution convenient property approach closely resembles way artificial data sets generated sampling random objects distribution theoretical foundation methods excellent suffer one key problem known overfitting unless constraints put model complexity complex model usually able explain data better makes choosing appropriate model complexity inherently difficult one prominent method known gaussian mixture models data set usually modeled fixed number gaussian distributions initialized randomly whose parameters iteratively optimized better fit data set converge local optimum multiple runs may produce different results order obtain hard clustering objects often assigned gaussian distribution likely belong soft clusterings necessary distribution based clustering produces complex models clusters capture correlation dependence attributes however algorithms put extra burden user many real data sets may concisely defined mathematical model gaussian distributed data em works well since uses gaussians modelling clusters density based clusters cannot modeled using gaussian distributions density based clustering clusters defined areas higher density remainder data set objects sparse areas required separate clusters usually considered noise border points popular density based clustering method dbscan contrast many newer methods features well defined cluster model called density reachability similar linkage based clustering based connecting points within certain distance thresholds however connects points satisfy density criterion original variant defined minimum number objects within radius cluster consists density connected objects plus objects within objects range another interesting property dbscan complexity fairly low requires linear number range queries database discover essentially results run therefore need run multiple times optics generalization dbscan removes need choose appropriate value range parameter displaystyle varepsilon produces hierarchical result related linkage clustering deli clu density link clustering combines ideas single linkage clustering optics eliminating displaystyle varepsilon parameter entirely offering performance improvements optics using r tree index key drawback dbscan optics expect kind density drop detect cluster borders data sets example overlapping gaussian distributions common use case artificial data cluster borders produced algorithms often look arbitrary cluster density decreases continuously data set consisting mixtures gaussians algorithms nearly always outperformed methods em clustering able precisely model kind data mean shift clustering approach object moved densest area vicinity based kernel density estimation eventually objects converge local maxima density similar k means clustering density attractors serve representatives data set mean shift detect arbitrary shaped clusters similar dbscan due expensive iterative procedure density estimation mean shift usually slower dbscan k means besides applicability mean shift algorithm multidimensional data hindered unsmooth behaviour kernel density estimate results fragmentation cluster tails density based clustering dbscan dbscan assumes clusters similar density may problems separating nearby clusters optics dbscan variant improving handling different densities clusters grid based technique used multi dimensional data set technique create grid structure comparison performed grids grid based technique fast low computational complexity two types grid based clustering methods sting clique steps involved grid based clustering algorithm recent years considerable effort put improving performance existing algorithms among clarans birch recent need process larger larger data sets willingness trade semantic meaning generated clusters performance increasing led development pre clustering methods canopy clustering process huge data sets efficiently resulting clusters merely rough pre partitioning data set analyze partitions existing slower methods k means clustering high dimensional data many existing methods fail due curse dimensionality renders particular distance functions problematic high dimensional spaces led new clustering algorithms high dimensional data focus subspace clustering correlation clustering also looks arbitrary rotated subspace clusters modeled giving correlation attributes examples clustering algorithms clique subclu ideas density based clustering methods adapted subspace clustering correlation clustering several different clustering systems based mutual information proposed one marina meil variation information metric another provides hierarchical clustering using genetic algorithms wide range different fit functions optimized including mutual information also belief propagation recent development computer science statistical physics led creation new types clustering algorithms evaluation clustering results difficult clustering popular approaches involve internal evaluation clustering summarized single quality score external evaluation clustering compared existing ground truth classification manual evaluation human expert indirect evaluation evaluating utility clustering intended application internal evaluation measures suffer problem represent functions seen clustering objective example one could cluster data set silhouette coefficient except known efficient algorithm using internal measure evaluation one rather compares similarity optimization problems necessarily useful clustering external evaluation similar problems ground truth labels would need cluster practical applications usually labels hand labels reflect one possible partitioning data set imply exist different maybe even better clustering neither approaches therefore ultimately judge actual quality clustering needs human evaluation highly subjective nevertheless statistics quite informative identifying bad clusterings one dismiss subjective human evaluation clustering result evaluated based data clustered called internal evaluation methods usually assign best score algorithm produces clusters high similarity within cluster low similarity clusters one drawback using internal criteria cluster evaluation high scores internal measure necessarily result effective information retrieval applications additionally evaluation biased towards algorithms use cluster model example k means clustering naturally optimizes object distances distance based internal criterion likely overrate resulting clustering therefore internal evaluation measures best suited get insight situations one algorithm performs better another shall imply one algorithm produces valid results another validity measured index depends claim kind structure exists data set algorithm designed kind models chance data set contains radically different set models evaluation measures radically different criterion example k means clustering find convex clusters many evaluation indexes assume convex clusters data set non convex clusters neither use k means evaluation criterion assumes convexity sound dozen internal evaluation measures exist usually based intuition items cluster similar items different clusters example following methods used assess quality clustering algorithms based internal criterion external evaluation clustering results evaluated based data used clustering known class labels external benchmarks benchmarks consist set pre classified items sets often created humans thus benchmark sets thought gold standard evaluation types evaluation methods measure close clustering predetermined benchmark classes however recently discussed whether adequate real data synthetic data sets factual ground truth since classes contain internal structure attributes present may allow separation clusters classes may contain anomalies additionally knowledge discovery point view reproduction known knowledge may necessarily intended result special scenario constrained clustering meta information used already clustering process hold information evaluation purposes non trivial number measures adapted variants used evaluate classification tasks place counting number times class correctly assigned single data point pair counting metrics assess whether pair data points truly cluster predicted cluster internal evaluation several external evaluation measures exist example one issue rand index false positives false negatives equally weighted may undesirable characteristic clustering applications f measure addresses concern citation needed chance corrected adjusted rand index measure cluster tendency measure degree clusters exist data clustered may performed initial test attempting clustering one way compare data random data average random data clusters
https://en.wikipedia.org/wiki/BIRCH,birch unsupervised data mining algorithm used perform hierarchical clustering particularly large data sets advantage birch ability incrementally dynamically cluster incoming multi dimensional metric data points attempt produce best quality clustering given set resources cases birch requires single scan database inventors claim birch first clustering algorithm proposed database area handle noise effectively beating dbscan two months algorithm received sigmod year test time award previous clustering algorithms performed less effectively large databases adequately consider case wherein data set large fit main memory result lot overhead maintaining high clustering quality minimizing cost additional io operations furthermore birch predecessors inspect data points equally clustering decision perform heuristic weighting based distance data points local clustering decision made without scanning data points currently existing clusters exploits observation data space usually uniformly occupied every data point equally important makes full use available memory derive finest possible sub clusters minimizing costs also incremental method require whole data set advance birch algorithm takes input set n data points represented real valued vectors desired number clusters k operates four phases second optional first phase builds clustering feature tree data points height balanced tree data structure defined follows second step algorithm scans leaf entries initial c f displaystyle cf tree rebuild smaller c f displaystyle cf tree removing outliers grouping crowded subclusters larger ones step marked optional original presentation birch step three existing clustering algorithm used cluster leaf entries agglomerative hierarchical clustering algorithm applied directly subclusters represented c f displaystyle cf vectors also provides flexibility allowing user specify either desired number clusters desired diameter threshold clusters step set clusters obtained captures major distribution pattern data however might exist minor localized inaccuracies handled optional step step centroids clusters produced step used seeds redistribute data points closest seeds obtain new set clusters step also provides us option discarding outliers point far closest seed treated outlier given clustering feature c f n l displaystyle cf n overrightarrow ls ss measures calculated without knowledge underlying actual values multidimensional cases square root replaced suitable norm
https://en.wikipedia.org/wiki/CURE_data_clustering_algorithm,cure efficient data clustering algorithm large databases citation needed compared k means clustering robust outliers able identify clusters non spherical shapes size variances popular k means clustering algorithm minimizes sum squared errors criterion given large differences sizes geometries different clusters square error method could split large clusters minimize square error always correct also hierarchic clustering algorithms problems exist none distance measures clusters tend work different cluster shapes also running time high n large problem birch algorithm clusters generated step uses centroids clusters assigns data point cluster closest centroid citation needed using centroid redistribute data problems clusters lack uniform sizes shapes avoid problems non uniform sized shaped clusters cure employs hierarchical clustering algorithm adopts middle ground centroid based point extremes cure constant number c well scattered points cluster chosen shrunk towards centroid cluster fraction scattered points shrinking used representatives cluster clusters closest pair representatives clusters merged step cure hierarchical clustering algorithm enables cure correctly identify clusters makes less sensitive outliers running time making rather expensive space complexity algorithm cannot directly applied large databases high runtime complexity enhancements address requirement cure input set points output k clusters
https://en.wikipedia.org/wiki/Hierarchical_clustering,data mining statistics hierarchical clustering method cluster analysis seeks build hierarchy clusters strategies hierarchical clustering generally fall two types general merges splits determined greedy manner results hierarchical clustering usually presented dendrogram standard algorithm hierarchical agglomerative clustering time complexity displaystyle mathcal requires displaystyle omega memory makes slow even medium data sets however special cases optimal efficient agglomerative methods displaystyle mathcal known slink single linkage clink complete linkage clustering heap runtime general case reduced displaystyle mathcal improvement aforementioned bound displaystyle mathcal cost increasing memory requirements many cases memory overheads approach large make practically usable except special case single linkage none algorithms displaystyle mathcal guaranteed find optimum solution divisive clustering exhaustive search displaystyle mathcal common use faster heuristics choose splits k means order decide clusters combined cluster split measure dissimilarity sets observations required methods hierarchical clustering achieved use appropriate metric linkage criterion specifies dissimilarity sets function pairwise distances observations sets choice appropriate metric influence shape clusters elements may relatively closer one another one metric another example two dimensions manhattan distance metric distance origin distance origin euclidean distance metric latter strictly greater commonly used metrics hierarchical clustering text non numeric data metrics hamming distance levenshtein distance often used review cluster analysis health psychology research found common distance measure published studies research area euclidean distance squared euclidean distance citation needed linkage criterion determines distance sets observations function pairwise distances observations commonly used linkage criteria two sets observations b chosen metric linkage criteria include hierarchical clustering distinct advantage valid measure distance used fact observations required used matrix distances example suppose data clustered euclidean distance distance metric hierarchical clustering dendrogram would cutting tree given height give partitioning clustering selected precision example cutting second row dendrogram yield clusters b c e f cutting third row yield clusters b c e f coarser clustering smaller number larger clusters method builds hierarchy individual elements progressively merging clusters example six elements b c e f first step determine elements merge cluster usually want take two closest elements according chosen distance optionally one also construct distance matrix stage number th row j th column distance th j th elements clustering progresses rows columns merged clusters merged distances updated common way implement type clustering benefit caching distances clusters simple agglomerative clustering algorithm described single linkage clustering page easily adapted different types linkage suppose merged two closest elements b c following clusters b c e f want merge need take distance b c therefore define distance two clusters usually distance two clusters displaystyle mathcal b displaystyle mathcal b one following case tied minimum distances pair randomly chosen thus able generate several structurally different dendrograms alternatively tied pairs may joined time generating unique dendrogram one always decide stop clustering sufficiently small number clusters linkages may also guarantee agglomeration occurs greater distance clusters previous agglomeration one stop clustering clusters far apart merged however case e g centroid linkage called reversals may occur basic principle divisive clustering published diana algorithm initially data cluster largest cluster split every object separate exist displaystyle ways splitting cluster heuristics needed diana chooses object maximum average dissimilarity moves objects cluster similar new cluster remainder
https://en.wikipedia.org/wiki/K-means_clustering,k means clustering method vector quantization originally signal processing aims partition n observations k clusters observation belongs cluster nearest mean serving prototype cluster results partitioning data space voronoi cells popular cluster analysis data mining k means clustering minimizes within cluster variances regular euclidean distances would difficult weber problem mean optimizes squared errors whereas geometric median minimizes euclidean distances instance better euclidean solutions found using k medians k medoids problem computationally difficult however efficient heuristic algorithms converge quickly local optimum usually similar expectation maximization algorithm mixtures gaussian distributions via iterative refinement approach employed k means gaussian mixture modeling use cluster centers model data however k means clustering tends find clusters comparable spatial extent expectation maximization mechanism allows clusters different shapes algorithm loose relationship k nearest neighbor classifier popular machine learning technique classification often confused k means due name applying nearest neighbor classifier cluster centers obtained k means classifies new data existing clusters known nearest centroid classifier rocchio algorithm given set observations observation dimensional real vector k means clustering aims partition n observations k sets sk minimize within cluster sum squares formally objective find mean points si equivalent minimizing pairwise squared deviations points cluster equivalence deduced identity x x x displaystyle sum mathbf x left mathbf x boldsymbol mu right sum mathbf x neq mathbf total variance constant equivalent maximizing sum squared deviations points different clusters follows law total variance term k means first used james macqueen though idea goes back hugo steinhaus standard algorithm first proposed stuart lloyd bell labs technique pulse code modulation although published journal article edward w forgy published essentially method sometimes referred lloyd forgy algorithm common algorithm uses iterative refinement technique due ubiquity often called k means algorithm also referred lloyd algorithm particularly computer science community sometimes also referred naive k means exist much faster alternatives given initial set k means mk algorithm proceeds alternating two steps algorithm converged assignments longer change algorithm guaranteed find optimum algorithm often presented assigning objects nearest cluster distance using different distance function euclidean distance may prevent algorithm converging various modifications k means spherical k means k medoids proposed allow using distance measures commonly used initialization methods forgy random partition forgy method randomly chooses k observations dataset uses initial means random partition method first randomly assigns cluster observation proceeds update step thus computing initial mean centroid cluster randomly assigned points forgy method tends spread initial means random partition places close center data set according hamerly et al random partition method generally preferable algorithms k harmonic means fuzzy k means expectation maximization standard k means algorithms forgy method initialization preferable comprehensive study celebi et al however found popular initialization methods forgy random partition maximin often perform poorly whereas bradley fayyad approach performs consistently best group k means performs generally well k initial means randomly generated within data domain k clusters created associating every observation nearest mean partitions represent voronoi diagram generated means centroid k clusters becomes new mean steps repeated convergence reached algorithm guarantee convergence global optimum result may depend initial clusters algorithm usually fast common run multiple times different starting conditions however worst case performance slow particular certain point sets even two dimensions converge exponential time point sets seem arise practice corroborated fact smoothed running time k means polynomial assignment step referred expectation step update step maximization step making algorithm variant generalized expectation maximization algorithm finding optimal solution k means clustering problem observations dimensions thus variety heuristic algorithms lloyd algorithm given generally used running time lloyd algorithm displaystyle data clustering structure number iterations convergence often small results improve slightly first dozen iterations lloyd algorithm therefore often considered linear complexity practice although worst case superpolynomial performed convergence lloyd algorithm standard approach problem however spends lot processing time computing distances k cluster centers n data points since points usually stay clusters iterations much work unnecessary making naive implementation inefficient implementations use caching triangle inequality order create bounds accelerate lloyd algorithm hartigan wong method provides variation k means algorithm progresses towards local minimum minimum sum squares problem different solution updates method local search iteratively attempts relocate sample different cluster long process improves objective function sample relocated different cluster improvement objective method stops similar way classical k means approach remains heuristic since necessarily guarantee final solution globally optimum let displaystyle varphi individual cost j displaystyle j defined x j displaystyle sum x j j displaystyle mu j center cluster assignment step hartigan wong method starts partitioning points random clusters j j k displaystyle j j cdots k update step next determines n k displaystyle n ldots k x n displaystyle x n following function reaches maximum x n displaystyle x n reach minimum x displaystyle x moves cluster n displaystyle n cluster displaystyle termination algorithm terminates displaystyle delta larger zero x n displaystyle x n different move acceptance strategies used first improvement strategy improving relocation applied whereas best improvement strategy possible relocations iteratively tested best applied iteration former approach favors speed whether latter approach generally favors solution quality expense additional computational time function displaystyle delta used calculate result relocation also efficiently evaluated using equality classical k means algorithm variations known converge local minima minimum sum squares clustering problem defined many studies attempted improve convergence behavior algorithm maximize chances attaining global optimum initialization restart techniques discussed previous sections one alternative find better solutions recently mathematical programming algorithms based branch bound column generation produced provenly optimal solutions datasets entities expected due np hardness subjacent optimization problem computational time optimal algorithms k means quickly increases beyond size optimal solutions small medium scale still remain valuable benchmark tool evaluate quality heuristics find high quality local minima within controlled computational time without optimality guarantees works explored metaheuristics global optimization techniques e g based incremental approaches convex optimization random swaps variable neighborhood search genetic algorithms indeed known finding better local minima minimum sum squares clustering problem make difference failure success recover cluster structures feature spaces high dimension three key features k means make efficient often regarded biggest drawbacks key limitation k means cluster model concept based spherical clusters separable mean converges towards cluster center clusters expected similar size assignment nearest cluster center correct assignment example applying k means value k displaystyle k onto well known iris flower data set result often fails separate three iris species contained data set k displaystyle k two visible clusters discovered whereas k displaystyle k one two clusters split two even parts fact k displaystyle k appropriate data set despite data set containing classes clustering algorithm k means result makes assumptions data satisfy certain criteria works well data sets fails others result k means seen voronoi cells cluster means since data split halfway cluster means lead suboptimal splits seen mouse example gaussian models used expectation maximization algorithm flexible variances covariances em result thus able accommodate clusters variable size much better k means well correlated clusters counterpart em requires optimization larger number free parameters poses methodological issues due vanishing clusters badly conditioned covariance matrices k means closely related nonparametric bayesian modeling k means clustering rather easy apply even large data sets particularly using heuristics lloyd algorithm successfully used market segmentation computer vision astronomy among many domains often used preprocessing step algorithms example find starting configuration k means originates signal processing still finds use domain example computer graphics color quantization task reducing color palette image fixed number colors k k means algorithm easily used task produces competitive results use case approach image segmentation uses vector quantization include non random sampling k means easily used choose k different prototypical objects large data set analysis cluster analysis k means algorithm used partition input data set k partitions however pure k means algorithm flexible limited use particular parameter k known hard choose given external constraints another limitation cannot used arbitrary distance functions non numerical data use cases many algorithms superior k means clustering used feature learning step either supervised learning unsupervised learning basic approach first train k means clustering representation using input training data project input datum new feature space encoding function thresholded matrix product datum centroid locations computes distance datum centroid simply indicator function nearest centroid smooth transformation distance alternatively transforming sample cluster distance gaussian rbf obtains hidden layer radial basis function network use k means successfully combined simple linear classifiers semi supervised learning nlp computer vision object recognition task found exhibit comparable performance sophisticated feature learning approaches autoencoders restricted boltzmann machines however generally requires data equivalent performance data point contributes one feature slow standard algorithm k means clustering associated expectation maximization algorithm special case gaussian mixture model specifically limiting case fixing covariances diagonal equal infinitesimal small variance instead small variances hard cluster assignment also used show another equivalence k means clustering special case hard gaussian mixture modelling mean efficient use gaussian mixture modelling compute k means theoretical relationship gaussian mixture modelling interpreted generalization k means contrary suggested use k means clustering find starting points gaussian mixture modelling difficult data another generalization k means algorithm k svd algorithm estimates data points sparse linear combination codebook vectors k means corresponds special case using single codebook vector weight relaxed solution k means clustering specified cluster indicators given principal component analysis intuition k means describe spherically shaped clusters data clusters line connecting two centroids best dimensional projection direction also first pca direction cutting line center mass separates clusters data three clusters dimensional plane spanned three cluster centroids best projection plane also defined first two pca dimensions well separated clusters effectively modelled ball shaped clusters thus discovered k means non ball shaped clusters hard separate close example two half moon shaped clusters intertwined space separate well projected onto pca subspace k means expected well data straightforward produce counterexamples statement cluster centroid subspace spanned principal directions basic mean shift clustering algorithms maintain set data points size input data set initially set copied input set set iteratively replaced mean points set within given distance point contrast k means restricts updated set k points usually much less number points input data set replaces point set mean points input set closer point mean shift algorithm similar k means called likelihood mean shift replaces set points undergoing replacement mean points input set within given distance changing set one advantages mean shift k means number clusters pre specified mean shift likely find clusters small number exist however mean shift much slower k means still requires selection bandwidth parameter mean shift soft variants sparsity assumptions input data pre processed whitening transformation k means produces solution linear independent component analysis task aids explaining successful application k means feature learning k means implicitly assumes ordering input data set matter bilateral filter similar k means mean shift maintains set data points iteratively replaced means however bilateral filter restricts calculation mean include points close ordering input data makes applicable problems image denoising spatial arrangement pixels image critical importance set squared error minimizing cluster functions also includes k medoids algorithm approach forces center point cluster one actual points e uses medoids place centroids different implementations algorithm exhibit performance differences fastest test data set finishing seconds slowest taking seconds differences attributed implementation quality language compiler differences different termination criteria precision levels use indexes acceleration following implementations available free open source software licenses publicly available source code following implementations available proprietary license terms may publicly available source code
https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm,statistics expectation maximization algorithm iterative method find maximum likelihood maximum posteriori estimates parameters statistical models model depends unobserved latent variables em iteration alternates performing expectation step creates function expectation log likelihood evaluated using current estimate parameters maximization step computes parameters maximizing expected log likelihood found e step parameter estimates used determine distribution latent variables next e step em algorithm explained given name classic paper arthur dempster nan laird donald rubin pointed method proposed many times special circumstances earlier authors one earliest gene counting method estimating allele frequencies cedric smith detailed treatment em method exponential families published rolf sundberg thesis several papers following collaboration per martin l f anders martin l f dempster laird rubin paper generalized method sketched convergence analysis wider class problems dempster laird rubin paper established em method important tool statistical analysis convergence analysis dempster laird rubin algorithm flawed correct convergence analysis published c f jeff wu wu proof established em method convergence outside exponential family claimed dempster laird rubin em algorithm used find maximum likelihood parameters statistical model cases equations cannot solved directly typically models involve latent variables addition unknown parameters known data observations either missing values exist among data model formulated simply assuming existence unobserved data points example mixture model described simply assuming observed data point corresponding unobserved data point latent variable specifying mixture component data point belongs finding maximum likelihood solution typically requires taking derivatives likelihood function respect unknown values parameters latent variables simultaneously solving resulting equations statistical models latent variables usually impossible instead result typically set interlocking equations solution parameters requires values latent variables vice versa substituting one set equations produces unsolvable equation em algorithm proceeds observation way solve two sets equations numerically one simply pick arbitrary values one two sets unknowns use estimate second set use new values find better estimate first set keep alternating two resulting values converge fixed points obvious work proven context derivative likelihood zero point turn means point either maximum saddle point general multiple maxima may occur guarantee global maximum found likelihoods also singularities e nonsensical maxima example one solutions may found em mixture model involves setting one components zero variance mean parameter component equal one data points given statistical model generates set x displaystyle mathbf x observed data set unobserved latent data missing values z displaystyle mathbf z vector unknown parameters displaystyle boldsymbol theta along likelihood function l p displaystyle l p maximum likelihood estimate unknown parameters determined maximizing marginal likelihood observed data however quantity often intractable em algorithm seeks find mle marginal likelihood iteratively applying two steps typical models em applied use z displaystyle mathbf z latent variable indicating membership one set groups however possible apply em sorts models motive follows value parameters displaystyle boldsymbol theta known usually value latent variables z displaystyle mathbf z found maximizing log likelihood possible values z displaystyle mathbf z either simply iterating z displaystyle mathbf z algorithm baum welch algorithm hidden markov models conversely know value latent variables z displaystyle mathbf z find estimate parameters displaystyle boldsymbol theta fairly easily typically simply grouping observed data points according value associated latent variable averaging values function values points group suggests iterative algorithm case displaystyle boldsymbol theta z displaystyle mathbf z unknown algorithm described monotonically approaches local minimum cost function speaking expectation step bit misnomer calculated first step fixed data dependent parameters function q parameters q known fully determined maximized second step em algorithm although em iteration increase observed data likelihood function guarantee exists sequence converges maximum likelihood estimator multimodal distributions means em algorithm may converge local maximum observed data likelihood function depending starting values variety heuristic metaheuristic approaches exist escape local maximum random restart hill climbing applying simulated annealing methods em especially useful likelihood exponential family e step becomes sum expectations sufficient statistics step involves maximizing linear function case usually possible derive closed form expression updates step using sundberg formula em method modified compute maximum posteriori estimates bayesian inference original paper dempster laird rubin methods exist find maximum likelihood estimates gradient descent conjugate gradient variants gauss newton algorithm unlike em methods typically require evaluation first second derivatives likelihood function expectation maximization works improve q displaystyle q rather directly improving log p displaystyle log p shown improvements former imply improvements latter z displaystyle mathbf z non zero probability p displaystyle p write take expectation possible values unknown data z displaystyle mathbf z current parameter estimate displaystyle theta multiplying sides p displaystyle p summing z displaystyle mathbf z left hand side expectation constant get h displaystyle h defined negated sum replacing last equation holds every value displaystyle boldsymbol theta including displaystyle boldsymbol theta boldsymbol theta subtracting last equation previous equation gives however gibbs inequality tells us h h displaystyle h geq h mid boldsymbol theta conclude words choosing displaystyle boldsymbol theta improve q displaystyle q causes log p displaystyle log p improve least much em algorithm viewed two alternating maximization steps example coordinate descent consider function q arbitrary probability distribution unobserved data z h entropy distribution q function written p z x displaystyle p z mid x conditional distribution unobserved data given observed data x displaystyle x k l displaystyle kl kullback leibler divergence steps em algorithm may viewed em frequently used data clustering machine learning computer vision natural language processing two prominent instances algorithm baum welch algorithm hidden markov models inside outside algorithm unsupervised induction probabilistic context free grammars em frequently used parameter estimation mixed models notably quantitative genetics psychometrics em almost indispensable estimating item parameters latent abilities item response theory models ability deal missing data observe unidentified variables em becoming useful tool price manage risk portfolio citation needed em algorithm also widely used medical image reconstruction especially positron emission tomography single photon emission computed tomography x ray computed tomography see faster variants em structural engineering structural identification using expectation maximization algorithm output method identifying natural vibration properties structural system using sensor data kalman filter typically used line state estimation minimum variance smoother may employed line batch state estimation however minimum variance solutions require estimates state space model parameters em algorithms used solving joint state parameter estimation problems filtering smoothing em algorithms arise repeating two step procedure suppose kalman filter minimum variance smoother operates measurements single input single output system possess additive white noise updated measurement noise variance estimate obtained maximum likelihood calculation x k displaystyle widehat x k scalar output estimates calculated filter smoother n scalar measurements z k displaystyle z k update also applied updating poisson measurement noise intensity similarly first order auto regressive process updated process noise variance estimate calculated x k displaystyle widehat x k x k displaystyle widehat x k scalar state estimates calculated filter smoother updated model coefficient estimate obtained via convergence parameter estimates well studied number methods proposed accelerate sometimes slow convergence em algorithm using conjugate gradient modified newton methods also em used constrained estimation methods parameter expanded expectation maximization algorithm often provides speed us ing covariance adjustment correct analysis step capitalising extra information captured imputed complete data expectation conditional maximization replaces step sequence conditional maximization steps parameter maximized individually conditionally parameters remaining fixed extended expectation conditional maximization either algorithm idea extended generalized expectation maximization algorithm sought increase objective function f e step step described maximization maximization procedure section gem developed distributed environment shows promising results also possible consider em algorithm subclass mm algorithm therefore use machinery developed general case q function used em algorithm based log likelihood therefore regarded log em algorithm use log likelihood generalized log likelihood ratio log likelihood ratio observed data exactly expressed equality using q function log likelihood ratio divergence obtaining q function generalized e step maximization generalized step pair called em algorithm contains log em algorithm subclass thus em algorithm yasuo matsuyama exact generalization log em algorithm computation gradient hessian matrix needed em shows faster convergence log em algorithm choosing appropriate em algorithm leads faster version hidden markov model estimation algorithm hmm em partially non bayesian maximum likelihood method final result gives probability distribution latent variables together point estimate fully bayesian version may wanted giving probability distribution latent variables bayesian approach inference simply treat another latent variable paradigm distinction e steps disappears using factorized q approximation described solving iterate latent variable optimize one time k steps per iteration needed k number latent variables graphical models easy variable new q depends markov blanket local message passing used efficient inference information geometry e step step interpreted projections dual affine connections called e connection connection kullback leibler divergence also understood terms let x displaystyle mathbf x sample n displaystyle n independent observations mixture two multivariate normal distributions dimension displaystyle let z displaystyle mathbf z latent variables determine component observation originates aim estimate unknown parameters representing mixing value gaussians means covariances incomplete data likelihood function complete data likelihood function displaystyle mathbb indicator function f displaystyle f probability density function multivariate normal last equality one indicator displaystyle mathbb equal zero one indicator equal one inner sum thus reduces one term given current estimate parameters conditional distribution zi determined bayes theorem proportional height normal density weighted called membership probabilities normally considered output e step e step corresponds setting function q expectation log l displaystyle log l inside sum taken respect probability density function p displaystyle p might different x displaystyle mathbf x training set everything e step known step taken except j displaystyle j computed according equation beginning e step section full conditional expectation need calculated one step appear separate linear terms thus maximized independently q quadratic form means determining maximizing values relatively straightforward also may maximized independently since appear separate linear terms begin consider constraint form mle binomial distribution next estimates form weighted mle normal distribution symmetry conclude iterative process e z x log l x z e z x log l x z displaystyle e z mid theta mathbf x log l mathbf x mathbf z leq e z mid theta mathbf x log l mathbf x mathbf z varepsilon displaystyle varepsilon preset threshold algorithm illustrated generalized mixtures two multivariate normal distributions em algorithm implemented case underlying linear regression model exists explaining variation quantity values actually observed censored truncated versions represented model special cases model include censored truncated observations one normal distribution em typically converges local optimum necessarily global optimum bound convergence rate general possible arbitrarily poor high dimensions exponential number local optima hence need exists alternative methods guaranteed learning especially high dimensional setting alternatives em exist better guarantees consistency termed moment based approaches called spectral techniques citation needed moment based approaches learning parameters probabilistic model increasing interest recently since enjoy guarantees global convergence certain conditions unlike em often plagued issue getting stuck local optima algorithms guarantees learning derived number important models mixture models hmms etc spectral methods spurious local optima occur true parameters consistently estimated regularity conditions citation needed
https://en.wikipedia.org/wiki/DBSCAN,density based spatial clustering applications noise data clustering algorithm proposed martin ester hans peter kriegel j rg sander xiaowei xu density based clustering non parametric algorithm given set points space groups together points closely packed together marking outliers points lie alone low density regions dbscan one common clustering algorithms also cited scientific literature algorithm awarded test time award leading data mining conference acm sigkdd july update follow paper dbscan revisited revisited use dbscan appears list downloaded articles prestigious acm transactions database systems journal robert f ling published closely related algorithm theory construction k clusters computer journal estimated runtime complexity dbscan worst case database oriented range query formulation dbscan allows index acceleration algorithms slightly differ handling border points consider set points space clustered let parameter specifying radius neighborhood respect point purpose dbscan clustering points classified core points reachable points outliers follows p core point forms cluster together points reachable cluster contains least one core point non core points part cluster form edge since cannot used reach points reachability symmetric relation definition core points reach non core points opposite true non core point may reachable nothing reached therefore notion connectedness needed formally define extent clusters found dbscan two points p q density connected point p q reachable density connectedness symmetric cluster satisfies two properties dbscan requires two parameters minimum number points required form dense region starts arbitrary starting point visited point neighborhood retrieved contains sufficiently many points cluster started otherwise point labeled noise note point might later found sufficiently sized environment different point hence made part cluster point found dense part cluster neighborhood also part cluster hence points found within neighborhood added neighborhood also dense process continues density connected cluster completely found new unvisited point retrieved processed leading discovery cluster noise dbscan used distance function distance function therefore seen additional parameter algorithm expressed pseudocode follows rangequery implemented using database index better performance using slow linear scan dbscan algorithm abstracted following steps naive implementation requires storing neighborhoods step thus requiring substantial memory original dbscan algorithm require performing steps one point time dbscan visits point database possibly multiple times practical considerations however time complexity mostly governed number regionquery invocations dbscan executes exactly one query point indexing structure used executes neighborhood query overall average runtime complexity obtained points returned without use accelerating index structure degenerated data worst case run time complexity remains distance matrix size materialized avoid distance recomputations needs memory whereas non matrix based implementation dbscan needs memory see section extensions algorithmic modifications handle issues every data mining task problem parameters every parameter influences algorithm specific ways dbscan parameters minpts needed parameters must specified user ideally value given problem solve minpts desired minimum cluster size optics seen generalization dbscan replaces parameter maximum value mostly affects performance minpts essentially becomes minimum cluster size find algorithm much easier parameterize dbscan results bit difficult use usually produce hierarchical clustering instead simple data partitioning dbscan produces recently one original authors dbscan revisited dbscan optics published refined version hierarchical dbscan longer notion border points instead core points form cluster dbscan seen special variant spectral clustering connected components correspond optimal spectral clusters dbscan finds connected components reachability graph however spectral clustering computationally intensive displaystyle without approximation assumptions one choose number clusters k displaystyle k number eigenvectors choose number clusters produce k means spectral embedding thus performance reasons original dbscan algorithm remains preferable spectral implementation relationship far theoretical interest generalized dbscan generalization authors arbitrary neighborhood dense predicates minpts parameters removed original algorithm moved predicates example polygon data neighborhood could intersecting polygon whereas density predicate uses polygon areas instead object count various extensions dbscan algorithm proposed including methods parallelization parameter estimation support uncertain data basic idea extended hierarchical clustering optics algorithm dbscan also used part subspace clustering algorithms like predecon subclu hdbscan hierarchical version dbscan also faster optics flat partition consisting prominent clusters extracted hierarchy different implementations algorithm found exhibit enormous performance differences fastest test data set finishing seconds slowest taking seconds differences attributed implementation quality language compiler differences use indexes acceleration
https://en.wikipedia.org/wiki/OPTICS_algorithm,ordering points identify clustering structure algorithm finding density based clusters spatial data presented mihael ankerst markus breunig hans peter kriegel j rg sander basic idea similar dbscan addresses one dbscan major weaknesses problem detecting meaningful clusters data varying density points database ordered spatially closest points become neighbors ordering additionally special distance stored point represents density must accepted cluster points belong cluster represented dendrogram like dbscan optics requires two parameters describes maximum distance consider minpts describing number points required form cluster point p core point least minpts points found within neighborhood n displaystyle n varepsilon contrast dbscan optics also considers points part densely packed cluster point assigned core distance describes distance minptsth closest point reachability distance another point point p either distance p core distance p whichever bigger p nearest neighbors displaystyle varepsilon max x displaystyle varepsilon max x possible leads quadratic complexity since every neighborhood query returns full data set even spatial index available comes additional cost managing heap therefore displaystyle varepsilon chosen appropriately data set optics outlier detection algorithm based optics main use extraction outliers existing run optics low cost compared using different outlier detection method better known version lof based concepts deli clu density link clustering combines ideas single linkage clustering optics eliminating displaystyle varepsilon parameter offering performance improvements optics hisc hierarchical subspace clustering method based optics hico hierarchical correlation clustering algorithm based optics dish improvement hisc find complex hierarchies foptics faster implementation using random projections hdbscan based refinement dbscan excluding border points clusters thus following strictly basic definition density levels hartigan java implementations optics optics deli clu hisc hico dish available elki data mining framework java implementations include weka extension r package dbscan includes c implementation optics using k tree index acceleration euclidean distance python implementations optics available pyclustering library scikit learn hdbscan available hdbscan library
https://en.wikipedia.org/wiki/Mean-shift,mean shift non parametric feature space analysis technique locating maxima density function called mode seeking algorithm application domains include cluster analysis computer vision image processing mean shift procedure originally presented fukunaga hostetler mean shift procedure locating maxima modes density function given discrete data sampled function iterative method start initial estimate x displaystyle x let kernel function k displaystyle k given function determines weight nearby points estimation mean typically gaussian kernel distance current estimate used k e c x x displaystyle k e c x x weighted mean density window determined k displaystyle k n displaystyle n neighborhood x displaystyle x set points k displaystyle k neq difference x displaystyle x called mean shift fukunaga hostetler mean shift algorithm sets x displaystyle x leftarrow repeats estimation displaystyle converges although mean shift algorithm widely used many applications rigid proof convergence algorithm using general kernel high dimensional space still known aliyari ghassabeh showed convergence mean shift algorithm one dimension differentiable convex strictly decreasing profile function however one dimensional case limited real world applications also convergence algorithm higher dimensions finite number stationary points proved however sufficient conditions general kernel function finite stationary points provided gaussian mean shift expectation maximization algorithm let data finite set displaystyle embedded n displaystyle n dimensional euclidean space x displaystyle x let k displaystyle k flat kernel characteristic function displaystyle lambda ball x displaystyle x k x x displaystyle k begin cases text x leq lambda text x lambda end cases iteration algorithm displaystyle leftarrow performed displaystyle simultaneously first question estimate density function given sparse set samples one simplest approaches smooth data e g convolving fixed kernel width h displaystyle h f k k displaystyle f sum k sum k left x displaystyle x input samples k displaystyle k kernel function h displaystyle h parameter algorithm called bandwidth approach known kernel density estimation parzen window technique computed f displaystyle f equation find local maxima using gradient ascent optimization technique problem brute force approach higher dimensions becomes computationally prohibitive evaluate f displaystyle f complete search space instead mean shift uses variant known optimization literature multiple restart gradient descent starting guess local maximum k displaystyle k random input data point x displaystyle x mean shift computes gradient density estimate f displaystyle f k displaystyle k takes uphill step direction kernel definition let x displaystyle x n displaystyle n dimensional euclidean space r n displaystyle mathbb r n norm x displaystyle x non negative number x x x displaystyle x x top x geq function k x r displaystyle k x rightarrow mathbb r said kernel exists profile k r displaystyle k infty rightarrow mathbb r k k displaystyle k k two frequently used kernel profiles mean shift k x x displaystyle k begin cases text x leq lambda text x lambda end cases k e x displaystyle k e frac x sigma standard deviation parameter displaystyle sigma works bandwidth parameter h displaystyle h consider set points two dimensional space assume circular window centered c radius r kernel mean shift hill climbing algorithm involves shifting kernel iteratively higher density region convergence every shift defined mean shift vector mean shift vector always points toward direction maximum increase density every iteration kernel shifted centroid mean points within method calculating mean depends choice kernel case gaussian kernel chosen instead flat kernel every point first assigned weight decay exponentially distance kernel center increases convergence direction shift accommodate points inside kernel mean shift algorithm used visual tracking simplest algorithm would create confidence map new image based color histogram object previous image use mean shift find peak confidence map near object old position confidence map probability density function new image assigning pixel new image probability probability pixel color occurring object previous image algorithms kernel based object tracking ensemble tracking camshift expand idea let x displaystyle x z n displaystyle z n displaystyle dimensional input filtered image pixels joint spatial range domain pixel variants algorithm found machine learning image processing packages
https://en.wikipedia.org/wiki/Dimensionality_reduction,dimensionality reduction dimension reduction transformation data high dimensional space low dimensional space low dimensional representation retains meaningful properties original data ideally close intrinsic dimension working high dimensional spaces undesirable many reasons raw data often sparse consequence curse dimensionality analyzing data usually computationally intractable dimensionality reduction common fields deal large numbers observations large numbers variables signal processing speech recognition neuroinformatics bioinformatics methods commonly divided linear non linear approaches approaches also divided feature selection feature extraction dimensionality reduction used noise reduction data visualization cluster analysis intermediate step facilitate analyses feature selection approaches try find subset input variables three strategies filter strategy wrapper strategy embedded strategy data analysis regression classification done reduced space accurately original space feature projection transforms data high dimensional space space fewer dimensions data transformation may linear principal component analysis many nonlinear dimensionality reduction techniques also exist multidimensional data tensor representation used dimensionality reduction multilinear subspace learning main linear technique dimensionality reduction principal component analysis performs linear mapping data lower dimensional space way variance data low dimensional representation maximized practice covariance matrix data constructed eigenvectors matrix computed eigenvectors correspond largest eigenvalues used reconstruct large fraction variance original data moreover first eigenvectors often interpreted terms large scale physical behavior system often contribute vast majority system energy especially low dimensional systems still must proven case case basis systems exhibit behavior original space reduced space spanned eigenvectors citation needed nmf decomposes non negative matrix product two non negative ones promising tool fields non negative signals exist astronomy nmf well known since multiplicative update rule lee seung continuously developed inclusion uncertainties consideration missing data parallel computation sequential construction leads stability linearity nmf well updates including handling missing data digital image processing stable component basis construction linear modeling process sequential nmf able preserve flux direct imaging circumstellar structures astromony one methods detecting exoplanets especially direct imaging circumstellar disks comparison pca nmf remove mean matrices leads unphysical non negative fluxes therefore nmf able preserve information pca demonstrated ren et al principal component analysis employed nonlinear way means kernel trick resulting technique capable constructing nonlinear mappings maximize variance data resulting technique entitled kernel pca prominent nonlinear techniques include manifold learning techniques isomap locally linear embedding hessian lle laplacian eigenmaps methods based tangent space analysis techniques construct low dimensional data representation using cost function retains local properties data viewed defining graph based kernel kernel pca recently techniques proposed instead defining fixed kernel try learn kernel using semidefinite programming prominent example technique maximum variance unfolding central idea mvu exactly preserve pairwise distances nearest neighbors maximizing distances points nearest neighbors alternative approach neighborhood preservation minimization cost function measures differences distances input output spaces important examples techniques include classical multidimensional scaling identical pca isomap uses geodesic distances data space diffusion maps use diffusion distances data space distributed stochastic neighbor embedding minimizes divergence distributions pairs points curvilinear component analysis different approach nonlinear dimensionality reduction use autoencoders special kind feed forward neural networks bottle neck hidden layer training deep encoders typically performed using greedy layer wise pre training followed finetuning stage based backpropagation linear discriminant analysis generalization fisher linear discriminant method used statistics pattern recognition machine learning find linear combination features characterizes separates two classes objects events gda deals nonlinear discriminant analysis using kernel function operator underlying theory close support vector machines insofar gda method provides mapping input vectors high dimensional feature space similar lda objective gda find projection features lower dimensional space maximizing ratio class scatter within class scatter autoencoders used learn non linear dimension reduction functions codings together inverse function coding original representation distributed stochastic neighbor embedding non linear dimensionality reduction technique useful visualization high dimensional datasets uniform manifold approximation projection nonlinear dimensionality reduction technique visually similar sne assumes data uniformly distributed locally connected riemannian manifold riemannian metric locally constant approximately locally constant high dimensional datasets dimension reduction usually performed prior applying k nearest neighbors algorithm order avoid effects curse dimensionality feature extraction dimension reduction combined one step using principal component analysis linear discriminant analysis canonical correlation analysis non negative matrix factorization techniques pre processing step followed clustering k nn feature vectors reduced dimension space machine learning process also called low dimensional embedding high dimensional datasets running fast approximate k nn search using locality sensitive hashing random projection sketches high dimensional similarity search techniques vldb toolbox might feasible option dimensionality reduction technique sometimes used neuroscience maximally informative dimensions citation needed finds lower dimensional representation dataset much information possible original data preserved
https://en.wikipedia.org/wiki/Factor_analysis,factor analysis statistical method used describe variability among observed correlated variables terms potentially lower number unobserved variables called factors example possible variations six observed variables mainly reflect variations two unobserved variables factor analysis searches joint variations response unobserved latent variables observed variables modelled linear combinations potential factors plus error terms factor analysis aims find independent latent variables theory behind factor analytic methods information gained interdependencies observed variables used later reduce set variables dataset factor analysis commonly used biology psychometrics personality theories marketing product management operations research finance may help deal data sets large numbers observed variables thought reflect smaller number underlying latent variables one commonly used inter dependency techniques used relevant set variables shows systematic inter dependence objective find latent factors create commonality factor analysis related principal component analysis two identical significant controversy field differences two techniques pca considered basic version exploratory factor analysis developed early days prior advent high speed computers pca factor analysis aim reduce dimensionality set data approaches taken different two techniques factor analysis clearly designed objective identify certain unobservable factors observed variables whereas pca directly address objective best pca provides approximation required factors point view exploratory analysis eigenvalues pca inflated component loadings e contaminated error variance suppose set p displaystyle p observable random variables x x p displaystyle x dots x p means p displaystyle mu dots mu p suppose unknown constants l j displaystyle l ij k displaystyle k unobserved random variables f j displaystyle f j p displaystyle dots p j k displaystyle j dots k k p displaystyle k
https://en.wikipedia.org/wiki/Canonical_correlation,statistics canonical correlation analysis also called canonical variates analysis way inferring information cross covariance matrices two vectors x random variables correlations among variables canonical correlation analysis find linear combinations x maximum correlation r knapp notes virtually commonly encountered parametric tests significance treated special cases canonical correlation analysis general procedure investigating relationships two sets variables method first introduced harold hotelling although context angles flats mathematical concept published jordan given two column vectors x displaystyle x displaystyle random variables finite second moments one may define cross covariance x cov displaystyle sigma xy operatorname cov n displaystyle n times matrix whose displaystyle entry covariance cov displaystyle operatorname cov practice would estimate covariance matrix based sampled data x displaystyle x displaystyle canonical correlation analysis seeks vectors displaystyle b displaystyle b random variables x displaystyle x b displaystyle b maximize correlation corr displaystyle rho operatorname corr random variables u x displaystyle u x v b displaystyle v b first pair canonical variables one seeks vectors maximizing correlation subject constraint uncorrelated first pair canonical variables gives second pair canonical variables procedure may continued min n displaystyle min n times let u v displaystyle sigma uv cross covariance matrix random variables u displaystyle u v displaystyle v parameter maximize first step define change basis define thus cauchy schwarz inequality equality vectors displaystyle x x x c displaystyle sigma yy sigma yx sigma xx c collinear addition maximum correlation attained c displaystyle c eigenvector maximum eigenvalue matrix x x x x x x displaystyle sigma xx sigma xy sigma yy sigma yx sigma xx subsequent pairs found using eigenvalues decreasing magnitudes orthogonality guaranteed symmetry correlation matrices another way viewing computation c displaystyle c displaystyle left right singular vectors correlation matrix x corresponding highest singular value solution therefore reciprocally also reversing change coordinates canonical variables defined cca computed using singular value decomposition correlation matrix available function cca computation using singular value decomposition correlation matrix related cosine angles flats cosine function ill conditioned small angles leading inaccurate computation highly correlated principal vectors finite precision computer arithmetic fix trouble alternative algorithms available row tested significance following method since correlations sorted saying row displaystyle zero implies correlations also zero p displaystyle p independent observations sample displaystyle widehat rho estimated correlation min n displaystyle dots min n displaystyle th row test statistic asymptotically distributed chi squared displaystyle degrees freedom large p displaystyle p since correlations min n displaystyle min n p displaystyle p logically zero product terms point irrelevant note small sample size limit p n displaystyle p
https://en.wikipedia.org/wiki/Independent_component_analysis,signal processing independent component analysis computational method separating multivariate signal additive subcomponents done assuming subcomponents non gaussian signals statistically independent ica special case blind source separation common example application cocktail party problem listening one person speech noisy room independent component analysis attempts decompose multivariate signal independent non gaussian signals example sound usually signal composed numerical addition time signals several sources question whether possible separate contributing sources observed total signal statistical independence assumption correct blind ica separation mixed signal gives good results citation needed also used signals supposed generated mixing analysis purposes simple application ica cocktail party problem underlying speech signals separated sample data consisting people talking simultaneously room usually problem simplified assuming time delays echoes note filtered delayed signal copy dependent component thus statistical independence assumption violated mixing weights constructing textstyle observed signals n textstyle n components placed n textstyle times n matrix important thing consider n textstyle n sources present least n textstyle n observations needed recover original signals equal number observations source signals mixing matrix square cases underdetermined investigated ica separation mixed signals gives good results based two assumptions three effects mixing source signals two assumptions three effects mixing source signals principles contribute basic establishment ica signals happen extract set mixtures independent like sources signals non gaussian histograms low complexity like source signals must source signals ica finds independent components maximizing statistical independence estimated components may choose one many ways define proxy independence choice governs form ica algorithm two broadest definitions independence ica minimization mutual information family ica algorithms uses measures like kullback leibler divergence maximum entropy non gaussianity family ica algorithms motivated central limit theorem uses kurtosis negentropy typical algorithms ica use centering whitening dimensionality reduction preprocessing steps order simplify reduce complexity problem actual iterative algorithm whitening dimension reduction achieved principal component analysis singular value decomposition whitening ensures dimensions treated equally priori algorithm run well known algorithms ica include infomax fastica jade kernel independent component analysis among others general ica cannot identify actual number source signals uniquely correct ordering source signals proper scaling source signals ica important blind signal separation many practical applications closely related search factorial code data e new vector valued representation data vector gets uniquely encoded resulting code vector code components statistically independent linear independent component analysis divided noiseless noisy cases noiseless ica special case noisy ica nonlinear ica considered separate case data represented observed random vector x displaystyle boldsymbol x hidden components random vector displaystyle boldsymbol task transform observed data x displaystyle boldsymbol x using linear static transformation w displaystyle boldsymbol w w x displaystyle boldsymbol boldsymbol w boldsymbol x vector maximally independent components displaystyle boldsymbol measured function f displaystyle f independence components x displaystyle x observed random vector x displaystyle boldsymbol x generated sum independent components k displaystyle k k n displaystyle k ldots n x k k n n displaystyle x cdots k k cdots n n weighted mixing weights k displaystyle k generative model written vector form x k n k k displaystyle boldsymbol x sum k n k boldsymbol k observed random vector x displaystyle boldsymbol x represented basis vectors k displaystyle boldsymbol k basis vectors k displaystyle boldsymbol k form columns mixing matrix displaystyle boldsymbol generative formula written x displaystyle boldsymbol x boldsymbol boldsymbol displaystyle boldsymbol given model realizations x x n displaystyle boldsymbol x ldots boldsymbol x n random vector x displaystyle boldsymbol x task estimate mixing matrix displaystyle boldsymbol sources displaystyle boldsymbol done adaptively calculating w displaystyle boldsymbol w vectors setting cost function either maximizes non gaussianity calculated k w x displaystyle k boldsymbol w boldsymbol x minimizes mutual information cases priori knowledge probability distributions sources used cost function original sources displaystyle boldsymbol recovered multiplying observed signals x displaystyle boldsymbol x inverse mixing matrix w displaystyle boldsymbol w boldsymbol also known unmixing matrix assumed mixing matrix square number basis vectors greater dimensionality observed vectors n displaystyle n task overcomplete still solvable pseudo inverse added assumption zero mean uncorrelated gaussian noise n n displaystyle n sim n ica model takes form x n displaystyle boldsymbol x boldsymbol boldsymbol n mixing sources need linear using nonlinear mixing function f displaystyle f parameters displaystyle theta nonlinear ica model x f n displaystyle x f n independent components identifiable permutation scaling sources identifiability requires special variant ica binary ica signal sources monitors binary form observations monitors disjunctive mixtures binary independent sources problem shown applications many domains including medical diagnosis multi cluster assignment network tomography internet resource management let x x x displaystyle x x ldots x set binary variables displaystyle monitors n displaystyle ldots n set binary variables n displaystyle n sources source monitor connections represented mixing matrix g textstyle boldsymbol g g j displaystyle g ij indicates signal th source observed j th monitor system works follows time source displaystyle active connected monitor j displaystyle j monitor j displaystyle j observe activity formally displaystyle wedge boolean displaystyle vee boolean note noise explicitly modelled rather treated independent sources problem heuristically solved assuming variables continuous running fastica binary observation data get mixing matrix g textstyle boldsymbol g apply round number techniques g textstyle boldsymbol g obtain binary values approach shown produce highly inaccurate result citation needed another method use dynamic programming recursively breaking observation matrix x textstyle boldsymbol x sub matrices run inference algorithm sub matrices key observation leads algorithm sub matrix x textstyle boldsymbol x x textstyle boldsymbol x x j j textstyle x ij forall j corresponds unbiased observation matrix hidden components connection displaystyle th monitor experimental results show approach accurate moderate noise levels generalized binary ica framework introduces broader problem formulation necessitate knowledge generative model words method attempts decompose source independent components prior assumption way generated although problem appears quite complex accurately solved branch bound search tree algorithm tightly upper bounded single multiplication matrix vector signal mixtures tend gaussian probability density functions source signals tend non gaussian probability density functions source signal extracted set signal mixtures taking inner product weight vector signal mixtures inner product provides orthogonal projection signal mixtures remaining challenge finding weight vector one type method projection pursuit projection pursuit seeks one projection time extracted signal non gaussian possible contrasts ica typically extracts signals simultaneously signal mixtures requires estimating unmixing matrix one practical advantage projection pursuit ica fewer signals extracted required source signal extracted signal mixtures using element weight vector use kurtosis recover multiple source signal finding correct weight vectors use projection pursuit kurtosis probability density function signal finite sample computed displaystyle mathbf overline sample mean displaystyle mathbf extracted signals constant ensures gaussian signals zero kurtosis super gaussian signals positive kurtosis sub gaussian signals negative kurtosis denominator variance displaystyle mathbf ensures measured kurtosis takes account signal variance goal projection pursuit maximize kurtosis make extracted signal non normal possible using kurtosis measure non normality examine kurtosis signal w x displaystyle mathbf mathbf w mathbf x extracted set mixtures x displaystyle mathbf x varies weight vector w displaystyle mathbf w rotated around origin given assumption source signal displaystyle mathbf super gaussian would expect multiple source mixture signals use kurtosis gram schmidt orthogonalization recover signals given signal mixtures dimensional space gso project data points onto dimensional space using weight vector guarantee independence extracted signals use gso order find correct value w displaystyle mathbf w use gradient descent method first whiten data transform x displaystyle mathbf x new mixture z displaystyle mathbf z unit variance z displaystyle mathbf z process achieved applying singular value decomposition x displaystyle mathbf x rescaling vector u u e displaystyle u u operatorname e let z u displaystyle mathbf z mathbf u signal extracted weighted vector w displaystyle mathbf w w z displaystyle mathbf mathbf w mathbf z weight vector w unit length e displaystyle operatorname e kurtosis written updating process w displaystyle mathbf w displaystyle eta small constant guarantee w displaystyle mathbf w converges optimal solution update normalize w n e w w n e w w n e w displaystyle mathbf w new frac mathbf w new mathbf w new set w l w n e w displaystyle mathbf w old mathbf w new repeat updating process convergence also use another algorithm update weight vector w displaystyle mathbf w another approach using negentropy instead kurtosis using negentropy robust method kurtosis kurtosis sensitive outliers negentropy methods based important property gaussian distribution gaussian variable largest entropy among continuous random variables equal variance also reason want find nongaussian variables simple proof found differential entropy gaussian random variable covariance matrix x approximation negentropy proof found original papers comon reproduced book independent component analysis aapo hyv rinen juha karhunen erkki oja approximation also suffers problem kurtosis approaches developed choice g displaystyle g g displaystyle g infomax ica essentially multivariate parallel version projection pursuit whereas projection pursuit extracts series signals one time set signal mixtures ica extracts signals parallel tends make ica robust projection pursuit projection pursuit method uses gram schmidt orthogonalization ensure independence extracted signal ica use infomax maximum likelihood estimate ensure independence extracted signal non normality extracted signal achieved assigning appropriate model prior signal process ica based infomax short given set signal mixtures x displaystyle mathbf x set identical independent model cumulative distribution functions g displaystyle g seek unmixing matrix w displaystyle mathbf w maximizes joint entropy signals g displaystyle mathbf g w x displaystyle mathbf mathbf wx signals extracted w displaystyle mathbf w given optimal w displaystyle mathbf w signals displaystyle mathbf maximum entropy therefore independent ensures extracted signals g displaystyle mathbf g also independent g displaystyle g invertible function signal model note source signal model probability density function p displaystyle p matches probability density function extracted signal p displaystyle p mathbf maximizing joint entropy displaystyle also maximizes amount mutual information x displaystyle mathbf x displaystyle mathbf reason using entropy extract independent signals known infomax consider entropy vector variable g displaystyle mathbf g w x displaystyle mathbf mathbf wx set signals extracted unmixing matrix w displaystyle mathbf w finite set values sampled distribution pdf p displaystyle p mathbf entropy displaystyle mathbf estimated joint pdf p displaystyle p mathbf shown related joint pdf p displaystyle p mathbf extracted signals multivariate form j displaystyle mathbf j frac partial mathbf partial mathbf jacobian matrix j g displaystyle mathbf j g g displaystyle g pdf assumed source signals g p displaystyle g p therefore therefore know p p displaystyle p mathbf p p displaystyle p mathbf uniform distribution h displaystyle h maximized since w displaystyle mathbf w absolute value determinant unmixing matix w displaystyle mathbf w therefore since h n n ln p x displaystyle h frac n sum n ln p mathbf x maximizing w displaystyle mathbf w affect h x displaystyle h mathbf x maximize function achieve independence extracted signal marginal pdfs model joint pdf p displaystyle p mathbf independent use commonly super gaussian model pdf source signals p displaystyle p mathbf sum given observed signal mixture x displaystyle mathbf x corresponding set extracted signals displaystyle mathbf source signal model p g displaystyle p mathbf g find optimal unmixing matrix w displaystyle mathbf w make extracted signals independent non gaussian like projection pursuit situation use gradient descent method find optimal solution unmixing matrix maximum likelihood estimation standard statistical tool finding parameter values provide best fit data given model p displaystyle p source signals ml model includes specification pdf case pdf p displaystyle p unknown source signals displaystyle using ml ica objective find unmixing matrix yields extracted signals w x displaystyle mathbf w x joint pdf similar possible joint pdf p displaystyle p unknown source signals displaystyle mle thus based assumption model pdf p displaystyle p model parameters displaystyle mathbf correct high probability obtained data x displaystyle x actually observed conversely displaystyle mathbf far correct parameter values low probability observed data would expected using mle call probability observed data given set model parameter values likelihood model parameter values given observed data define likelihood function l displaystyle mathbf l w displaystyle mathbf w l p det w displaystyle mathbf l p det mathbf w equals probability density x displaystyle x since w x displaystyle mathbf w x thus wish find w displaystyle mathbf w likely generated observed mixtures x displaystyle x unknown source signals displaystyle pdf p displaystyle p need find w displaystyle mathbf w maximizes likelihood l displaystyle mathbf l unmixing matrix maximizes equation known mle optimal unmixing matrix common practice use log likelihood easier evaluate logarithm monotonic function w displaystyle mathbf w maximizes function l displaystyle mathbf l also maximizes logarithm ln l displaystyle ln mathbf l allows us take logarithm equation yields log likelihood function ln l ln p n ln det w displaystyle ln mathbf l sum sum ln p n ln det mathbf w substitute commonly used high kurtosis model pdf source signals p displaystyle p ln l n n ln ln det w displaystyle ln mathbf l n sum sum n ln ln det mathbf w matrix w displaystyle mathbf w maximizes function maximum likelihood estimation early general framework independent component analysis introduced jeanny h rault bernard ans developed christian jutten refined pierre comon popularized paper tony bell terry sejnowski introduced fast efficient ica algorithm based infomax principle introduced ralph linsker many algorithms available literature ica largely used one including industrial applications fastica algorithm developed hyv rinen oja uses kurtosis cost function examples rather related blind source separation general approach used example one drop independence assumption separate mutually correlated signals thus statistically dependent signals sepp hochreiter j rgen schmidhuber showed obtain non linear ica source separation product regularization method require priori knowledge number independent sources ica extended analyze non physical signals instance ica applied discover discussion topics bag news list archives ica applications listed
https://en.wikipedia.org/wiki/Linear_discriminant_analysis,linear discriminant analysis normal discriminant analysis discriminant function analysis generalization fisher linear discriminant method used statistics pattern recognition machine learning find linear combination features characterizes separates two classes objects events resulting combination may used linear classifier commonly dimensionality reduction later classification lda closely related analysis variance regression analysis also attempt express one dependent variable linear combination features measurements however anova uses categorical independent variables continuous dependent variable whereas discriminant analysis continuous independent variables categorical dependent variable logistic regression probit regression similar lda anova also explain categorical variable values continuous independent variables methods preferable applications reasonable assume independent variables normally distributed fundamental assumption lda method lda also closely related principal component analysis factor analysis look linear combinations variables best explain data lda explicitly attempts model difference classes data pca contrast take account difference class factor analysis builds feature combinations based differences rather similarities discriminant analysis also different factor analysis interdependence technique distinction independent variables dependent variables must made lda works measurements made independent variables observation continuous quantities dealing categorical independent variables equivalent technique discriminant correspondence analysis discriminant analysis used groups known priori case must score one quantitative predictor measures score group measure simple terms discriminant function analysis classification act distributing things groups classes categories type original dichotomous discriminant analysis developed sir ronald fisher different anova manova used predict one multiple continuous dependent variables one independent categorical variables discriminant function analysis useful determining whether set variables effective predicting category membership consider set observations x displaystyle vec x sample object event known class displaystyle set samples called training set classification problem find good predictor class displaystyle sample distribution given observation x displaystyle vec x lda approaches problem assuming conditional probability density functions p displaystyle p p displaystyle p normally distributed mean covariance parameters displaystyle left displaystyle left respectively assumption bayes optimal solution predict points second class log likelihood ratios bigger threshold without assumptions resulting classifier referred qda lda instead makes additional simplifying homoscedasticity assumption covariances full rank case several terms cancel decision criterion becomes threshold dot product threshold constant c means criterion input x displaystyle vec x class displaystyle purely function linear combination known observations often useful see conclusion geometrical terms criterion input x displaystyle vec x class displaystyle purely function projection multidimensional space point x displaystyle vec x onto vector w displaystyle vec w words observation belongs displaystyle corresponding x displaystyle vec x located certain side hyperplane perpendicular w displaystyle vec w location plane defined threshold c assumptions discriminant analysis manova analysis quite sensitive outliers size smallest group must larger number predictor variables suggested discriminant analysis relatively robust slight violations assumptions also shown discriminant analysis may still reliable using dichotomous variables discriminant analysis works creating one linear combinations predictors creating new latent variable function functions called discriminant functions number functions possible either n g displaystyle n g n g displaystyle n g number groups p displaystyle p whichever smaller first function created maximizes differences groups function second function maximizes differences function also must correlated previous function continues subsequent functions requirement new function correlated previous functions given group j displaystyle j r j displaystyle mathbb r j sets sample space discriminant rule x r j displaystyle x mathbb r j x j displaystyle x j discriminant analysis finds good regions r j displaystyle mathbb r j minimize classification error therefore leading high percent correct classified classification table function given discriminant score clarification needed determine well predicts group placement eigenvalue discriminant analysis characteristic root function clarification needed indication well function differentiates groups larger eigenvalue better function differentiates however interpreted caution eigenvalues upper limit eigenvalue viewed ratio ssbetween sswithin anova dependent variable discriminant function groups levels iv clarification needed means largest eigenvalue associated first function second largest second etc suggest use eigenvalues effect size measures however generally supported instead canonical correlation preferred measure effect size similar eigenvalue square root ratio ssbetween sstotal correlation groups function another popular measure effect size percent variance clarification needed function calculated x x eigenvalue function sum eigenvalues tells us strong prediction particular function compared others percent correctly classified also analyzed effect size kappa value describe correcting chance agreement kappa normalizes across categorizes rather biased significantly good poorly performing classes clarification needed canonical discriminant analysis finds axes best separate categories linear functions uncorrelated define effect optimal k space n dimensional cloud data best separates k groups see multiclass lda details terms fisher linear discriminant lda often used interchangeably although fisher original article actually describes slightly different discriminant make assumptions lda normally distributed classes equal class covariances suppose two classes observations means displaystyle vec mu vec mu covariances displaystyle sigma sigma linear combination features w x displaystyle vec w cdot vec x means w displaystyle vec w cdot vec mu variances w w displaystyle vec w sigma vec w displaystyle fisher defined separation two distributions ratio variance classes variance within classes measure sense measure signal noise ratio class labelling shown maximum separation occurs assumptions lda satisfied equation equivalent lda sure note vector w displaystyle vec w normal discriminant hyperplane example two dimensional problem line best divides two groups perpendicular w displaystyle vec w generally data points discriminated projected onto w displaystyle vec w threshold best separates data chosen analysis one dimensional distribution general rule threshold however projections points classes exhibit approximately distributions good choice would hyperplane projections two means w displaystyle vec w cdot vec mu w displaystyle vec w cdot vec mu case parameter c threshold condition w x c displaystyle vec w cdot vec x c found explicitly otsu method related fisher linear discriminant created binarize histogram pixels grayscale image optimally picking black white threshold minimizes intra class variance maximizes inter class variance within grayscales assigned black white pixel classes case two classes analysis used derivation fisher discriminant extended find subspace appears contain class variability generalization due c r rao suppose c classes mean displaystyle mu covariance displaystyle sigma scatter class variability may defined sample covariance class means displaystyle mu mean class means class separation direction w displaystyle vec w case given means w displaystyle vec w eigenvector b displaystyle sigma sigma b separation equal corresponding eigenvalue b displaystyle sigma sigma b diagonalizable variability features contained subspace spanned eigenvectors corresponding c largest eigenvalues eigenvectors primarily used feature reduction pca eigenvectors corresponding smaller eigenvalues tend sensitive exact choice training data often necessary use regularisation described next section classification required instead dimension reduction number alternative techniques available instance classes may partitioned standard fisher discriminant lda used classify partition common example one rest points one class put one group everything else lda applied result c classifiers whose results combined another common method pairwise classification new classifier created pair classes classifiers total individual classifiers combined produce final classification typical implementation lda technique requires samples available advance however situations entire data set available input data observed stream case desirable lda feature extraction ability update computed lda features observing new samples without running algorithm whole data set example many real time applications mobile robotics line face recognition important update extracted lda features soon new observations available lda feature extraction technique update lda features simply observing new samples incremental lda algorithm idea extensively studied last two decades chatterjee roychowdhury proposed incremental self organized lda algorithm updating lda features work demir ozmehmet proposed online local learning algorithms updating lda features incrementally using error correcting hebbian learning rules later aliyari et al derived fast incremental algorithms update lda features observing new samples practice class means covariances known however estimated training set either maximum likelihood estimate maximum posteriori estimate may used place exact value equations although estimates covariance may considered optimal sense mean resulting discriminant obtained substituting values optimal sense even assumption normally distributed classes correct another complication applying lda fisher discriminant real data occurs number measurements sample exceeds number samples class case covariance estimates full rank cannot inverted number ways deal one use pseudo inverse instead usual matrix inverse formulae however better numeric stability may achieved first projecting problem onto subspace spanned b displaystyle sigma b another strategy deal small sample size use shrinkage estimator covariance matrix expressed mathematically displaystyle identity matrix displaystyle lambda shrinkage intensity regularisation parameter leads framework regularized discriminant analysis shrinkage discriminant analysis also many practical cases linear discriminants suitable lda fisher discriminant extended use non linear classification via kernel trick original observations effectively mapped higher dimensional non linear space linear classification non linear space equivalent non linear classification original space commonly used example kernel fisher discriminant lda generalized multiple discriminant analysis c becomes categorical variable n possible states instead two analogously class conditional densities p displaystyle p normal shared covariances sufficient statistic p displaystyle p values n projections subspace spanned n means affine projected inverse covariance matrix projections found solving generalized eigenvalue problem numerator covariance matrix formed treating means samples denominator shared covariance matrix see multiclass lda details addition examples given lda applied positioning product management bankruptcy prediction based accounting ratios financial variables linear discriminant analysis first statistical method applied systematically explain firms entered bankruptcy vs survived despite limitations including known nonconformance accounting ratios normal distribution assumptions lda edward altman model still leading model practical applications computerised face recognition face represented large number pixel values linear discriminant analysis primarily used reduce number features manageable number classification new dimensions linear combination pixel values form template linear combinations obtained using fisher linear discriminant called fisher faces obtained using related principal component analysis called eigenfaces marketing discriminant analysis often used determine factors distinguish different types customers products basis surveys forms collected data logistic regression methods commonly used use discriminant analysis marketing described following steps main application discriminant analysis medicine assessment severity state patient prognosis disease outcome example retrospective analysis patients divided groups according severity disease mild moderate severe form results clinical laboratory analyses studied order reveal variables statistically different studied groups using variables discriminant functions built help objectively classify disease future patient mild moderate severe form biology similar principles used order classify define groups different biological objects example define phage types salmonella enteritidis based fourier transform infrared spectra detect animal source escherichia coli studying virulence factors etc method used separate alteration zones example different data various zones available discriminant analysis find pattern within data classify effectively discriminant function analysis similar logistic regression used answer research questions logistic regression many assumptions restrictions discriminant analysis however discriminant analysis assumptions met powerful logistic regression unlike logistic regression discriminant analysis used small sample sizes shown sample sizes equal homogeneity variance covariance holds discriminant analysis accurate despite advantages logistic regression none less become common choice since assumptions discriminant analysis rarely met geometric anomalities high dimension lead well known curse dimensionality nevertheless proper utilization concentration measure phenomena make computation easier important case blessing dimensionality phenomena highlighted donoho tanner sample essentially high dimensional point separated rest sample linear inequality high probability even exponentially large samples linear inequalities selected standard form linear discriminant rich family probability distribution particular theorems proven log concave distributions including multidimensional normal distribution product measures multidimensional cube data separability classical linear discriminants simplifies problem error correction artificial intelligence systems high dimension
https://en.wikipedia.org/wiki/Non-negative_matrix_factorization,non negative matrix factorization also non negative matrix approximation group algorithms multivariate analysis linear algebra matrix v factorized two matrices w h property three matrices negative elements non negativity makes resulting matrices easier inspect also applications processing audio spectrograms muscular activity non negativity inherent data considered since problem exactly solvable general commonly approximated numerically nmf finds applications fields astronomy computer vision document clustering missing data imputation chemometrics audio signal processing recommender systems bioinformatics chemometrics non negative matrix factorization long history name self modeling curve resolution framework vectors right matrix continuous curves rather discrete vectors also early work non negative matrix factorizations performed finnish group researchers name positive matrix factorization became widely known non negative matrix factorization lee seung investigated properties algorithm published simple useful algorithms two types factorizations let matrix v product matrices w h matrix multiplication implemented computing column vectors v linear combinations column vectors w using coefficients supplied columns h column v computed follows vi th column vector product matrix v hi th column vector matrix h multiplying matrices dimensions factor matrices may significantly lower product matrix property forms basis nmf nmf generates factors significantly reduced dimensions compared original matrix example v n matrix w p matrix h p n matrix p significantly less n example based text mining application last point basis nmf consider original document example built small set hidden features nmf generates features useful think feature features matrix w document archetype comprising set words word cell value defines word rank feature higher word cell value higher word rank feature column coefficients matrix h represents original document cell value defining document rank feature reconstruct document input matrix linear combination features feature weighted feature cell value document column h nmf inherent clustering property e automatically clusters columns input data v displaystyle mathbf v specifically approximation v displaystyle mathbf v v w h displaystyle mathbf v simeq mathbf w mathbf h achieved finding w displaystyle w h displaystyle h minimize error function v w h f displaystyle v wh f subject w h displaystyle w geq h geq furthermore impose orthogonality constraint h displaystyle mathbf h e h h displaystyle mathbf h mathbf h minimization mathematically equivalent minimization k means clustering furthermore computed h displaystyle h gives cluster membership e h k j h j displaystyle mathbf h kj mathbf h ij k suggests input data v j displaystyle v j belongs k h displaystyle k th cluster computed w displaystyle w gives cluster centroids e k h displaystyle k th column gives cluster centroid k h displaystyle k th cluster centroid representation significantly enhanced convex nmf orthogonality constraint h h displaystyle mathbf h mathbf h explicitly imposed orthogonality holds large extent clustering property holds clustering main objective data mining applications nmf citation needed error function used kullback leibler divergence nmf identical probabilistic latent semantic analysis popular document clustering method usually number columns w number rows h nmf selected product wh become approximation v full decomposition v amounts two non negative matrices w h well residual u v wh u elements residual matrix either negative positive w h smaller v become easier store manipulate another reason factorizing v smaller matrices w h one able approximately represent elements v significantly less data one infer latent structure data standard nmf matrix factor w k e w anything space convex nmf restricts columns w convex combinations input data vectors displaystyle greatly improves quality data representation w furthermore resulting matrix factor h becomes sparse orthogonal case nonnegative rank v equal actual rank v wh called nonnegative rank factorization problem finding nrf v exists known np hard different types non negative matrix factorizations different types arise using different cost functions measuring divergence v wh possibly regularization w h matrices two simple divergence functions studied lee seung squared error extension kullback leibler divergence positive matrices divergence leads different nmf algorithm usually minimizing divergence using iterative update rules factorization problem squared error version nmf may stated given matrix v displaystyle mathbf v find nonnegative matrices w h minimize function another type nmf images based total variation norm l regularization added nmf mean squared error cost function resulting problem may called non negative sparse coding due similarity sparse coding problem although may also still referred nmf many standard nmf algorithms analyze data together e whole matrix available start may unsatisfactory applications many data fit memory data provided streaming fashion one use collaborative filtering recommendation systems may many users many items recommend would inefficient recalculate everything one user one item added system cost function optimization cases may may standard nmf algorithms need rather different several ways w h may found lee seung multiplicative update rule popular method due simplicity implementation algorithm note updates done element element basis matrix multiplication note multiplicative factors w h e w v w w h textstyle frac mathbf w mathsf mathbf v mathbf w mathsf mathbf w mathbf h v h w h h textstyle textstyle frac mathbf v mathbf h mathsf mathbf w mathbf h mathbf h mathsf terms matrices ones v w h displaystyle mathbf v mathbf w mathbf h recently algorithms developed approaches based alternating non negative least squares step algorithm first h fixed w found non negative least squares solver w fixed h found analogously procedures used solve w h may different nmf variants regularize one w h specific approaches include projected gradient descent methods active set method optimal gradient method block principal pivoting method among several others current algorithms sub optimal guarantee finding local minimum rather global minimum cost function provably optimal algorithm unlikely near future problem shown generalize k means clustering problem known np complete however many data mining applications local minimum may still prove useful sequential construction nmf components firstly used relate nmf principal component analysis astronomy contribution pca components ranked magnitude corresponding eigenvalues nmf components ranked empirically constructed one one e learn displaystyle th component first n displaystyle n components constructed contribution sequential nmf components compared karhunen lo theorem application pca using plot eigenvalues typical choice number components pca based elbow point existence flat plateau indicating pca capturing data efficiently last exists sudden drop reflecting capture random noise falls regime overfitting sequential nmf plot eigenvalues approximated plot fractional residual variance curves curves decreases continuously converge higher level pca indication less fitting sequential nmf exact solutions variants nmf expected additional constraints hold matrix v polynomial time algorithm solving nonnegative rank factorization v contains monomial sub matrix rank equal rank given campbell poole kalofolias gallopoulos solved symmetric counterpart problem v symmetric contains diagonal principal sub matrix rank r algorithm runs time dense case arora ge halpern mimno moitra sontag wu zhu give polynomial time algorithm exact nmf works case one factors w satisfies separability condition learning parts objects non negative matrix factorization lee seung proposed nmf mainly parts based decomposition images compares nmf vector quantization principal component analysis shows although three techniques may written factorizations implement different constraints therefore produce different results later shown types nmf instance general probabilistic model called multinomial pca nmf obtained minimizing kullback leibler divergence fact equivalent another instance multinomial pca probabilistic latent semantic analysis trained maximum likelihood estimation method commonly used analyzing clustering textual data also related latent class model nmf least squares objective equivalent relaxed form k means clustering matrix factor w contains cluster centroids h contains cluster membership indicators provides theoretical foundation using nmf data clustering however k means enforce non negativity centroids closest analogy fact semi nmf nmf seen two layer directed graphical model one layer observed random variables one layer hidden random variables nmf extends beyond matrices tensors arbitrary order extension may viewed non negative counterpart e g parafac model extensions nmf include joint factorization several data matrices tensors factors shared models useful sensor fusion relational learning nmf instance nonnegative quadratic programming like support vector machine however svm nmf related intimate level nqp allows direct application solution algorithms developed either two methods problems domains factorization unique matrix inverse used transform two factorization matrices e g two new matrices w w b displaystyle mathbf tilde w wb h b h displaystyle mathbf tilde h mathbf b mathbf h non negative form another parametrization factorization non negativity w displaystyle mathbf tilde w h displaystyle mathbf tilde h applies least b non negative monomial matrix simple case correspond scaling permutation control non uniqueness nmf obtained sparsity constraints astronomy nmf promising method dimension reduction sense astrophysical signals non negative nmf applied spectroscopic observations direct imaging observations method study common properties astronomical objects post process astronomical observations advances spectroscopic observations blanton roweis takes account uncertainties astronomical observations later improved zhu missing data also considered parallel computing enabled method adopted ren et al direct imaging field one methods detecting exoplanets especially direct imaging circumstellar disks ren et al able prove stability nmf components constructed sequentially enables linearity nmf modeling process linearity property used separate stellar light light scattered exoplanets circumstellar disks direct imaging reveal faint exoplanets circumstellar disks bright surrounding stellar lights typical contrast various statistical methods adopted however light exoplanets circumstellar disks usually fitted forward modeling adopted recover true flux forward modeling currently optimized point sources however extended sources especially irregularly shaped structures circumstellar disks situation nmf excellent method less fitting sense non negativity sparsity nmf modeling coefficients therefore forward modeling performed scaling factors rather computationally intensive data reduction generated models impute missing data statistics nmf take missing data minimizing cost function rather treating missing data zeros makes mathematically proven method data imputation statistics first proving missing data ignored cost function proving impact missing data small second order effect ren et al studied applied approach field astronomy work focuses two dimensional matrices specifically includes mathematical derivation simulated data imputation application sky data data imputation procedure nmf composed two steps first nmf components known ren et al proved impact missing data data imputation second order effect second nmf components unknown authors proved impact missing data component construction first second order effect depending way nmf components obtained former step either independent dependent latter addition imputation quality increased nmf components used see figure ren et al illustration nmf used text mining applications process document term matrix constructed weights various terms set documents matrix factored term feature feature document matrix features derived contents documents feature document matrix describes data clusters related documents one specific application used hierarchical nmf small subset scientific abstracts pubmed another research group clustered parts enron email dataset messages terms clusters nmf also applied citations data one example clustering english wikipedia articles scientific journals based outbound scientific citations english wikipedia arora ge halpern mimno moitra sontag wu zhu given polynomial time algorithms learn topic models using nmf algorithm assumes topic matrix satisfies separability condition often found hold settings hassani iranmanesh mansouri proposed feature agglomeration method term document matrices operates using nmf algorithm reduces term document matrix smaller matrix suitable text clustering nmf also used analyze spectral data one use classification space objects debris nmf applied scalable internet distance prediction network n displaystyle n hosts help nmf distances n displaystyle n end end links predicted conducting displaystyle measurements kind method firstly introduced internet distance estimation service afterwards fully decentralized approach phoenix network coordinate system proposed achieves better overall prediction accuracy introducing concept weight speech denoising long lasting problem audio signal processing many algorithms denoising noise stationary example wiener filter suitable additive gaussian noise however noise non stationary classical denoising algorithms usually poor performance statistical information non stationary noise difficult estimate schmidt et al use nmf speech denoising non stationary noise completely different classical statistical approaches key idea clean speech signal sparsely represented speech dictionary non stationary noise cannot similarly non stationary noise also sparsely represented noise dictionary speech cannot algorithm nmf denoising goes follows two dictionaries one speech one noise need trained offline noisy speech given first calculate magnitude short time fourier transform second separate two parts via nmf one sparsely represented speech dictionary part sparsely represented noise dictionary third part represented speech dictionary estimated clean speech sparse nmf used population genetics estimating individual admixture coefficients detecting genetic clusters individuals population sample evaluating genetic admixture sampled genomes human genetic clustering nmf algorithms provide estimates similar computer program structure algorithms efficient computationally allow analysis large population genomic data sets nmf successfully applied bioinformatics clustering gene expression dna methylation data finding genes representative clusters analysis cancer mutations used identify common patterns mutations occur many cancers probably distinct causes nmf techniques identify sources variation cell types disease subtypes population stratification tissue composition tumor clonality nmf also referred field factor analysis used since analyze sequences images spect pet dynamic medical imaging non uniqueness nmf addressed using sparsity constraints current research nonnegative matrix factorization includes limited
https://en.wikipedia.org/wiki/Principal_component_analysis,principal components collection points real p space sequence p displaystyle p direction vectors h displaystyle th vector direction line best fits data orthogonal first displaystyle vectors best fitting line defined one minimizes average squared distance points line directions constitute orthonormal basis different individual dimensions data linearly uncorrelated principal component analysis process computing principal components using perform change basis data sometimes using first principal components ignoring rest pca used exploratory data analysis making predictive models commonly used dimensionality reduction projecting data point onto first principal components obtain lower dimensional data preserving much data variation possible first principal component equivalently defined direction maximizes variance projected data h displaystyle th principal component taken direction orthogonal first displaystyle principal components maximizes variance projected data either objective shown principal components eigenvectors data covariance matrix thus principal components often computed eigendecomposition data covariance matrix singular value decomposition data matrix pca simplest true eigenvector based multivariate analyses closely related factor analysis factor analysis typically incorporates domain specific assumptions underlying structure solves eigenvectors slightly different matrix pca also related canonical correlation analysis cca defines coordinate systems optimally describe cross covariance two datasets pca defines new orthogonal coordinate system optimally describes variance single dataset robust l norm based variants standard pca also proposed pca invented karl pearson analogue principal axis theorem mechanics later independently developed named harold hotelling depending field application also named discrete karhunen lo transform signal processing hotelling transform multivariate quality control proper orthogonal decomposition mechanical engineering singular value decomposition x eigenvalue decomposition xtx linear algebra factor analysis eckart young theorem empirical orthogonal functions meteorological science empirical eigenfunction decomposition empirical component analysis quasiharmonic modes spectral decomposition noise vibration empirical modal analysis structural dynamics pca thought fitting p dimensional ellipsoid data axis ellipsoid represents principal component axis ellipsoid small variance along axis also small find axes ellipsoid must first subtract mean variable dataset center data around origin compute covariance matrix data calculate eigenvalues corresponding eigenvectors covariance matrix must normalize orthogonal eigenvectors turn unit vectors done mutually orthogonal unit eigenvectors interpreted axis ellipsoid fitted data choice basis transform covariance matrix diagonalised form diagonal elements representing variance axis proportion variance eigenvector represents calculated dividing eigenvalue corresponding eigenvector sum eigenvalues pca defined orthogonal linear transformation transforms data new coordinate system greatest variance scalar projection data comes lie first coordinate second greatest variance second coordinate consider n p displaystyle n times p data matrix x column wise zero empirical mean n rows represents different repetition experiment p columns gives particular kind feature mathematically transformation defined set size l displaystyle l p dimensional vectors weights coefficients w displaystyle mathbf w map row vector x displaystyle mathbf x x new vector principal component scores displaystyle mathbf given way individual variables l displaystyle dots l considered data set successively inherit maximum possible variance x coefficient vector w constrained unit vector order maximize variance first weight vector w thus satisfy equivalently writing matrix form gives since w defined unit vector equivalently also satisfies quantity maximised recognised rayleigh quotient standard result positive semidefinite matrix xtx quotient maximum possible value largest eigenvalue matrix occurs w corresponding eigenvector w found first principal component data vector x given score x w transformed co ordinates corresponding vector original variables x w w kth component found subtracting first k principal components x finding weight vector extracts maximum variance new data matrix turns gives remaining eigenvectors xtx maximum values quantity brackets given corresponding eigenvalues thus weight vectors eigenvectors xtx kth principal component data vector x therefore given score tk x w transformed co ordinates corresponding vector space original variables x w w w kth eigenvector xtx full principal components decomposition x therefore given w p p matrix weights whose columns eigenvectors xtx transpose w sometimes called whitening sphering transformation columns w multiplied square root corresponding eigenvalues eigenvectors scaled variances called loadings pca factor analysis xtx recognised proportional empirical sample covariance matrix dataset xt sample covariance q two different principal components dataset given eigenvalue property w used move line line however eigenvectors w w corresponding eigenvalues symmetric matrix orthogonal orthogonalised product final line therefore zero sample covariance different principal components dataset another way characterise principal components transformation therefore transformation coordinates diagonalise empirical sample covariance matrix matrix form empirical covariance matrix original variables written empirical covariance matrix principal components becomes diagonal matrix eigenvalues xtx equal sum squares dataset associated component k tk w transformation x w maps data vector x original space p variables new space p variables uncorrelated dataset however principal components need kept keeping first l principal components produced using first l eigenvectors gives truncated transformation matrix tl n rows l columns words pca learns linear transformation w x x r p r l displaystyle w x x r p r l columns p l matrix w form orthogonal basis l features decorrelated construction transformed data matrices l columns score matrix maximises variance original data preserved minimising total squared reconstruction error w l w l displaystyle mathbf mathbf w mathbf l mathbf w l x x l displaystyle mathbf x mathbf x l dimensionality reduction useful step visualising processing high dimensional datasets still retaining much variance dataset possible example selecting l keeping first two principal components finds two dimensional plane high dimensional dataset data spread data contains clusters may spread therefore visible plotted two dimensional diagram whereas two directions data chosen random clusters may much less spread apart may fact much likely substantially overlay making indistinguishable similarly regression analysis larger number explanatory variables allowed greater chance overfitting model producing conclusions fail generalise datasets one approach especially strong correlations different possible explanatory variables reduce principal components run regression method called principal component regression dimensionality reduction may also appropriate variables dataset noisy column dataset contains independent identically distributed gaussian noise columns also contain similarly identically distributed gaussian noise however total variance concentrated first principal components compared noise variance proportionate effect noise less first components achieve higher signal noise ratio pca thus effect concentrating much signal first principal components usefully captured dimensionality reduction later principal components may dominated noise disposed without great loss dataset large significance principal components tested using parametric bootstrap aid determining many principal components retain principal components transformation also associated another matrix factorization singular value decomposition x n p rectangular diagonal matrix positive numbers called singular values x u n n matrix columns orthogonal unit vectors length n called left singular vectors x w p p whose columns orthogonal unit vectors length p called right singular vectors x terms factorization matrix xtx written displaystyle mathbf hat sigma square diagonal matrix singular values x excess zeros chopped satisfies displaystyle mathbf hat sigma mathbf sigma mathbf sigma comparison eigenvector factorization xtx establishes right singular vectors w x equivalent eigenvectors xtx singular values x displaystyle mathbf x equal square root eigenvalues xtx using singular value decomposition score matrix written column given one left singular vectors x multiplied corresponding singular value form also polar decomposition efficient algorithms exist calculate svd x without form matrix xtx computing svd standard way calculate principal components analysis data matrix citation needed unless handful components required eigen decomposition truncated n l score matrix tl obtained considering first l largest singular values singular vectors truncation matrix using truncated singular value decomposition way produces truncated matrix nearest possible matrix rank l original matrix sense difference two smallest possible frobenius norm result known eckart young theorem given set points euclidean space first principal component corresponds line passes multidimensional mean minimizes sum squares distances points line second principal component corresponds concept correlation first principal component subtracted points singular values square roots eigenvalues matrix xtx eigenvalue proportional portion variance associated eigenvector sum eigenvalues equal sum squared distances points multidimensional mean pca essentially rotates set points around mean order align principal components moves much variance possible first dimensions values remaining dimensions therefore tend small may dropped minimal loss information pca often used manner dimensionality reduction pca distinction optimal orthogonal transformation keeping subspace largest variance advantage however comes price greater computational requirements compared example applicable discrete cosine transform particular dct ii simply known dct nonlinear dimensionality reduction techniques tend computationally demanding pca pca sensitive scaling variables two variables sample variance positively correlated pca entail rotation weights two variables respect principal component equal multiply values first variable first principal component almost variable small contribution variable whereas second component almost aligned second original variable means whenever different variables different units pca somewhat arbitrary method analysis pearson original paper entitled lines planes closest fit systems points space space implies physical euclidean space concerns arise one way making pca less arbitrary use variables scaled unit variance standardizing data hence use autocorrelation matrix instead autocovariance matrix basis pca however compresses fluctuations dimensions signal space unit variance mean subtraction necessary performing classical pca ensure first principal component describes direction maximum variance mean subtraction performed first principal component might instead correspond less mean data mean zero needed finding basis minimizes mean square error approximation data mean centering unnecessary performing principal components analysis correlation matrix data already centered calculating correlations correlations derived cross product two standard scores statistical moments also see article kromrey foster johnson mean centering moderated regression much ado nothing pca popular primary technique pattern recognition however optimized class separability however used quantify distance two classes calculating center mass class principal component space reporting euclidean distance center mass two classes linear discriminant analysis alternative optimized class separability properties pca include statistical implication property last pcs simply unstructured left overs removing important pcs last pcs variances small possible useful right help detect unsuspected near constant linear relationships elements x may also useful regression selecting subset variables x outlier detection look usage first look diagonal elements perhaps main statistical implication result decompose combined variances elements x decreasing contributions due pc also decompose whole covariance matrix contributions k k k displaystyle lambda k alpha k alpha k pc although strictly decreasing elements k k k displaystyle lambda k alpha k alpha k tend become smaller k displaystyle k increases k k k displaystyle lambda k alpha k alpha k nonincreasing increasing k displaystyle k whereas elements k displaystyle alpha k tend stay size normalization constraints k k k p displaystyle alpha k alpha k k dots p noted results pca depend scaling variables cured scaling feature standard deviation one ends dimensionless features unital variance applicability pca described limited certain assumptions made derivation particular pca capture linear correlations features fails assumption violated cases coordinate transformations restore linearity assumption pca applied another limitation mean removal process constructing covariance matrix pca fields astronomy signals non negative mean removal process force mean astrophysical exposures zero consequently creates unphysical negative fluxes forward modeling performed recover true magnitude signals alternative method non negative matrix factorization focusing non negative elements matrices well suited astrophysical observations see relation pca non negative matrix factorization dimensionality reduction loses information general pca based dimensionality reduction tends minimize information loss certain signal noise models assumption data vector x displaystyle mathbf x sum desired information bearing signal displaystyle mathbf noise signal n displaystyle mathbf n one show pca optimal dimensionality reduction information theoretic point view particular linsker showed displaystyle mathbf gaussian n displaystyle mathbf n gaussian noise covariance matrix proportional identity matrix pca maximizes mutual information displaystyle desired information displaystyle mathbf dimensionality reduced output w l x displaystyle mathbf mathbf w l mathbf x noise still gaussian covariance matrix proportional identity matrix information bearing signal displaystyle mathbf non gaussian pca least minimizes upper bound information loss defined optimality pca also preserved noise n displaystyle mathbf n iid least gaussian information bearing signal displaystyle mathbf general even signal model holds pca loses information theoretic optimality soon noise n displaystyle mathbf n becomes dependent following detailed description pca using covariance method opposed correlation method goal transform given data set x dimension p alternative data set smaller dimension l equivalently seeking find matrix karhunen lo transform matrix x suppose data comprising set observations p variables want reduce data observation described l variables l p suppose data arranged set n data vectors x x n displaystyle mathbf x ldots mathbf x n x displaystyle mathbf x representing single grouped observation p variables mean subtraction integral part solution towards finding principal component basis minimizes mean square error approximating data hence proceed centering data follows applications variable may also scaled variance equal step affects calculated principal components makes independent units used measure different variables first column displaystyle mathbf projection data points onto first principal component second column projection onto second principal component etc let x dimensional random vector expressed column vector without loss generality assume x zero mean want find displaystyle orthonormal transformation matrix p px diagonal covariance matrix quick computation assuming p displaystyle p unitary yields hence displaystyle holds cov displaystyle operatorname cov diagonalisable p displaystyle p constructive cov guaranteed non negative definite matrix thus guaranteed diagonalisable unitary matrix practical implementations especially high dimensional data naive covariance method rarely used efficient due high computational memory costs explicitly determining covariance matrix covariance free approach avoids np operations explicitly calculating storing covariance matrix xtx instead utilizing one matrix free methods example based function evaluating product xt cost np operations one way compute first principal component efficiently shown following pseudo code data matrix x zero mean without ever computing covariance matrix power iteration algorithm simply calculates vector xt normalizes places result back r eigenvalue approximated rt r rayleigh quotient unit vector r covariance matrix xtx largest singular value well separated next largest one vector r gets close first principal component x within number iterations c small relative p total cost cnp power iteration convergence accelerated without noticeably sacrificing small cost per iteration using advanced matrix free methods lanczos algorithm locally optimal block preconditioned conjugate gradient method subsequent principal components computed one one via deflation simultaneously block former approach imprecisions already computed approximate principal components additively affect accuracy subsequently computed principal components thus increasing error every new computation latter approach block power method replaces single vectors r block vectors matrices r every column r approximates one leading principal components columns iterated simultaneously main calculation evaluation product xt implemented example lobpcg efficient blocking eliminates accumulation errors allows using high level blas matrix matrix product functions typically leads faster convergence compared single vector one one technique non linear iterative partial least squares variant classical power iteration matrix deflation subtraction implemented computing first components principal component partial least squares analysis high dimensional datasets generated omics sciences usually necessary compute first pcs non linear iterative partial least squares algorithm updates iterative approximations leading scores loadings r power iteration multiplying every iteration x left right calculation covariance matrix avoided matrix free implementation power iterations xtx based function evaluating product xt tx matrix deflation subtraction performed subtracting outer product r x leaving deflated residual matrix used calculate subsequent leading pcs large data matrices matrices high degree column collinearity nipals suffers loss orthogonality pcs due machine precision round errors accumulated iteration matrix deflation subtraction gram schmidt orthogonalization algorithm applied scores loadings iteration step eliminate loss orthogonality nipals reliance single vector multiplications cannot take advantage high level blas results slow convergence clustered leading singular values deficiencies resolved sophisticated matrix free block solvers locally optimal block preconditioned conjugate gradient method online streaming situation data arriving piece piece rather stored single batch useful make estimate pca projection updated sequentially done efficiently requires different algorithms pca common want introduce qualitative variables supplementary elements example many quantitative variables measured plants plants qualitative variables available example species plant belongs data subjected pca quantitative variables analyzing results natural connect principal components qualitative variable species following results produced results called introducing qualitative variable supplementary element procedure detailed husson l pag pag software offer option automatic way case spad historically following work ludovic lebart first propose option r package factominer quantitative finance principal component analysis directly applied risk management interest rate derivative portfolios trading multiple swap instruments usually function market quotable swap instruments sought reduced usually principal components representing path interest rates macro basis converting risks represented factor loadings provides assessments understanding beyond available simply collectively viewing risks individual buckets pca also applied equity portfolios similar fashion portfolio risk risk return one application reduce portfolio risk allocation strategies applied principal portfolios instead underlying stocks second enhance portfolio return using principal components select stocks upside potential citation needed variant principal components analysis used neuroscience identify specific properties stimulus increase neuron probability generating action potential technique known spike triggered covariance analysis typical application experimenter presents white noise process stimulus records train action potentials spikes produced neuron result presumably certain features stimulus make neuron likely spike order extract features experimenter calculates covariance matrix spike triggered ensemble set stimuli immediately preceded spike eigenvectors difference spike triggered covariance matrix covariance matrix prior stimulus ensemble indicate directions space stimuli along variance spike triggered ensemble differed prior stimulus ensemble specifically eigenvectors largest positive eigenvalues correspond directions along variance spike triggered ensemble showed largest positive change compared variance prior since directions varying stimulus led spike often good approximations sought relevant stimulus features neuroscience pca also used discern identity neuron shape action potential spike sorting important procedure extracellular recording techniques often pick signals one neuron spike sorting one first uses pca reduce dimensionality space action potential waveforms performs clustering analysis associate specific action potentials individual neurons pca dimension reduction technique particularly suited detect coordinated activities large neuronal ensembles used determining collective variables order parameters phase transitions brain correspondence analysis developed jean paul benz cri conceptually similar pca scales data rows columns treated equivalently traditionally applied contingency tables ca decomposes chi squared statistic associated table orthogonal factors ca descriptive technique applied tables chi squared statistic appropriate several variants ca available including detrended correspondence analysis canonical correspondence analysis one special extension multiple correspondence analysis may seen counterpart principal component analysis categorical data principal component analysis creates variables linear combinations original variables new variables property variables orthogonal pca transformation helpful pre processing step clustering pca variance focused approach seeking reproduce total variable variance components reflect common unique variance variable pca generally preferred purposes data reduction goal detect latent construct factors factor analysis similar principal component analysis factor analysis also involves linear combinations variables different pca factor analysis correlation focused approach seeking reproduce inter correlations among variables factors represent common variance variables excluding unique variance terms correlation matrix corresponds focusing explaining diagonal terms pca focuses explaining terms sit diagonal however side result trying reproduce diagonal terms pca also tends fit relatively well diagonal correlations results given pca factor analysis similar situations always case problems results significantly different factor analysis generally used research purpose detecting data structure causal modeling factor model incorrectly formulated assumptions met factor analysis give erroneous results asserted relaxed solution k means clustering specified cluster indicators given principal components pca subspace spanned principal directions identical cluster centroid subspace however pca useful relaxation k means clustering new result straightforward uncover counterexamples statement cluster centroid subspace spanned principal directions non negative matrix factorization dimension reduction method non negative elements matrices used therefore promising method astronomy sense astrophysical signals non negative pca components orthogonal nmf components non negative therefore constructs non orthogonal basis pca contribution component ranked based magnitude corresponding eigenvalue equivalent fractional residual variance analyzing empirical data nmf components ranked based empirical frv curves residual fractional eigenvalue plots k j n j displaystyle sum k lambda sum j n lambda j function component number k displaystyle k given total n displaystyle n components pca flat plateau data captured remove quasi static noise curves dropped quickly indication fitting captures random noise frv curves nmf decreasing continuously nmf components constructed sequentially indicating continuous capturing quasi static noise converge higher levels pca indicating less fitting property nmf particular disadvantage pca principal components usually linear combinations input variables sparse pca overcomes disadvantage finding linear combinations contain input variables extends classic method principal component analysis reduction dimensionality data adding sparsity constraint input variables several approaches proposed including methodological theoretical developments sparse pca well applications scientific studies recently reviewed survey paper modern methods nonlinear dimensionality reduction find theoretical algorithmic roots pca k means pearson original idea take straight line best fit set data points principal curves manifolds give natural geometric framework pca generalization extend geometric interpretation pca explicitly constructing embedded manifold data approximation encoding using standard geometric projection onto manifold illustrated fig see also elastic map algorithm principal geodesic analysis another popular generalization kernel pca corresponds pca performed reproducing kernel hilbert space associated positive definite kernel multilinear subspace learning pca generalized multilinear pca extracts features directly tensor representations mpca solved performing pca mode tensor iteratively mpca applied face recognition gait recognition etc mpca extended uncorrelated mpca non negative mpca robust mpca n way principal component analysis may performed models tucker decomposition parafac multiple factor analysis co inertia analysis statis distatis pca finds mathematically optimal method still sensitive outliers data produce large errors something method tries avoid first place therefore common practice remove outliers computing pca however contexts outliers difficult identify example data mining algorithms like correlation clustering assignment points clusters outliers known beforehand recently proposed generalization pca based weighted pca increases robustness assigning different weights data objects based estimated relevancy outlier resistant variants pca also proposed based l norm formulations robust principal component analysis via decomposition low rank sparse matrices modification pca works well respect grossly corrupted observations independent component analysis directed similar problems principal component analysis finds additively separable components rather successive approximations given matrix e displaystyle e tries decompose two matrices e p displaystyle e ap key difference techniques pca ica entries displaystyle constrained p displaystyle p termed regulatory layer general decomposition multiple solutions prove following conditions satisfied decomposition unique multiplication scalar ouyang hua bi iterative least square method subspace tracking ieee transactions signal processing pp vol august hua chen convergence nic algorithm subspace computation ieee transactions signal processing pp vol april hua asymptotical orthonormalization subspace matrices without square root ieee signal processing magazine vol pp july hua nikpour p stoica optimal reduced rank estimation filtering ieee transactions signal processing pp vol march hua xiang chen k abed meraim miao new look power method fast subspace tracking digital signal processing vol pp hua w liu generalized karhunen loeve transform ieee signal processing letters vol pp june miao hua fast subspace tracking neural network learning novel information criterion ieee transactions signal processing vol pp july chen hua w yan global convergence oja subspace algorithm principal component extraction ieee transactions neural networks vol pp jan
https://en.wikipedia.org/wiki/Proper_generalized_decomposition,proper generalized decomposition iterative numerical method solving boundary value problems partial differential equations constrained set boundary conditions pgd algorithm computes approximation solution bvp successive enrichment means iteration new component computed added approximation modes obtained closer approximation theoretical solution selecting first pgd modes reduced order model solution obtained pgd considered dimensionality reduction algorithm addition considered generalized form proper orthogonal decomposition proper generalized decomposition method characterized variational formulation problem discretization domain style finite element method assumption solution approximated separated representation numerical greedy algorithm find solution implemented variational formulation pgd bubnov galerkin method although implementations exist discretization domain well defined set procedures cover creation finite element meshes definition basis function reference elements mapping reference elements onto elements mesh pgd assumes solution u problem approximated separated representation form number addends n functional products x x xd depending variable unknown beforehand solution sought applying greedy algorithm usually fixed point algorithm weak formulation problem iteration algorithm mode solution computed mode consists set numerical values functional products x xd enrich approximation solution note due greedy nature algorithm term enrich used rather improve number computed modes required obtain approximation solution certain error threshold depends stop criterium iterative algorithm unlike pod pgd modes necessarily orthogonal pgd suitable solving high dimensional problems since overcomes limitations classical approaches particular pgd avoids curse dimensionality solving decoupled problems computationally much less expensive solving multidimensional problems therefore pgd enables adapt parametric problems multidimensional framework setting parameters problem extra coordinates series functional products k k kp depending parameter incorporated equation case obtained approximation solution called computational vademecum general meta model containing particular solutions every possible value involved parameters sparse subspace learning method leverages use hierarchical collocation approximate numerical solution parametric models respect traditional projection based reduced order modeling use collocation enables non intrusive approach based sparse adaptive sampling parametric space allows recover lowdimensional structure parametric solution subspace also learning functional dependency parameters explicit form sparse low rank approximate tensor representation parametric solution built incremental strategy needs access output deterministic solver non intrusiveness makes approach straightforwardly applicable challenging problems characterized nonlinearity non affine weak forms
https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding,distributed stochastic neighbor embedding machine learning algorithm visualization based stochastic neighbor embedding originally developed sam roweis geoffrey hinton laurens van der maaten proposed distributed variant nonlinear dimensionality reduction technique well suited embedding high dimensional data visualization low dimensional space two three dimensions specifically models high dimensional object two three dimensional point way similar objects modeled nearby points dissimilar objects modeled distant points high probability sne algorithm comprises two main stages first sne constructs probability distribution pairs high dimensional objects way similar objects assigned higher probability dissimilar points assigned lower probability second sne defines similar probability distribution points low dimensional map minimizes kullback leibler divergence two distributions respect locations points map original algorithm uses euclidean distance objects base similarity metric changed appropriate sne used visualization wide range applications including computer security research music analysis cancer research bioinformatics biomedical signal processing often used visualize high level representations learned artificial neural network sne plots often seem display clusters visual clusters influenced strongly chosen parameterization therefore good understanding parameters sne necessary clusters shown even appear non clustered data thus may false findings interactive exploration may thus necessary choose parameters validate results demonstrated sne often able recover well separated clusters special parameter choices approximates simple form spectral clustering given set n displaystyle n high dimensional objects x x n displaystyle mathbf x dots mathbf x n sne first computes probabilities p j displaystyle p ij proportional similarity objects x displaystyle mathbf x x j displaystyle mathbf x j follows j displaystyle neq j define set p displaystyle p mid note j p j displaystyle sum j p j mid displaystyle van der maaten hinton explained similarity datapoint x j displaystyle x j datapoint x displaystyle x conditional probability p j displaystyle p j x displaystyle x would pick x j displaystyle x j neighbor neighbors picked proportion probability density gaussian centered x displaystyle x define note p j p j displaystyle p ij p ji p displaystyle p ii j p j displaystyle sum j p ij bandwidth gaussian kernels displaystyle sigma set way perplexity conditional distribution equals predefined perplexity using bisection method result bandwidth adapted density data smaller values displaystyle sigma used denser parts data space since gaussian kernel uses euclidean distance x x j displaystyle lvert x x j rvert affected curse dimensionality high dimensional data distances lose ability discriminate p j displaystyle p ij become similar proposed adjust distances power transform based intrinsic dimension point alleviate sne aims learn displaystyle dimensional map n displaystyle mathbf dots mathbf n reflects similarities p j displaystyle p ij well possible end measures similarities q j displaystyle q ij two points map displaystyle mathbf j displaystyle mathbf j using similar approach specifically j displaystyle neq j define q j displaystyle q ij set q displaystyle q ii herein heavy tailed student distribution used measure similarities low dimensional points order allow dissimilar objects modeled far apart map locations points displaystyle mathbf map determined minimizing kullback leibler divergence distribution p displaystyle p distribution q displaystyle q minimization kullback leibler divergence respect points displaystyle mathbf performed using gradient descent result optimization map reflects similarities high dimensional inputs
https://en.wikipedia.org/wiki/Structured_prediction,structured prediction structured learning umbrella term supervised machine learning techniques involves predicting structured objects rather scalar discrete real values similar commonly used supervised learning techniques structured prediction models typically trained means observed data true prediction value used adjust model parameters due complexity model interrelations predicted variables process prediction using trained model training often computationally infeasible approximate inference learning methods used example problem translating natural language sentence syntactic representation parse tree seen structured prediction problem structured output domain set possible parse trees structured prediction also used wide variety application domains including bioinformatics natural language processing speech recognition computer vision sequence tagging class problems prevalent natural language processing input data often sequences sequence tagging problem appears several guises e g part speech tagging named entity recognition pos tagging example word sequence must receive tag expresses type word main challenge problem resolve ambiguity word sentence also verb english tagged problem solved simply performing classification individual tokens approach take account empirical fact tags occur independently instead tag displays strong conditional dependence tag previous word fact exploited sequence model hidden markov model conditional random field predicts entire tag sequence sentence rather individual tags means viterbi algorithm probabilistic graphical models form large class structured prediction models particular bayesian networks random fields popular algorithms models structured prediction include inductive logic programming case based reasoning structured svms markov logic networks constrained conditional models main techniques one easiest ways understand algorithms general structured prediction structured perceptron collins algorithm combines perceptron algorithm learning linear classifiers inference algorithm described abstractly follows first define joint feature function maps training sample x candidate prediction vector length n let gen function generates candidate predictions practice finding argmax g e n displaystyle gen done using algorithm viterbi algorithm max sum rather exhaustive search exponentially large set candidates idea learning similar multiclass perceptron
https://en.wikipedia.org/wiki/Graphical_model,graphical model probabilistic graphical model structured probabilistic model probabilistic model graph expresses conditional dependence structure random variables commonly used probability theory statistics particularly bayesian statistics machine learning generally probabilistic graphical models use graph based representation foundation encoding distribution multi dimensional space graph compact factorized representation set independences hold specific distribution two branches graphical representations distributions commonly used namely bayesian networks markov random fields families encompass properties factorization independences differ set independences encode factorization distribution induce network structure model directed acyclic graph model represents factorization joint probability random variables precisely events x x n displaystyle x ldots x n joint probability satisfies pa displaystyle text pa set parents node x displaystyle x words joint distribution factors product conditional distributions example graphical model figure shown consists random variables b c displaystyle b c joint probability density factors two nodes conditionally independent given values parents general two sets nodes conditionally independent given third set criterion called separation holds graph local independences global independences equivalent bayesian networks type graphical model known directed graphical model bayesian network belief network classic machine learning models like hidden markov models neural networks newer models variable order markov models considered special cases bayesian networks framework models provides algorithms discovering analyzing structure complex distributions describe succinctly extract unstructured information allows constructed utilized effectively applications graphical models include causal inference information extraction speech recognition computer vision decoding low density parity check codes modeling gene regulatory networks gene finding diagnosis diseases graphical models protein structure
https://en.wikipedia.org/wiki/Bayesian_network,bayesian network probabilistic graphical model represents set variables conditional dependencies via directed acyclic graph bayesian networks ideal taking event occurred predicting likelihood one several possible known causes contributing factor example bayesian network could represent probabilistic relationships diseases symptoms given symptoms network used compute probabilities presence various diseases efficient algorithms perform inference learning bayesian networks bayesian networks model sequences variables called dynamic bayesian networks generalizations bayesian networks represent solve decision problems uncertainty called influence diagrams formally bayesian networks directed acyclic graphs whose nodes represent variables bayesian sense may observable quantities latent variables unknown parameters hypotheses edges represent conditional dependencies nodes connected represent variables conditionally independent node associated probability function takes input particular set values node parent variables gives probability variable represented node example displaystyle parent nodes represent displaystyle boolean variables probability function could represented table displaystyle entries one entry displaystyle possible parent combinations similar ideas may applied undirected possibly cyclic graphs markov networks two events cause grass wet active sprinkler rain rain direct effect use sprinkler situation modeled bayesian network variable two possible values f joint probability function g grass wet sprinkler turned r raining model answer questions presence cause given presence effect like probability raining given grass wet using conditional probability formula summing nuisance variables using expansion joint probability function pr displaystyle pr conditional probabilities conditional probability tables stated diagram one evaluate term sums numerator denominator example numerical results answer interventional question probability would rain given wet grass answer governed post intervention joint distribution function obtained removing factor pr displaystyle pr pre intervention distribution operator forces value g true probability rain unaffected action predict impact turning sprinkler term pr displaystyle pr removed showing action affects grass rain predictions may feasible given unobserved variables policy evaluation problems effect action displaystyle text still predicted however whenever back door criterion satisfied states set z nodes observed separates back door paths x back door path one ends arrow x sets satisfy back door criterion called sufficient admissible example set z r admissible predicting effect g r separates back door path r g however observed set separates path effect turning sprinkler grass cannot predicted passive observations case p identified reflects fact lacking interventional data observed dependence g due causal connection spurious determine whether causal relation identified arbitrary bayesian network unobserved variables one use three rules calculus test whether terms removed expression relation thus confirming desired quantity estimable frequency data using bayesian network save considerable amounts memory exhaustive probability tables dependencies joint distribution sparse example naive way storing conditional probabilities two valued variables table requires storage space displaystyle values variable local distribution depends three parent variables bayesian network representation stores displaystyle cdot values one advantage bayesian networks intuitively easier human understand direct dependencies local distributions complete joint distributions bayesian networks perform three main inference tasks bayesian network complete model variables relationships used answer probabilistic queries example network used update knowledge state subset variables variables observed process computing posterior distribution variables given evidence called probabilistic inference posterior gives universal sufficient statistic detection applications choosing values variable subset minimize expected loss function instance probability decision error bayesian network thus considered mechanism automatically applying bayes theorem complex problems common exact inference methods variable elimination eliminates non observed non query variables one one distributing sum product clique tree propagation caches computation many variables queried one time new evidence propagated quickly recursive conditioning search allow space time tradeoff match efficiency variable elimination enough space used methods complexity exponential network treewidth common approximate inference algorithms importance sampling stochastic mcmc simulation mini bucket elimination loopy belief propagation generalized belief propagation variational methods order fully specify bayesian network thus fully represent joint probability distribution necessary specify node x probability distribution x conditional upon x parents distribution x conditional upon parents may form common work discrete gaussian distributions since simplifies calculations sometimes constraints distribution known one use principle maximum entropy determine single distribution one greatest entropy given constraints often conditional distributions include parameters unknown must estimated data e g via maximum likelihood approach direct maximization likelihood often complex given unobserved variables classical approach problem expectation maximization algorithm alternates computing expected values unobserved variables conditional observed data maximizing complete likelihood assuming previously computed expected values correct mild regularity conditions process converges maximum likelihood values parameters fully bayesian approach parameters treat additional unobserved variables compute full posterior distribution nodes conditional upon observed data integrate parameters approach expensive lead large dimension models making classical parameter setting approaches tractable simplest case bayesian network specified expert used perform inference applications task defining network complex humans case network structure parameters local distributions must learned data automatically learning graph structure bayesian network challenge pursued within machine learning basic idea goes back recovery algorithm developed rebane pearl rests distinction three possible patterns allowed node dag first represent dependencies therefore indistinguishable collider however uniquely identified since x displaystyle x z displaystyle z marginally independent pairs dependent thus skeletons three triplets identical directionality arrows partially identifiable distinction applies x displaystyle x z displaystyle z common parents except one must first condition parents algorithms developed systematically determine skeleton underlying graph orient arrows whose directionality dictated conditional independences observed alternative method structural learning uses optimization based search requires scoring function search strategy common scoring function posterior probability structure given training data like bic bdeu time requirement exhaustive search returning structure maximizes score superexponential number variables local search strategy makes incremental changes aimed improving score structure global search algorithm like markov chain monte carlo avoid getting trapped local minima friedman et al discuss using mutual information variables finding structure maximizes restricting parent candidate set k nodes exhaustively searching therein particularly fast method exact bn learning cast problem optimization problem solve using integer programming acyclicity constraints added integer program solving form cutting planes method handle problems variables order deal problems thousands variables different approach necessary one first sample one ordering find optimal bn structure respect ordering implies working search space possible orderings convenient smaller space network structures multiple orderings sampled evaluated method proven best available literature number variables huge another method consists focusing sub class decomposable models mle closed form possible discover consistent structure hundreds variables learning bayesian networks bounded treewidth necessary allow exact tractable inference since worst case inference complexity exponential treewidth k yet global property graph considerably increases difficulty learning process context possible use k tree effective learning given data x displaystyle x parameter displaystyle theta simple bayesian analysis starts prior probability p displaystyle p likelihood p displaystyle p compute posterior probability p p p displaystyle p propto pp often prior displaystyle theta depends turn parameters displaystyle varphi mentioned likelihood prior p displaystyle p must replaced likelihood p displaystyle p prior p displaystyle p newly introduced parameters displaystyle varphi required resulting posterior probability simplest example hierarchical bayes model clarification needed process may repeated example parameters displaystyle varphi may depend turn additional parameters displaystyle psi require prior eventually process must terminate priors depend unmentioned parameters given measured quantities x x n displaystyle x dots x n normally distributed errors known standard deviation displaystyle sigma suppose interested estimating displaystyle theta approach would estimate displaystyle theta using maximum likelihood approach since observations independent likelihood factorizes maximum likelihood estimate simply however quantities related example individual displaystyle theta drawn underlying distribution relationship destroys independence suggests complex model e g improper priors flat displaystyle varphi sim text flat flat displaystyle tau sim text flat n displaystyle n geq identified model posterior distributions individual displaystyle theta tend move shrink away maximum likelihood estimates towards common mean shrinkage typical behavior hierarchical bayes models care needed choosing priors hierarchical model particularly scale variables higher levels hierarchy variable displaystyle tau example usual priors jeffreys prior often work posterior distribution normalizable estimates made minimizing expected loss inadmissible several equivalent definitions bayesian network offered following let g directed acyclic graph let x v v set random variables indexed v x bayesian network respect g joint probability density function written product individual density functions conditional parent variables pa set parents v set random variables probability member joint distribution calculated conditional probabilities using chain rule follows using definition written difference two expressions conditional independence variables non descendants given values parent variables x bayesian network respect g satisfies local markov property variable conditionally independent non descendants given parent variables de set descendants v de set non descendants v expressed terms similar first definition set parents subset set non descendants graph acyclic developing bayesian network often begins creating dag g x satisfies local markov property respect g sometimes causal dag conditional probability distributions variable given parents g assessed many cases particular case variables discrete joint distribution x product conditional distributions x bayesian network respect g markov blanket node set nodes consisting parents children parents children markov blanket renders node independent rest network joint distribution variables markov blanket node sufficient knowledge calculating distribution node x bayesian network respect g every node conditionally independent nodes network given markov blanket definition made general defining separation two nodes stands directional first define separation trail define separation two nodes terms let p trail node u v trail loop free undirected path two nodes p said separated set nodes z following conditions holds nodes u v separated z trails separated u v separated connected x bayesian network respect g two nodes u v z set separates u v although bayesian networks often used represent causal relationships need case directed edge u v require xv causally dependent xu demonstrated fact bayesian networks graphs equivalent impose exactly conditional independence requirements causal network bayesian network requirement relationships causal additional semantics causal networks specify node x actively caused given state x probability density function changes network obtained cutting links parents x x setting x caused value x using semantics impact external interventions data obtained prior intervention predicted working stanford university large bioinformatic applications cooper proved exact inference bayesian networks np hard result prompted research approximation algorithms aim developing tractable approximation probabilistic inference dagum luby proved two surprising results complexity approximation probabilistic inference bayesian networks first proved tractable deterministic algorithm approximate probabilistic inference within absolute error even bayesian networks restricted architecture np hard practical terms complexity results suggested bayesian networks rich representations ai machine learning applications use large real world applications would need tempered either topological structural constraints na bayes networks restrictions conditional probabilities bounded variance algorithm first provable fast approximation algorithm efficiently approximate probabilistic inference bayesian networks guarantees error approximation powerful algorithm required minor restriction conditional probabilities bayesian network bounded away zero one p p polynomial number nodes network n notable software bayesian networks include term bayesian network coined judea pearl emphasize late pearl probabilistic reasoning intelligent systems neapolitan probabilistic reasoning expert systems summarized properties established field study
https://en.wikipedia.org/wiki/Conditional_random_field,conditional random fields class statistical modeling method often applied pattern recognition machine learning used structured prediction whereas classifier predicts label single sample without considering neighboring samples crf take context account prediction modeled graphical model implements dependencies predictions kind graph used depends application example natural language processing linear chain crfs popular implement sequential dependencies predictions image processing graph typically connects locations nearby similar locations enforce receive similar predictions examples crfs used labeling parsing sequential data natural language processing biological sequences pos tagging shallow parsing named entity recognition gene finding peptide critical functional region finding object recognition image segmentation computer vision crfs type discriminative undirected probabilistic graphical model lafferty mccallum pereira define crf observations x displaystyle boldsymbol x random variables displaystyle boldsymbol follows let g displaystyle g graph v v displaystyle boldsymbol v v displaystyle boldsymbol indexed vertices g displaystyle g displaystyle conditional random field random variables v displaystyle boldsymbol v conditioned x displaystyle boldsymbol x obey markov property respect graph p p displaystyle p p w v displaystyle mathit w sim v means w displaystyle w v displaystyle v neighbors g displaystyle g means crf undirected graphical model whose nodes divided exactly two disjoint sets x displaystyle boldsymbol x displaystyle boldsymbol observed output variables respectively conditional distribution p displaystyle p modeled general graphs problem exact inference crfs intractable inference problem crf basically mrf arguments hold however exist special cases exact inference feasible exact inference impossible several algorithms used obtain approximate solutions include learning parameters displaystyle theta usually done maximum likelihood learning p displaystyle p nodes exponential family distributions nodes observed training optimization convex solved example using gradient descent algorithms quasi newton methods l bfgs algorithm hand variables unobserved inference problem solved variables exact inference intractable general graphs approximations used sequence modeling graph interest usually chain graph input sequence observed variables x displaystyle x represents sequence observations displaystyle represents hidden state variable needs inferred given observations displaystyle structured form chain edge displaystyle displaystyle well simple interpretation displaystyle labels element input sequence layout admits efficient algorithms conditional dependency displaystyle x displaystyle x defined fixed set feature functions form f displaystyle f thought measurements input sequence partially determine likelihood possible value displaystyle model assigns feature numerical weight combines determine probability certain value displaystyle linear chain crfs many applications conceptually simpler hidden markov models relax certain assumptions input output sequence distributions hmm loosely understood crf specific feature functions use constant probabilities model state transitions emissions conversely crf loosely understood generalization hmm makes constant transition probabilities arbitrary functions vary across positions sequence hidden states depending input sequence notably contrast hmms crfs contain number feature functions feature functions inspect entire input sequence x displaystyle x point inference range feature functions need probabilistic interpretation crfs extended higher order models making displaystyle dependent fixed number k displaystyle k previous variables k displaystyle k conventional formulations higher order crfs training inference practical small values k displaystyle k since computational cost increases exponentially k displaystyle k however another recent advance managed ameliorate issues leveraging concepts tools field bayesian nonparametrics specifically crf infinity approach constitutes crf type model capable learning infinitely long temporal dynamics scalable fashion effected introducing novel potential function crfs based sequence memoizer nonparametric bayesian model learning infinitely long dynamics sequential observations render model computationally tractable crf infinity employs mean field approximation postulated novel potential functions allows devising efficient approximate training inference algorithms model without undermining capability capture model temporal dependencies arbitrary length exists another generalization crfs semi markov conditional random field models variable length segmentations label sequence displaystyle provides much power higher order crfs model long range dependencies displaystyle reasonable computational cost finally large margin models structured prediction structured support vector machine seen alternative training procedure crfs latent dynamic conditional random fields discriminative probabilistic latent variable models type crfs sequence tagging tasks latent variable models trained discriminatively ldcrf like sequence tagging task given sequence observations x x x n displaystyle x dots x n main problem model must solve assign sequence labels n displaystyle dots n one finite set labels instead directly modeling p ordinary linear chain crf would set latent variables h inserted x using chain rule probability allows capturing latent structure observations labels ldcrfs trained using quasi newton methods specialized version perceptron algorithm called latent variable perceptron developed well based collins structured perceptron algorithm models find applications computer vision specifically gesture recognition video streams shallow parsing partial list software implement generic crf tools partial list software implement crf related tools
https://en.wikipedia.org/wiki/Hidden_Markov_model,hidden markov model statistical markov model system modeled assumed markov process call x displaystyle x unobservable states hmm assumes another process displaystyle whose behavior depends x displaystyle x goal learn x displaystyle x observing displaystyle hmm stipulates time instance n displaystyle n conditional probability distribution n displaystyle n given history x n x n n n displaystyle x n x n n leq n must depend x n n n displaystyle x n n
https://en.wikipedia.org/wiki/Anomaly_detection,various domains limited statistics signal processing finance econometrics manufacturing networking data mining anomaly detection identification rare items events observations raise suspicions differing significantly majority data typically anomalous items translate kind problem bank fraud structural defect medical problems errors text anomalies also referred outliers novelties noise deviations exceptions particular context abuse network intrusion detection interesting objects often rare objects unexpected bursts activity pattern adhere common statistical definition outlier rare object many outlier detection methods fail data unless aggregated appropriately instead cluster analysis algorithm may able detect micro clusters formed patterns three broad categories anomaly detection techniques exist unsupervised anomaly detection techniques detect anomalies unlabeled test data set assumption majority instances data set normal looking instances seem fit least remainder data set supervised anomaly detection techniques require data set labeled normal abnormal involves training classifier semi supervised anomaly detection techniques construct model representing normal behavior given normal training data set test likelihood test instance generated learnt model anomaly detection applicable variety domains intrusion detection fraud detection fault detection system health monitoring event detection sensor networks detecting ecosystem disturbances often used preprocessing remove anomalous data dataset supervised learning removing anomalous data dataset often results statistically significant increase accuracy several anomaly detection techniques proposed literature popular techniques performance different methods depends lot data set parameters methods little systematic advantages another compared across many data sets parameters anomaly detection proposed intrusion detection systems dorothy denning anomaly detection ids normally accomplished thresholds statistics also done soft computing inductive learning types statistics proposed included profiles users workstations networks remote hosts groups users programs based frequencies means variances covariances standard deviations counterpart anomaly detection intrusion detection misuse detection
https://en.wikipedia.org/wiki/K-nearest_neighbors_classification,pattern recognition k nearest neighbors algorithm non parametric method proposed thomas cover used classification regression cases input consists k closest training examples feature space output depends whether k nn used classification regression k nn type instance based learning lazy learning function approximated locally computation deferred function evaluation since algorithm relies distance classification normalizing training data improve accuracy dramatically classification regression useful technique assign weights contributions neighbors nearer neighbors contribute average distant ones example common weighting scheme consists giving neighbor weight distance neighbor neighbors taken set objects class object property value known thought training set algorithm though explicit training step required peculiarity k nn algorithm sensitive local structure data suppose pairs displaystyle dots taking values r displaystyle mathbb r times class label x x r p r displaystyle x r sim p r r displaystyle r given norm displaystyle cdot r displaystyle mathbb r point x r displaystyle x mathbb r let displaystyle dots reordering training data x x x x displaystyle x x leq dots leq x x training examples vectors multidimensional feature space class label training phase algorithm consists storing feature vectors class labels training samples classification phase k user defined constant unlabeled vector classified assigning label frequent among k training samples nearest query point commonly used distance metric continuous variables euclidean distance discrete variables text classification another metric used overlap metric context gene expression microarray data example k nn employed correlation coefficients pearson spearman metric often classification accuracy k nn improved significantly distance metric learned specialized algorithms large margin nearest neighbor neighbourhood components analysis drawback basic majority voting classification occurs class distribution skewed examples frequent class tend dominate prediction new example tend common among k nearest neighbors due large number one way overcome problem weight classification taking account distance test point k nearest neighbors class k nearest points multiplied weight proportional inverse distance point test point another way overcome skew abstraction data representation example self organizing map node representative cluster similar points regardless density original training data k nn applied som best choice k depends upon data generally larger values k reduces effect noise classification make boundaries classes less distinct good k selected various heuristic techniques special case class predicted class closest training sample called nearest neighbor algorithm accuracy k nn algorithm severely degraded presence noisy irrelevant features feature scales consistent importance much research effort put selecting scaling features improve classification particularly popular citation needed approach use evolutionary algorithms optimize feature scaling another popular approach scale features mutual information training data training classes citation needed binary classification problems helpful choose k odd number avoids tied votes one popular way choosing empirically optimal k setting via bootstrap method intuitive nearest neighbour type classifier one nearest neighbour classifier assigns point x class closest neighbour feature space c n n n displaystyle c n nn size training data set approaches infinity one nearest neighbour classifier guarantees error rate worse twice bayes error rate k nearest neighbour classifier viewed assigning k nearest neighbours weight k displaystyle k others weight generalised weighted nearest neighbour classifiers ith nearest neighbour assigned weight w n displaystyle w ni n w n displaystyle sum n w ni analogous result strong consistency weighted nearest neighbour classifiers also holds let c n w n n displaystyle c n wnn denote weighted nearest classifier weights w n n displaystyle w ni n subject regularity conditions explanation needed class distributions excess risk following asymptotic expansion constants b displaystyle b b displaystyle b n n w n displaystyle n sum n w ni n n n w n displaystyle n n sum n w ni optimal weighting scheme w n n displaystyle w ni n balances two terms display given follows set k b n displaystyle k lfloor bn frac rfloor optimal weights dominant term asymptotic expansion excess risk displaystyle mathcal similar results true using bagged nearest neighbour classifier k nn special case variable bandwidth kernel density balloon estimator uniform kernel naive version algorithm easy implement computing distances test example stored examples computationally intensive large training sets using approximate nearest neighbor search algorithm makes k nn computationally tractable even large data sets many nearest neighbor search algorithms proposed years generally seek reduce number distance evaluations actually performed k nn strong consistency results amount data approaches infinity two class k nn algorithm guaranteed yield error rate worse twice bayes error rate various improvements k nn speed possible using proximity graphs multi class k nn classification cover hart prove upper bound error rate r displaystyle r bayes error rate r k n n displaystyle r knn k nn error rate number classes problem displaystyle bayesian error rate r displaystyle r approaches zero limit reduces twice bayesian error rate many results error rate k nearest neighbour classifiers k nearest neighbour classifier strongly displaystyle consistent provided k k n displaystyle k k n diverges k n n displaystyle k n n converges zero n displaystyle n infty let c n k n n displaystyle c n knn denote k nearest neighbour classifier based training set size n certain regularity conditions excess risk yields following asymptotic expansion constants b displaystyle b b displaystyle b choice k b n displaystyle k lfloor bn frac rfloor offers trade two terms display k displaystyle k nearest neighbour error converges bayes error optimal rate displaystyle mathcal k nearest neighbor classification performance often significantly improved metric learning popular algorithms neighbourhood components analysis large margin nearest neighbor supervised metric learning algorithms use label information learn new metric pseudo metric input data algorithm large processed suspected redundant input data transformed reduced representation set features transforming input data set features called feature extraction features extracted carefully chosen expected features set extract relevant information input data order perform desired task using reduced representation instead full size input feature extraction performed raw data prior applying k nn algorithm transformed data feature space example typical computer vision computation pipeline face recognition using k nn including feature extraction dimension reduction pre processing steps high dimensional data dimension reduction usually performed prior applying k nn algorithm order avoid effects curse dimensionality curse dimensionality k nn context basically means euclidean distance unhelpful high dimensions vectors almost equidistant search query vector feature extraction dimension reduction combined one step using principal component analysis linear discriminant analysis canonical correlation analysis techniques pre processing step followed clustering k nn feature vectors reduced dimension space machine learning process also called low dimensional embedding high dimensional datasets running fast approximate k nn search using locality sensitive hashing random projections sketches high dimensional similarity search techniques vldb toolbox might feasible option nearest neighbor rules effect implicitly compute decision boundary also possible compute decision boundary explicitly efficiently computational complexity function boundary complexity data reduction one important problems work huge data sets usually data points needed accurate classification data called prototypes found follows training example surrounded examples classes called class outlier causes class outliers include class outliers k nn produce noise detected separated future analysis given two natural numbers k r training example called nn class outlier k nearest neighbors include r examples classes condensed nearest neighbor algorithm designed reduce data set k nn classification selects set prototypes u training data nn u classify examples almost accurately nn whole data set given training set x cnn works iteratively use u instead x classification examples prototypes called absorbed points efficient scan training examples order decreasing border ratio border ratio training example x defined x distance closest example different color x x distance closest example x label x border ratio interval x never exceeds x ordering gives preference borders classes inclusion set prototypes u point different label x called external x calculation border ratio illustrated figure right data points labeled colors initial point x label red external points blue green closest x external point closest red point x border ratio x x attribute initial point x illustration cnn series figures three classes fig initially points class fig shows nn classification map pixel classified nn using data fig shows nn classification map white areas correspond unclassified regions nn voting tied fig shows reduced data set crosses class outliers selected nn rule squares prototypes empty circles absorbed points left bottom corner shows numbers class outliers prototypes absorbed points three classes number prototypes varies different classes example fig shows nn classification map prototypes similar initial data set figures produced using mirkes applet fig dataset fig nn classification map fig nn classification map fig cnn reduced dataset fig nn classification map based cnn extracted prototypes k nn regression k nn algorithm citation needed used estimating continuous variables one algorithm uses weighted average k nearest neighbors weighted inverse distance algorithm works follows distance kth nearest neighbor also seen local density estimate thus also popular outlier score anomaly detection larger distance k nn lower local density likely query point outlier although quite simple outlier model along another classic data mining method local outlier factor works quite well also comparison recent complex approaches according large scale experimental analysis confusion matrix matching matrix often used tool validate accuracy k nn classification robust statistical methods likelihood ratio test also applied
https://en.wikipedia.org/wiki/Local_outlier_factor,anomaly detection local outlier factor algorithm proposed markus breunig hans peter kriegel raymond ng j rg sander finding anomalous data points measuring local deviation given data point respect neighbours lof shares concepts dbscan optics concepts core distance reachability distance used local density estimation local outlier factor based concept local density locality given k nearest neighbors whose distance used estimate density comparing local density object local densities neighbors one identify regions similar density points substantially lower density neighbors considered outliers local density estimated typical distance point reached neighbors definition reachability distance used lof additional measure produce stable results within clusters reachability distance used lof subtle details often found incorrect secondary sources e g textbook ethem alpaydin let k distance distance object k th nearest neighbor note set k nearest neighbors includes objects distance case tie k objects denote set k nearest neighbors nk distance used define called reachability distance reachability distancek max k distance words reachability distance object b true distance two objects least k distance b objects belong k nearest neighbors b considered equally distant reason distance get stable results citation needed note distance mathematical definition since symmetric yields slightly different method referred simplified lof local reachability density object defined lrdk reachability distancek nk inverse average reachability distance object neighbors note average reachability neighbors distance reached neighbors duplicate points value become infinite local reachability densities compared neighbors using lofk b nklrdk lrdk nk b nklrdk nk lrdk average local reachability density neighbors divided object local reachability density value approximately indicates object comparable neighbors value indicates denser region values significantly larger indicate outliers lof means similar density neighbors lof means higher density neighbors lof means lower density neighbors due local approach lof able identify outliers data set would outliers another area data set example point small distance dense cluster outlier point within sparse cluster might exhibit similar distances neighbors geometric intuition lof applicable low dimensional vector spaces algorithm applied context dissimilarity function defined experimentally shown work well numerous setups often outperforming competitors example network intrusion detection processed classification benchmark data lof family methods easily generalized applied various problems detecting outliers geographic data video streams authorship networks resulting values quotient values hard interpret value even less indicates clear inlier clear rule point outlier one data set value may already outlier another dataset parameterization value could still inlier differences also occur within dataset due locality method exist extensions lof try improve lof aspects
https://en.wikipedia.org/wiki/Artificial_neural_network,collective intelligence collective action self organized criticality herd mentality phase transition agent based modelling synchronization ant colony optimization particle swarm optimization swarm behaviour social network analysis small world networks centrality motifs graph theory scaling robustness systems biology dynamic networks evolutionary computation genetic algorithms genetic programming artificial life machine learning evolutionary developmental biology artificial intelligence evolutionary robotics reaction diffusion systems partial differential equations dissipative structures percolation cellular automata spatial ecology self replication operationalization feedback self reference goal oriented system dynamics sensemaking entropy cybernetics autopoiesis information theory ordinary differential equations phase space attractors population dynamics chaos multistability bifurcation rational choice theory bounded rationality artificial neural networks usually simply called neural networks computing systems vaguely inspired biological neural networks constitute animal brains ann based collection connected units nodes called artificial neurons loosely model neurons biological brain connection like synapses biological brain transmit signal neurons artificial neuron receives signal processes signal neurons connected signal connection real number output neuron computed non linear function sum inputs connections called edges neurons edges typically weight adjusts learning proceeds weight increases decreases strength signal connection neurons may threshold signal sent aggregate signal crosses threshold typically neurons aggregated layers different layers may perform different transformations inputs signals travel first layer last layer possibly traversing layers multiple times mw parser output toclimit toclevel ul mw parser output toclimit toclevel ul mw parser output toclimit toclevel ul mw parser output toclimit toclevel ul mw parser output toclimit toclevel ul mw parser output toclimit toclevel ul display none neural networks learn processing examples contains known input result forming probability weighted associations two stored within data structure net training neural network given example usually conducted determining difference processed output network target output error network adjusts weighted associations according learning rule using error value successive adjustments cause neural network produce output increasingly similar target output sufficient number adjustments training terminated based upon certain criteria known supervised learning systems learn perform tasks considering examples generally without programmed task specific rules example image recognition might learn identify images contain cats analyzing example images manually labeled cat cat using results identify cats images without prior knowledge cats example fur tails whiskers cat like faces instead automatically generate identifying characteristics examples process warren mcculloch walter pitts opened subject creating computational model neural networks late hebb created learning hypothesis based mechanism neural plasticity became known hebbian learning farley wesley clark first used computational machines called calculators simulate hebbian network rosenblatt created perceptron first functional networks many layers published ivakhnenko lapa group method data handling basics continuous backpropagation derived context control theory kelley bryson using principles dynamic programming seppo linnainmaa published general method automatic differentiation discrete connected networks nested differentiable functions dreyfus used backpropagation adapt parameters controllers proportion error gradients werbos backpropagation algorithm enabled practical training multi layer networks applied linnainmaa ad method neural networks way became widely used thereafter research stagnated following minsky papert discovered basic perceptrons incapable processing exclusive circuit computers lacked sufficient power process useful neural networks increasing transistor count digital electronics provided processing power enabled development practical artificial neural networks max pooling introduced help least shift invariance tolerance deformation aid object recognition schmidhuber adopted multi level hierarchy networks pre trained one level time unsupervised learning fine tuned backpropagation geoffrey hinton et al proposed learning high level representation using successive layers binary real valued latent variables restricted boltzmann machine model layer ng dean created network learned recognize higher level concepts cats watching unlabeled images unsupervised pre training increased computing power gpus distributed computing allowed use larger networks particularly image visual recognition problems became known deep learning ciresan colleagues showed despite vanishing gradient problem gpus make backpropagation feasible many layered feedforward neural networks anns began winning prizes ann contests approaching human level performance various tasks initially pattern recognition machine learning example bi directional multi dimensional long short term memory graves et al three competitions connected handwriting recognition without prior knowledge three languages learned ciresan colleagues built first pattern recognizers achieve human competitive superhuman performance benchmarks traffic sign recognition anns began attempt exploit architecture human brain perform tasks conventional algorithms little success soon reoriented towards improving empirical results mostly abandoning attempts remain true biological precursors neurons connected various patterns allow output neurons become input others network forms directed weighted graph artificial neural network consists collection simulated neurons neuron node connected nodes via links correspond biological axon synapse dendrite connections link weight determines strength one node influence another anns composed artificial neurons conceptually derived biological neurons artificial neuron inputs produce single output sent multiple neurons inputs feature values sample external data images documents outputs neurons outputs final output neurons neural net accomplish task recognizing object image find output neuron first take weighted sum inputs weighted weights connections inputs neuron add bias term sum weighted sum sometimes called activation weighted sum passed activation function produce output initial inputs external data images documents ultimate outputs accomplish task recognizing object image network consists connections connection providing output one neuron input another neuron connection assigned weight represents relative importance given neuron multiple input output connections propagation function computes input neuron outputs predecessor neurons connections weighted sum bias term added result propagation neurons typically organized multiple layers especially deep learning neurons one layer connect neurons immediately preceding immediately following layers layer receives external data input layer layer produces ultimate result output layer zero hidden layers single layer unlayered networks also used two layers multiple connection patterns possible fully connected every neuron one layer connecting every neuron next layer pooling group neurons one layer connect single neuron next layer thereby reducing number neurons layer neurons connections form directed acyclic graph known feedforward networks alternatively networks allow connections neurons previous layers known recurrent networks hyperparameter constant parameter whose value set learning process begins values parameters derived via learning examples hyperparameters include learning rate number hidden layers batch size values hyperparameters dependent hyperparameters example size layers depend overall number layers learning adaptation network better handle task considering sample observations learning involves adjusting weights network improve accuracy result done minimizing observed errors learning complete examining additional observations usefully reduce error rate even learning error rate typically reach learning error rate high network typically must redesigned practically done defining cost function evaluated periodically learning long output continues decline learning continues cost frequently defined statistic whose value approximated outputs actually numbers error low difference output correct answer small learning attempts reduce total differences across observations learning models viewed straightforward application optimization theory statistical estimation learning rate defines size corrective steps model takes adjust errors observation high learning rate shortens training time lower ultimate accuracy lower learning rate takes longer potential greater accuracy optimizations quickprop primarily aimed speeding error minimization improvements mainly try increase reliability order avoid oscillation inside network alternating connection weights improve rate convergence refinements use adaptive learning rate increases decreases appropriate concept momentum allows balance gradient previous change weighted weight adjustment depends degree previous change momentum close emphasizes gradient value close emphasizes last change possible define cost function ad hoc frequently choice determined function desirable properties arises model backpropagation method adjust connection weights compensate error found learning error amount effectively divided among connections technically backprop calculates gradient cost function associated given state respect weights weight updates done via stochastic gradient descent methods extreme learning machines prop networks training without backtracking weightless networks non connectionist neural networks three major learning paradigms supervised learning unsupervised learning reinforcement learning correspond particular learning task supervised learning uses set paired inputs desired outputs learning task produce desired output input case cost function related eliminating incorrect deductions commonly used cost mean squared error tries minimize average squared error network output desired output tasks suited supervised learning pattern recognition regression supervised learning also applicable sequential data thought learning teacher form function provides continuous feedback quality solutions obtained thus far unsupervised learning input data given along cost function function data x displaystyle textstyle x network output cost function dependent task priori assumptions trivial example consider model f displaystyle textstyle f displaystyle textstyle constant cost c e displaystyle textstyle c e minimizing cost produces value displaystyle textstyle equal mean data cost function much complicated form depends application example compression could related mutual information x displaystyle textstyle x f displaystyle textstyle f whereas statistical modeling could related posterior probability model given data tasks fall within paradigm unsupervised learning general estimation problems applications include clustering estimation statistical distributions compression filtering applications playing video games actor takes string actions receiving generally unpredictable response environment one goal win game e generate positive responses reinforcement learning aim weight network perform actions minimize long term cost point time agent performs action environment generates observation instantaneous cost according rules rules long term cost usually estimated juncture agent decides whether explore new actions uncover costs exploit prior learning proceed quickly formally environment modeled markov decision process states n displaystyle textstyle n actions displaystyle textstyle state transitions known probability distributions used instead instantaneous cost distribution p displaystyle textstyle p observation distribution p displaystyle textstyle p transition distribution p displaystyle textstyle p policy defined conditional distribution actions given observations taken together two define markov chain aim discover lowest cost mc anns serve learning component applications dynamic programming coupled anns applied problems involved vehicle routing video games natural resource management medicine anns ability mitigate losses accuracy even reducing discretization grid density numerically approximating solution control problems tasks fall within paradigm reinforcement learning control problems games sequential decision making tasks self learning neural networks introduced along neural network capable self learning named crossbar adaptive array system one input situation one output action neither external advice input external reinforcement input environment caa computes crossbar fashion decisions actions emotions encountered situations system driven interaction cognition emotion given memory matrix w w crossbar self learning algorithm iteration performs following computation backpropagated value emotion toward consequence situation caa exists two environments one behavioral environment behaves genetic environment initially receives initial emotions encountered situations behavioral environment received genome vector genetic environment caa learn goal seeking behavior behavioral environment contains desirable undesirable situations bayesian framework distribution set allowed models chosen minimize cost evolutionary methods gene expression programming simulated annealing expectation maximization non parametric methods particle swarm optimization learning algorithms convergent recursion learning algorithm cerebellar model articulation controller neural networks two modes learning available stochastic batch stochastic learning input creates weight adjustment batch learning weights adjusted based batch inputs accumulating errors batch stochastic learning introduces noise process using local gradient calculated one data point reduces chance network getting stuck local minima however batch learning typically yields faster stable descent local minimum since update performed direction batch average error common compromise use mini batches small batches samples batch selected stochastically entire data set anns evolved broad family techniques advanced state art across multiple domains simplest types one static components including number units number layers unit weights topology dynamic types allow one evolve via learning latter much complicated shorten learning periods produce better results types allow require learning supervised operator others operate independently types operate purely hardware others purely software run general purpose computers main breakthroughs include convolutional neural networks proven particularly successful processing visual two dimensional data long short term memory avoid vanishing gradient problem handle signals mix low high frequency components aiding large vocabulary speech recognition text speech synthesis photo real talking heads competitive networks generative adversarial networks multiple networks compete tasks winning game deceiving opponent authenticity input neural architecture search uses machine learning automate ann design various approaches nas designed networks compare well hand designed systems basic search algorithm propose candidate model evaluate dataset use results feedback teach nas network available systems include automl autokeras design issues include deciding number type connectedness network layers well size connection type hyperparameters must also defined part design governing matters many neurons layer learning rate step stride depth receptive field padding etc using artificial neural networks requires understanding characteristics ann capabilities fall within following broad categories citation needed ability reproduce model nonlinear processes artificial neural networks found applications many disciplines application areas include system identification control quantum chemistry general game playing pattern recognition sequence recognition medical diagnosis finance data mining visualization machine translation social network filtering e mail spam filtering anns used diagnose cancers including lung cancer prostate cancer colorectal cancer distinguish highly invasive cancer cell lines less invasive lines using cell shape information anns used accelerate reliability analysis infrastructures subject natural disasters predict foundation settlements anns also used building black box models geoscience hydrology ocean modelling coastal engineering geomorphology anns employed cybersecurity objective discriminate legitimate activities malicious ones example machine learning used classifying android malware identifying domains belonging threat actors detecting urls posing security risk research underway ann systems designed penetration testing detecting botnets credit cards frauds network intrusions anns proposed tool simulate properties many body open quantum systems brain research anns studied short term behavior individual neurons dynamics neural circuitry arise interactions individual neurons behavior arise abstract neural modules represent complete subsystems studies considered long short term plasticity neural systems relation learning memory individual neuron system level multilayer perceptron universal function approximator proven universal approximation theorem however proof constructive regarding number neurons required network topology weights learning parameters specific recurrent architecture rational valued weights power universal turing machine using finite number neurons standard linear connections use irrational values weights results machine super turing power model capacity property corresponds ability model given function related amount information stored network notion complexity two notions capacity known community information capacity vc dimension information capacity perceptron intensively discussed sir david mackay book summarizes work thomas cover capacity network standard neurons derived four rules derive understanding neuron electrical element information capacity captures functions modelable network given data input second notion vc dimension vc dimension uses principles measure theory finds maximum capacity best possible circumstances given input data specific form noted vc dimension arbitrary inputs half information capacity perceptron vc dimension arbitrary points sometimes referred memory capacity models may consistently converge single solution firstly local minima may exist depending cost function model secondly optimization method used might guarantee converge begins far local minimum thirdly sufficiently large data parameters methods become impractical convergence behavior certain types ann architectures understood others width network approaches infinity ann well described first order taylor expansion throughout training inherits convergence behavior affine models another example parameters small observed anns often fits target functions low high frequencies phenomenon opposite behavior well studied iterative numerical schemes jacobi method applications whose goal create system generalizes well unseen examples face possibility training arises convoluted specified systems network capacity significantly exceeds needed free parameters two approaches address training first use cross validation similar techniques check presence training select hyperparameters minimize generalization error second use form regularization concept emerges probabilistic framework regularization performed selecting larger prior probability simpler models also statistical learning theory goal minimize two quantities empirical risk structural risk roughly corresponds error training set predicted error unseen data due overfitting supervised neural networks use mean squared error cost function use formal statistical methods determine confidence trained model mse validation set used estimate variance value used calculate confidence interval network output assuming normal distribution confidence analysis made way statistically valid long output probability distribution stays network modified assigning softmax activation function generalization logistic function output layer neural network categorical target variables outputs interpreted posterior probabilities useful classification gives certainty measure classifications softmax activation function common criticism neural networks particularly robotics require much training real world operation citation needed potential solutions include randomly shuffling training examples using numerical optimization algorithm take large steps changing network connections following example grouping examples called mini batches introducing recursive least squares algorithm cmac fundamental objection anns sufficiently reflect neuronal function backpropagation critical step although mechanism exists biological neural networks information coded real neurons known sensor neurons fire action potentials frequently sensor activation muscle cells pull strongly associated motor neurons receive action potentials frequently case relaying information sensor neuron motor neuron almost nothing principles information handled biological neural networks known central claim anns embody new powerful general principles processing information unfortunately principles ill defined often claimed emergent network allows simple statistical association described learning recognition alexander dewdney commented result artificial neural networks something nothing quality one imparts peculiar aura laziness distinct lack curiosity good computing systems human hand intervenes solutions found magic one seems learned anything one response dewdney neural networks handle many complex diverse tasks ranging autonomously flying aircraft detecting credit card fraud mastering game go technology writer roger bridgman commented neural networks instance dock hyped high heaven also could create successful net without understanding worked bunch numbers captures behaviour would probability opaque unreadable table valueless scientific resource spite emphatic declaration science technology dewdney seems pillory neural nets bad science devising trying good engineers unreadable table useful machine could read would still well worth biological brains use shallow deep circuits reported brain anatomy displaying wide variety invariance weng argued brain self wires largely according signal statistics therefore serial cascade cannot catch major statistical dependencies large effective neural networks require considerable computing resources brain hardware tailored task processing signals graph neurons simulating even simplified neuron von neumann architecture may consume vast amounts memory storage furthermore designer often needs transmit signals many connections associated neurons require enormous cpu power time schmidhuber noted resurgence neural networks twenty first century largely attributable advances hardware computing power especially delivered gpgpus increased around million fold making standard backpropagation algorithm feasible training networks several layers deeper use accelerators fpgas gpus reduce training times months days neuromorphic engineering addresses hardware difficulty directly constructing non von neumann chips directly implement neural networks circuitry another type chip optimized neural network processing called tensor processing unit tpu analyzing learned ann much easier analyze learned biological neural network furthermore researchers involved exploring learning algorithms neural networks gradually uncovering general principles allow learning machine successful example local vs non local learning shallow vs deep architecture advocates hybrid models claim mixture better capture mechanisms human mind single layer feedforward artificial neural network arrows originating x displaystyle scriptstyle x omitted clarity p inputs network q outputs system value qth output q displaystyle scriptstyle q would calculated q k b q displaystyle scriptstyle q k b q two layer feedforward artificial neural network artificial neural network ann dependency graph single layer feedforward artificial neural network inputs hidden outputs given position state direction outputs wheel based control values two layer feedforward artificial neural network inputs x hidden outputs given position state direction environment values outputs thruster based control values parallel pipeline structure cmac neural network learning algorithm converge one step
https://en.wikipedia.org/wiki/Autoencoder,autoencoder type artificial neural network used learn efficient data codings unsupervised manner aim autoencoder learn representation set data typically dimensionality reduction training network ignore signal noise along reduction side reconstructing side learnt autoencoder tries generate reduced encoding representation close possible original input hence name several variants exist basic model aim forcing learned representations input assume useful properties examples regularized autoencoders proven effective learning representations subsequent classification tasks variational autoencoders recent applications generative models autoencoders effectively used solving many applied problems face recognition acquiring semantic meaning words autoencoder neural network learns copy input output internal layer describes code used represent input constituted two main parts encoder maps input code decoder maps code reconstruction original input performing copying task perfectly would simply duplicate signal autoencoders usually restricted ways force reconstruct input approximately preserving relevant aspects data copy idea autoencoders popular field neural networks decades first applications date back traditional application dimensionality reduction feature learning recently autoencoder concept become widely used learning generative models data powerful ais involved sparse autoencoders stacked inside deep neural networks simplest form autoencoder feedforward non recurrent neural network similar single layer perceptrons participate multilayer perceptrons input layer output layer one hidden layers connecting output layer number nodes input layer purpose reconstructing inputs instead predicting target value displaystyle given inputs x displaystyle x therefore autoencoders unsupervised learning models autoencoder consists two parts encoder decoder defined transitions displaystyle phi displaystyle psi simplest case given one hidden layer encoder stage autoencoder takes input x r x displaystyle mathbf x mathbb r mathcal x maps h r p f displaystyle mathbf h mathbb r p mathcal f image h displaystyle mathbf h usually referred code latent variables latent representation displaystyle sigma element wise activation function sigmoid function rectified linear unit w displaystyle mathbf w weight matrix b displaystyle mathbf b bias vector weights biases usually initialized randomly updated iteratively training backpropagation decoder stage autoencoder maps h displaystyle mathbf h reconstruction x displaystyle mathbf x shape x displaystyle mathbf x w b displaystyle mathbf sigma mathbf w text mathbf b decoder may unrelated corresponding w b displaystyle mathbf sigma mathbf w text mathbf b encoder autoencoders trained minimise reconstruction errors often referred loss x displaystyle mathbf x usually averaged input training set mentioned training autoencoder performed backpropagation error like regular feedforward neural network feature space f displaystyle mathcal f lower dimensionality input space x displaystyle mathcal x feature vector displaystyle phi regarded compressed representation input x displaystyle x case undercomplete autoencoders hidden layers larger equal input layer hidden units given enough capacity autoencoder potentially learn identity function become useless however experimental results shown autoencoders might still learn useful features cases ideal setting one able tailor code dimension model capacity basis complexity data distribution modeled one way exploit model variants known regularized autoencoders various techniques exist prevent autoencoders learning identity function improve ability capture important information learn richer representations recently observed representations learnt way encourages sparsity improved performance obtained classification tasks sparse autoencoder may include hidden units inputs small number hidden units allowed active sparsity constraint forces model respond unique statistical features input data used training specifically sparse autoencoder autoencoder whose training criterion involves sparsity penalty displaystyle omega code layer h displaystyle boldsymbol h l displaystyle mathcal l omega recalling h f displaystyle boldsymbol h f penalty encourages model activate specific areas network basis input data forcing neurons inactive sparsity activation achieved formulating penalty terms different ways j h j displaystyle hat rho j frac sum h j average activation hidden unit j displaystyle j note notation h j displaystyle h j makes explicit input affecting activation e identifies input value activation function encourage neurons inactive would like j displaystyle hat rho j close possible therefore method enforces constraint j displaystyle hat rho j rho displaystyle rho sparsity parameter value close zero leading activation hidden units mostly zero well penalty term displaystyle omega take form penalizes j displaystyle hat rho j deviating significantly displaystyle rho exploiting kl divergence j k l j log j log j displaystyle sum j kl sum j left rho log frac rho hat rho j log frac rho hat rho j right j displaystyle j summing displaystyle hidden nodes hidden layer k l displaystyle kl kl divergence bernoulli random variable mean displaystyle rho bernoulli random variable mean j displaystyle hat rho j l h displaystyle mathcal l lambda sum h differently sparse autoencoders undercomplete autoencoders constrain representation denoising autoencoders try achieve good representation changing reconstruction criterion indeed daes take partially corrupted input trained recover original undistorted input practice objective denoising autoencoders cleaning corrupted input denoising two underlying assumptions inherent approach words denoising advocated training criterion learning extract useful features constitute better higher level representations input training process dae works follows model parameters displaystyle theta displaystyle theta trained minimize average reconstruction error training data specifically minimizing difference z displaystyle boldsymbol z original uncorrupted input x displaystyle boldsymbol x note time random example x displaystyle boldsymbol x presented model new corrupted version generated stochastically basis q displaystyle q mentioned training process could developed kind corruption process examples might additive isotropic gaussian noise masking noise salt pepper noise finally notice corruption input performed training phase dae model learnt optimal parameters order extract representations original data corruption added contractive autoencoder adds explicit regularizer objective function forces model learn function robust slight variations input values regularizer corresponds frobenius norm jacobian matrix encoder activations respect input since penalty applied training examples term forces model learn useful information training distribution final objective function following form name contractive comes fact cae encouraged map neighborhood input points smaller neighborhood output points connection denoising autoencoder contractive autoencoder limit small gaussian input noise dae make reconstruction function resist small finite sized perturbations input cae make extracted features resist infinitesimal perturbations input unlike classical autoencoders variational autoencoders generative models like generative adversarial networks association group models derives mainly architectural affinity basic autoencoder mathematical formulation differs significantly vaes directed probabilistic graphical models whose posterior approximated neural network forming autoencoder like architecture differently discriminative modeling aims learn predictor given observation generative modeling tries simulate data generated order understand underlying causal relations causal relations indeed great potential generalizable variational autoencoder models make strong assumptions concerning distribution latent variables use variational approach latent representation learning results additional loss component specific estimator training algorithm called stochastic gradient variational bayes estimator assumes data generated directed graphical model p displaystyle p theta encoder learning approximation q displaystyle q phi posterior distribution p displaystyle p theta displaystyle mathbf phi displaystyle mathbf theta denote parameters encoder decoder respectively probability distribution latent vector vae typically matches training data much closer standard autoencoder objective vae following form k l displaystyle mathrm kl stands kullback leibler divergence prior latent variables usually set centred isotropic multivariate gaussian p n displaystyle p theta mathcal n however alternative configurations considered commonly shape variational likelihood distributions chosen factorized gaussians displaystyle boldsymbol rho displaystyle boldsymbol omega encoder outputs displaystyle boldsymbol mu displaystyle boldsymbol sigma decoder outputs choice justified simplifications produces evaluating kl divergence likelihood term variational objective defined vae criticized generate blurry images however researchers employing model showing mean distributions displaystyle boldsymbol mu rather sample learned gaussian distribution samples shown overly noisy due choice factorized gaussian distribution employing gaussian distribution full covariance matrix could solve issue computationally intractable numerically unstable requires estimating covariance matrix single data sample however later research showed restricted approach inverse matrix displaystyle boldsymbol sigma sparse could tractably employed generate images high frequency details large scale vae models developed different domains represent data compact probabilistic latent space example vq vae image generation optimus language modeling autoencoders often trained single layer encoder single layer decoder using deep encoders decoders offers many advantages geoffrey hinton developed pretraining technique training many layered deep autoencoders method involves treating neighbouring set two layers restricted boltzmann machine pretraining approximates good solution using backpropagation technique fine tune results model takes name deep belief network recently researchers debated whether joint training would better deep auto encoders study published empirically showed joint training method learns better data models also learned representative features classification compared layerwise method however experiments highlighted success joint training deep autoencoder architectures depends heavily regularization strategies adopted modern variants model two main applications autoencoders since dimensionality reduction information retrieval modern variations basic model proven successful applied different domains tasks dimensionality reduction one first applications deep learning one early motivations study autoencoders nutshell objective find proper projection method maps data high feature space low feature space one milestone paper subject geoffrey hinton publication science magazine study pretrained multi layer autoencoder stack rbms used weights initialize deep autoencoder gradually smaller hidden layers bottleneck neurons resulting dimensions code yielded smaller reconstruction error compared first principal components pca learned representation qualitatively easier interpret clearly separating clusters original data representing data lower dimensional space improve performance different tasks classification indeed many forms dimensionality reduction place semantically related examples near aiding generalization linear activations used single sigmoid hidden layer optimal solution autoencoder strongly related principal component analysis weights autoencoder single hidden layer size p displaystyle p span vector subspace one spanned first p displaystyle p principal components output autoencoder orthogonal projection onto subspace autoencoder weights equal principal components generally orthogonal yet principal components may recovered using singular value decomposition however potential autoencoders resides non linearity allowing model learn powerful generalizations compared pca reconstruct back input significantly lower loss information information retrieval benefits particularly dimensionality reduction search become extremely efficient certain kinds low dimensional spaces autoencoders indeed applied semantic hashing proposed salakhutdinov hinton nutshell training algorithm produce low dimensional binary code database entries could stored hash table mapping binary code vectors entries table would allow perform information retrieval returning entries binary code query slightly less similar entries flipping bits encoding query another field application autoencoders anomaly detection learning replicate salient features training data constraints described previously model encouraged learn precisely reproduce frequent characteristics observations facing anomalies model worsen reconstruction performance cases data normal instances used train autoencoder others frequency anomalies small compared whole population observations contribution representation learnt model could ignored training autoencoder reconstruct normal data well failing anomaly data autoencoder encountered reconstruction error data point error original data point low dimensional reconstruction used anomaly score detect anomalies peculiar characteristics autoencoders rendered model extremely useful processing images various tasks one example found lossy image compression task autoencoders demonstrated potential outperforming approaches proven competitive jpeg another useful application autoencoders field image preprocessing image denoising need efficient image restoration methods grown massive production digital images movies kinds often taken poor conditions autoencoders increasingly proving ability even delicate contexts medical imaging context also used image denoising well super resolution field image assisted diagnosis exist experiments using autoencoders detection breast cancer even modelling relation cognitive decline alzheimer disease latent features autoencoder trained mri lastly successful experiments carried exploiting variations basic autoencoder image super resolution tasks molecules generated special type variational autoencoders validated experimentally way mice variational autoencoder framework used population synthesis approximating high dimensional survey data sampling agents approximated distribution new synthetic fake populations similar statistical properties original population generated recently stacked autoencoder framework shown promising results predicting popularity social media posts helpful online advertisement strategies autoencoder successfully applied machine translation human languages usually referred neural machine translation nmt language texts treated sequences encoded learning procedure decoder side target languages generated recent years also see application language specific autoencoders incorporate linguistic features learning procedure chinese decomposition features
https://en.wikipedia.org/wiki/Deep_learning,deep learning part broader family machine learning methods based artificial neural networks representation learning learning supervised semi supervised unsupervised deep learning architectures deep neural networks deep belief networks recurrent neural networks convolutional neural networks applied fields including computer vision machine vision speech recognition natural language processing audio recognition social network filtering machine translation bioinformatics drug design medical image analysis material inspection board game programs produced results comparable cases surpassing human expert performance artificial neural networks inspired information processing distributed communication nodes biological systems anns various differences biological brains specifically neural networks tend static symbolic biological brain living organisms dynamic analog adjective deep deep learning comes use multiple layers network early work showed linear perceptron cannot universal classifier network nonpolynomial activation function one hidden layer unbounded width hand deep learning modern variation concerned unbounded number layers bounded size permits practical application optimized implementation retaining theoretical universality mild conditions deep learning layers also permitted heterogeneous deviate widely biologically informed connectionist models sake efficiency trainability understandability whence structured part deep learning class machine learning algorithms uses multiple layers progressively extract higher level features raw input example image processing lower layers may identify edges higher layers may identify concepts relevant human digits letters faces modern deep learning models based artificial neural networks specifically convolutional neural networks although also include propositional formulas latent variables organized layer wise deep generative models nodes deep belief networks deep boltzmann machines deep learning level learns transform input data slightly abstract composite representation image recognition application raw input may matrix pixels first representational layer may abstract pixels encode edges second layer may compose encode arrangements edges third layer may encode nose eyes fourth layer may recognize image contains face importantly deep learning process learn features optimally place level word deep deep learning refers number layers data transformed precisely deep learning systems substantial credit assignment path depth cap chain transformations input output caps describe potentially causal connections input output feedforward neural network depth caps network number hidden layers plus one recurrent neural networks signal may propagate layer cap depth potentially unlimited universally agreed upon threshold depth divides shallow learning deep learning researchers agree deep learning involves cap depth higher cap depth shown universal approximator sense emulate function beyond layers add function approximator ability network deep models able extract better features shallow models hence extra layers help learning features effectively deep learning architectures constructed greedy layer layer method deep learning helps disentangle abstractions pick features improve performance supervised learning tasks deep learning methods eliminate feature engineering translating data compact intermediate representations akin principal components derive layered structures remove redundancy representation deep learning algorithms applied unsupervised learning tasks important benefit unlabeled data abundant labeled data examples deep structures trained unsupervised manner neural history compressors deep belief networks deep neural networks generally interpreted terms universal approximation theorem probabilistic inference classic universal approximation theorem concerns capacity feedforward neural networks single hidden layer finite size approximate continuous functions first proof published george cybenko sigmoid activation functions citation needed generalised feed forward multi layer architectures kurt hornik recent work also showed universal approximation also holds non bounded activation functions rectified linear unit universal approximation theorem deep neural networks concerns capacity networks bounded width depth allowed grow lu et al proved width deep neural network relu activation strictly larger input dimension network approximate lebesgue integrable function width smaller equal input dimension deep neural network universal approximator probabilistic interpretation derives field machine learning features inference well optimization concepts training testing related fitting generalization respectively specifically probabilistic interpretation considers activation nonlinearity cumulative distribution function probabilistic interpretation led introduction dropout regularizer neural networks probabilistic interpretation introduced researchers including hopfield widrow narendra popularized surveys one bishop first general working learning algorithm supervised deep feedforward multilayer perceptrons published alexey ivakhnenko lapa paper described deep network eight layers trained group method data handling deep learning working architectures specifically built computer vision began neocognitron introduced kunihiko fukushima term deep learning introduced machine learning community rina dechter artificial neural networks igor aizenberg colleagues context boolean threshold neurons yann lecun et al applied standard backpropagation algorithm around reverse mode automatic differentiation since deep neural network purpose recognizing handwritten zip codes mail algorithm worked training required days systems used recognizing isolated hand written digits recognizing objects done matching images handcrafted object model weng et al suggested human brain use monolithic object model published cresceptron method performing object recognition cluttered scenes directly used natural images cresceptron started beginning general purpose visual learning natural worlds cresceptron cascade layers similar neocognitron neocognitron required human programmer hand merge features cresceptron learned open number features layer without supervision feature represented convolution kernel cresceptron segmented learned object cluttered scene back analysis network max pooling often adopted deep neural networks first used cresceptron reduce position resolution factor cascade better generalization andr de carvalho together mike fairhurst david bisset published experimental results multi layer boolean neural network also known weightless neural network composed layers self organising feature extraction neural network module followed multi layer classification neural network module independently trained layer feature extraction module extracted features growing complexity regarding previous layer brendan frey demonstrated possible train network containing six fully connected layers several hundred hidden units using wake sleep algorithm co developed peter dayan hinton many factors contribute slow speed including vanishing gradient problem analyzed sepp hochreiter simpler models use task specific handcrafted features gabor filters support vector machines popular choice artificial neural network computational cost lack understanding brain wires biological networks shallow deep learning anns explored many years methods never outperformed non uniform internal handcrafting gaussian mixture model hidden markov model technology based generative models speech trained discriminatively key difficulties analyzed including gradient diminishing weak temporal correlation structure neural predictive models additional difficulties lack training data limited computing power speech recognition researchers moved away neural nets pursue generative modeling exception sri international late funded us government nsa darpa sri studied deep neural networks speech speaker recognition speaker recognition team led larry heck reported significant success deep neural networks speech processing national institute standards technology speaker recognition evaluation sri deep neural network deployed nuance verifier representing first major industrial application deep learning principle elevating raw features hand crafted optimization first explored successfully architecture deep autoencoder raw spectrogram linear filter bank features late showing superiority mel cepstral features contain stages fixed transformation spectrograms raw features speech waveforms later produced excellent larger scale results many aspects speech recognition taken deep learning method called long short term memory recurrent neural network published hochreiter schmidhuber lstm rnns avoid vanishing gradient problem learn deep learning tasks require memories events happened thousands discrete time steps important speech lstm started become competitive traditional speech recognizers certain tasks later combined connectionist temporal classification stacks lstm rnns google speech recognition reportedly experienced dramatic performance jump ctc trained lstm made available google voice search publications geoff hinton ruslan salakhutdinov osindero teh showed many layered feedforward neural network could effectively pre trained one layer time treating layer turn unsupervised restricted boltzmann machine fine tuning using supervised backpropagation papers referred learning deep belief nets deep learning part state art systems various disciplines particularly computer vision automatic speech recognition results commonly used evaluation sets timit mnist well range large vocabulary speech recognition tasks steadily improved convolutional neural networks superseded asr ctc lstm successful computer vision impact deep learning industry began early cnns already processed estimated checks written us according yann lecun industrial applications deep learning large scale speech recognition started around nips workshop deep learning speech recognition motivated limitations deep generative models speech possibility given capable hardware large scale data sets deep neural nets might become practical believed pre training dnns using generative models deep belief nets would overcome main difficulties neural nets however discovered replacing pre training large amounts training data straightforward backpropagation using dnns large context dependent output layers produced error rates dramatically lower state art gaussian mixture model hidden markov model also advanced generative model based systems nature recognition errors produced two types systems characteristically different offering technical insights integrate deep learning existing highly efficient run time speech decoding system deployed major speech recognition systems analysis around contrasting gmm vs dnn models stimulated early industrial investment deep learning speech recognition eventually leading pervasive dominant use industry analysis done comparable performance discriminative dnns generative models researchers extended deep learning timit large vocabulary speech recognition adopting large output layers dnn based context dependent hmm states constructed decision trees advances hardware driven renewed interest deep learning nvidia involved called big bang deep learning deep learning neural networks trained nvidia graphics processing units year google brain used nvidia gpus create capable dnns andrew ng determined gpus could increase speed deep learning systems times particular gpus well suited matrix vector computations involved machine learning gpus speed training algorithms orders magnitude reducing running times weeks days specialized hardware algorithm optimizations used efficient processing deep learning models team led george e dahl merck molecular activity challenge using multi task deep neural networks predict biomolecular target one drug hochreiter group used deep learning detect target toxic effects environmental chemicals nutrients household products drugs tox data challenge nih fda ncats significant additional impacts image object recognition felt although cnns trained backpropagation around decades gpu implementations nns years including cnns fast implementations cnns gpus needed progress computer vision approach achieved first time superhuman performance visual pattern recognition contest also icdar chinese handwriting contest may isbi image segmentation contest cnns play major role computer vision conferences june paper ciresan et al leading conference cvpr showed max pooling cnns gpu dramatically improve many vision benchmark records october similar system krizhevsky et al large scale imagenet competition significant margin shallow machine learning methods november ciresan et al system also icpr contest analysis large medical images cancer detection following year also miccai grand challenge topic error rate imagenet task using deep learning reduced following similar trend large scale speech recognition wolfram image identification project publicized improvements image classification extended challenging task generating descriptions images often combination cnns lstms researchers state october imagenet victory anchored start deep learning revolution transformed ai industry march yoshua bengio geoffrey hinton yann lecun awarded turing award conceptual engineering breakthroughs made deep neural networks critical component computing artificial neural networks connectionist systems computing systems inspired biological neural networks constitute animal brains systems learn tasks considering examples generally without task specific programming example image recognition might learn identify images contain cats analyzing example images manually labeled cat cat using analytic results identify cats images found use applications difficult express traditional computer algorithm using rule based programming ann based collection connected units called artificial neurons connection neurons transmit signal another neuron receiving neuron process signal signal downstream neurons connected neurons may state generally represented real numbers typically neurons synapses may also weight varies learning proceeds increase decrease strength signal sends downstream typically neurons organized layers different layers may perform different kinds transformations inputs signals travel first last layer possibly traversing layers multiple times original goal neural network approach solve problems way human brain would time attention focused matching specific mental abilities leading deviations biology backpropagation passing information reverse direction adjusting network reflect information neural networks used variety tasks including computer vision speech recognition machine translation social network filtering playing board video games medical diagnosis neural networks typically thousand million units millions connections despite number several order magnitude less number neurons human brain networks perform many tasks level beyond humans deep neural network artificial neural network multiple layers input output layers dnn finds correct mathematical manipulation turn input output whether linear relationship non linear relationship network moves layers calculating probability output example dnn trained recognize dog breeds go given image calculate probability dog image certain breed user review results select probabilities network display return proposed label mathematical manipulation considered layer complex dnn many layers hence name deep networks dnns model complex non linear relationships dnn architectures generate compositional models object expressed layered composition primitives extra layers enable composition features lower layers potentially modeling complex data fewer units similarly performing shallow network deep architectures include many variants basic approaches architecture found success specific domains always possible compare performance multiple architectures unless evaluated data sets dnns typically feedforward networks data flows input layer output layer without looping back first dnn creates map virtual neurons assigns random numerical values weights connections weights inputs multiplied return output network accurately recognize particular pattern algorithm would adjust weights way algorithm make certain parameters influential determines correct mathematical manipulation fully process data recurrent neural networks data flow direction used applications language modeling long short term memory particularly effective use convolutional deep neural networks used computer vision cnns also applied acoustic modeling automatic speech recognition anns many issues arise naively trained dnns two common issues overfitting computation time dnns prone overfitting added layers abstraction allow model rare dependencies training data regularization methods ivakhnenko unit pruning weight decay sparsity applied training combat overfitting alternatively dropout regularization randomly omits units hidden layers training helps exclude rare dependencies finally data augmented via methods cropping rotating smaller training sets increased size reduce chances overfitting dnns must consider many training parameters size learning rate initial weights sweeping parameter space optimal parameters may feasible due cost time computational resources various tricks batching speed computation large processing capabilities many core architectures produced significant speedups training suitability processing architectures matrix vector computations alternatively engineers may look types neural networks straightforward convergent training algorithms cmac one kind neural network require learning rates randomized initial weights cmac training process guaranteed converge one step new batch data computational complexity training algorithm linear respect number neurons involved since advances machine learning algorithms computer hardware led efficient methods training deep neural networks contain many layers non linear hidden units large output layer graphic processing units often ai specific enhancements displaced cpus dominant method training large scale commercial cloud ai openai estimated hardware compute used largest deep learning projects alexnet alphazero found fold increase amount compute required doubling time trendline months large scale automatic speech recognition first convincing successful case deep learning lstm rnns learn deep learning tasks involve multi second intervals containing speech events separated thousands discrete time steps one time step corresponds ms lstm forget gates competitive traditional speech recognizers certain tasks initial success speech recognition based small scale recognition tasks based timit data set contains speakers eight major dialects american english speaker reads sentences small size lets many configurations tried importantly timit task concerns phone sequence recognition unlike word sequence recognition allows weak phone bigram language models lets strength acoustic modeling aspects speech recognition easily analyzed error rates listed including early results measured percent phone error rates summarized since debut dnns speaker recognition late speech recognition around lstm around accelerated progress eight major areas major commercial speech recognition systems based deep learning electromyography signals used extensively identification user intention potentially control assistive devices smart wheelchairs exoskeletons prosthetic devices past century feed forward dense neural network used researcher used spectrogram map emg signal use input deep convolutional neural networks recently end end deep learning used map raw signals directly identification user intention common evaluation set image classification mnist database data set mnist composed handwritten digits includes training examples test examples timit small size lets users test multiple configurations comprehensive list results set available deep learning based image recognition become superhuman producing accurate results human contestants first occurred deep learning trained vehicles interpret camera views another example facial dysmorphology novel analysis used analyze cases human malformation connected large database genetic syndromes closely related progress made image recognition increasing application deep learning techniques various visual art tasks dnns proven capable example identifying style period given painting b neural style transfer capturing style given artwork applying visually pleasing manner arbitrary photograph video c generating striking imagery based random visual input fields neural networks used implementing language models since early lstm helped improve machine translation language modeling key techniques field negative sampling word embedding word embedding word vec thought representational layer deep learning architecture transforms atomic word positional representation word relative words dataset position represented point vector space using word embedding rnn input layer allows network parse sentences phrases using effective compositional vector grammar compositional vector grammar thought probabilistic context free grammar implemented rnn recursive auto encoders built atop word embeddings assess sentence similarity detect paraphrasing deep neural architectures provide best results constituency parsing sentiment analysis information retrieval spoken language understanding machine translation contextual entity linking writing style recognition text classification others recent developments generalize word embedding sentence embedding google translate uses large end end long short term memory network google neural machine translation uses example based machine translation method system learns millions examples translates whole sentences time rather pieces google translate supports one hundred languages network encodes semantics sentence rather simply memorizing phrase phrase translations gt uses english intermediate language pairs large percentage candidate drugs fail win regulatory approval failures caused insufficient efficacy undesired interactions unanticipated toxic effects research explored use deep learning predict biomolecular targets targets toxic effects environmental chemicals nutrients household products drugs atomnet deep learning system structure based rational drug design atomnet used predict novel candidate biomolecules disease targets ebola virus multiple sclerosis generative neural networks used produce molecules validated experimentally way mice deep reinforcement learning used approximate value possible direct marketing actions defined terms rfm variables estimated value function shown natural interpretation customer lifetime value recommendation systems used deep learning extract meaningful features latent factor model content based music journal recommendations multi view deep learning applied learning user preferences multiple domains model uses hybrid collaborative content based approach enhances recommendations multiple tasks autoencoder ann used bioinformatics predict gene ontology annotations gene function relationships medical informatics deep learning used predict sleep quality based data wearables predictions health complications electronic health record data deep learning shown produce competitive results medical application cancer cell classification lesion detection organ segmentation image enhancement finding appropriate mobile audience mobile advertising always challenging since many data points must considered analyzed target segment created used ad serving ad server deep learning used interpret large many dimensioned advertising datasets many data points collected request serve click internet advertising cycle information form basis machine learning improve ad selection deep learning successfully applied inverse problems denoising super resolution inpainting film colorization applications include learning methods shrinkage fields effective image restoration trains image dataset deep image prior trains image needs restoration deep learning successfully applied financial fraud detection anti money laundering deep anti money laundering detection system spot recognize relationships similarities data road learn detect anomalies classify predict specific events solution leverages supervised learning techniques classification suspicious transactions unsupervised learning e g anomaly detection united states department defense applied deep learning train robots new tasks observation deep learning closely related class theories brain development proposed cognitive neuroscientists early developmental theories instantiated computational models making predecessors deep learning systems developmental models share property various proposed learning dynamics brain support self organization somewhat analogous neural networks utilized deep learning models like neocortex neural networks employ hierarchy layered filters layer considers information prior layer passes output layers process yields self organizing stack transducers well tuned operating environment description stated infant brain seems organize influence waves called trophic factors different regions brain become connected sequentially one layer tissue maturing another whole brain mature variety approaches used investigate plausibility deep learning models neurobiological perspective one hand several variants backpropagation algorithm proposed order increase processing realism researchers argued unsupervised forms deep learning based hierarchical generative models deep belief networks may closer biological reality respect generative neural network models related neurobiological evidence sampling based processing cerebral cortex although systematic comparison human brain organization neuronal encoding deep networks yet established several analogies reported example computations performed deep learning units could similar actual neurons neural populations similarly representations developed deep learning models similar measured primate visual system single unit population levels facebook ai lab performs tasks automatically tagging uploaded pictures names people google deepmind technologies developed system capable learning play atari video games using pixels data input demonstrated alphago system learned game go well enough beat professional go player google translate uses neural network translate languages blippar demonstrated mobile augmented reality application uses deep learning recognize objects real time covariant ai launched focuses integrating deep learning factories researchers university texas austin developed machine learning framework called training agent manually via evaluative reinforcement tamer proposed new methods robots computer programs learn perform tasks interacting human instructor first developed tamer new algorithm called deep tamer later introduced collaboration u army research laboratory ut researchers deep tamer used deep learning provide robot ability learn new tasks observation using deep tamer robot learned task human trainer watching video streams observing human perform task person robot later practiced task help coaching trainer provided feedback good job bad job deep learning attracted criticism comment cases outside field computer science main criticism concerns lack theory surrounding methods learning common deep architectures implemented using well understood gradient descent however theory surrounding algorithms contrastive divergence less clear citation needed deep learning methods often looked black box confirmations done empirically rather theoretically others point deep learning looked step towards realizing strong ai encompassing solution despite power deep learning methods still lack much functionality needed realizing goal entirely research psychologist gary marcus noted realistically deep learning part larger challenge building intelligent machines techniques lack ways representing causal relationships obvious ways performing logical inferences also still long way integrating abstract knowledge information objects typically used powerful systems like watson use techniques like deep learning one element complicated ensemble techniques ranging statistical technique bayesian inference deductive reasoning reference idea artistic sensitivity might inhere within relatively low levels cognitive hierarchy published series graphic representations internal states deep neural networks attempting discern within essentially random data images trained demonstrate visual appeal original research notice received well comments subject time frequently accessed article guardian website deep learning architectures display problematic behaviors confidently classifying unrecognizable images belonging familiar category ordinary images misclassifying minuscule perturbations correctly classified images goertzel hypothesized behaviors due limitations internal representations limitations would inhibit integration heterogeneous multi component artificial general intelligence architectures issues may possibly addressed deep learning architectures internally form states homologous image grammar decompositions observed entities events learning grammar training data would equivalent restricting system commonsense reasoning operates concepts terms grammatical production rules basic goal human language acquisition artificial intelligence deep learning moves lab world research experience shows artificial neural networks vulnerable hacks deception identifying patterns systems use function attackers modify inputs anns way ann finds match human observers would recognize example attacker make subtle changes image ann finds match even though image looks human nothing like search target manipulation termed adversarial attack researchers used one ann doctor images trial error fashion identify another focal points thereby generate images deceived modified images looked different human eyes another group showed printouts doctored images photographed successfully tricked image classification system one defense reverse image search possible fake image submitted site tineye find instances refinement search using parts image identify images piece may taken another group showed certain psychedelic spectacles could fool facial recognition system thinking ordinary people celebrities potentially allowing one person impersonate another researchers added stickers stop signs caused ann misclassify anns however trained detect attempts deception potentially leading attackers defenders arms race similar kind already defines malware defense industry anns trained defeat ann based anti malware software repeatedly attacking defense malware continually altered genetic algorithm tricked anti malware retaining ability damage target another group demonstrated certain sounds could make google voice command system open particular web address would download malware data poisoning false data continually smuggled machine learning system training set prevent achieving mastery deep learning systems rely training verification data generated annotated humans argued media philosophy low paid clickwork regularly deployed purpose also implicit forms human microwork often recognized philosopher rainer hlhoff distinguishes five types machinic capture human microwork generate training data gamification trapping tracking exploitation social motivations information mining clickwork hlhoff argues commercial end user applications deep learning facebook face recognition system need training data stop ann trained rather continued demand human generated verification data constantly calibrate update ann purpose facebook introduced feature user automatically recognized image receive notification choose whether like publicly labeled image tell facebook picture user interface mechanism generate constant stream verification data train network real time hlhoff argues involvement human users generate training verification data typical commercial end user applications deep learning systems may referred human aided artificial intelligence
https://en.wikipedia.org/wiki/DeepDream,deepdream computer vision program created google engineer alexander mordvintsev uses convolutional neural network find enhance patterns images via algorithmic pareidolia thus creating dream like hallucinogenic appearance deliberately processed images google program popularized term dreaming refer generation images produce desired activations trained deep network term refers collection related approaches deepdream software originated deep convolutional network codenamed inception film name developed imagenet large scale visual recognition challenge released july dreaming idea name became popular internet thanks google deepdream program idea dates early history neural networks similar methods used synthesize visual textures related visualization ideas developed several research groups google published techniques made code open source number tools form web services mobile applications desktop software appeared market enable users transform photos software designed detect faces patterns images aim automatically classifying images however trained network also run reverse asked adjust original image slightly given output neuron yields higher confidence score used visualizations understand emergent structure neural network better basis deepdream concept reversal procedure never perfectly clear unambiguous utilizes one many mapping process however enough reiterations even imagery initially devoid sought features adjusted enough form pareidolia results psychedelic surreal images generated algorithmically optimization resembles backpropagation however instead adjusting network weights weights held fixed input adjusted example existing image altered cat like resulting enhanced image input procedure usage resembles activity looking animals patterns clouds applying gradient descent independently pixel input produces images adjacent pixels little relation thus image much high frequency information generated images greatly improved including prior regularizer prefers inputs natural image statistics simply smooth example mahendran et al used total variation regularizer prefers images piecewise constant various regularizers discussed depth visual exploration feature visualization regularization techniques published recently cited resemblance imagery lsd psilocybin induced hallucinations suggestive functional resemblance artificial neural networks particular layers visual cortex dreaming idea applied hidden neurons output allows exploration roles representations various parts network also possible optimize input satisfy either single neuron entire layer neurons dreaming often used visualizing networks producing computer art recently proposed adding dreamed inputs training set improve training times abstractions computer science deepdream model also demonstrated application field art history deepdream used foster people music video song money research group university sussex created hallucination machine applying deepdream algorithm pre recorded panoramic video allowing users explore virtual reality environments mimic experience psychoactive substances psychopathological conditions able demonstrate subjective experiences induced hallucination machine differed significantly control videos bearing phenomenological similarities psychedelic state
https://en.wikipedia.org/wiki/Multilayer_perceptron,multilayer perceptron class feedforward artificial neural network term mlp used ambiguously sometimes loosely feedforward ann sometimes strictly refer networks composed multiple layers perceptrons see terminology multilayer perceptrons sometimes colloquially referred vanilla neural networks especially single hidden layer mlp consists least three layers nodes input layer hidden layer output layer except input nodes node neuron uses nonlinear activation function mlp utilizes supervised learning technique called backpropagation training multiple layers non linear activation distinguish mlp linear perceptron distinguish data linearly separable multilayer perceptron linear activation function neurons linear function maps weighted inputs output neuron linear algebra shows number layers reduced two layer input output model mlps neurons use nonlinear activation function developed model frequency action potentials firing biological neurons two historically common activation functions sigmoids described recent developments deep learning rectifier linear unit frequently used one possible ways overcome numerical problems related sigmoids first hyperbolic tangent ranges logistic function similar shape ranges displaystyle output displaystyle th node v displaystyle v weighted sum input connections alternative activation functions proposed including rectifier softplus functions specialized activation functions include radial basis functions mlp consists three layers nonlinearly activating nodes since mlps fully connected node one layer connects certain weight w j displaystyle w ij every node following layer learning occurs perceptron changing connection weights piece data processed based amount error output compared expected result example supervised learning carried backpropagation generalization least mean squares algorithm linear perceptron represent degree error output node j displaystyle j n displaystyle n th data point e j j j displaystyle e j j j displaystyle target value displaystyle value produced perceptron node weights adjusted based corrections minimize error entire output given using gradient descent change weight displaystyle output previous neuron displaystyle eta learning rate selected ensure weights quickly converge response without oscillations derivative calculated depends induced local field v j displaystyle v j varies easy prove output node derivative simplified displaystyle phi prime derivative activation function described vary analysis difficult change weights hidden node shown relevant derivative depends change weights k displaystyle k th nodes represent output layer change hidden layer weights output layer weights change according derivative activation function algorithm represents backpropagation activation function term multilayer perceptron refer single perceptron multiple layers rather contains many perceptrons organized layers alternative multilayer perceptron network moreover mlp perceptrons perceptrons strictest possible sense true perceptrons formally special case artificial neurons use threshold activation function heaviside step function mlp perceptrons employ arbitrary activation functions true perceptron performs binary classification mlp neuron free either perform classification regression depending upon activation function term multilayer perceptron later applied without respect nature nodes layers composed arbitrarily defined artificial neurons perceptrons specifically interpretation avoids loosening definition perceptron mean artificial neuron general mlps useful research ability solve problems stochastically often allows approximate solutions extremely complex problems like fitness approximation mlps universal function approximators shown cybenko theorem used create mathematical models regression analysis classification particular case regression response variable categorical mlps make good classifier algorithms mlps popular machine learning solution finding applications diverse fields speech recognition image recognition machine translation software thereafter faced strong competition much simpler support vector machines interest backpropagation networks returned due successes deep learning
https://en.wikipedia.org/wiki/Recurrent_neural_network,recurrent neural network class artificial neural networks connections nodes form directed graph along temporal sequence allows exhibit temporal dynamic behavior derived feedforward neural networks rnns use internal state process variable length sequences inputs makes applicable tasks unsegmented connected handwriting recognition speech recognition term recurrent neural network used indiscriminately refer two broad classes networks similar general structure one finite impulse infinite impulse classes networks exhibit temporal dynamic behavior finite impulse recurrent network directed acyclic graph unrolled replaced strictly feedforward neural network infinite impulse recurrent network directed cyclic graph unrolled finite impulse infinite impulse recurrent networks additional stored states storage direct control neural network storage also replaced another network graph incorporates time delays feedback loops controlled states referred gated state gated memory part long short term memory networks gated recurrent units also called feedback neural network recurrent neural networks based david rumelhart work hopfield networks special kind rnn discovered john hopfield neural history compressor system solved deep learning task required subsequent layers rnn unfolded time long short term memory networks invented hochreiter schmidhuber set accuracy records multiple applications domains around lstm started revolutionize speech recognition outperforming traditional models certain speech applications connectionist temporal classification trained lstm network first rnn win pattern recognition contests several competitions connected handwriting recognition chinese search giant baidu used ctc trained rnns break switchboard hub speech recognition dataset benchmark without using traditional speech processing methods lstm also improved large vocabulary speech recognition text speech synthesis used google android google speech recognition reportedly experienced dramatic performance jump citation needed ctc trained lstm used lstm broke records improved machine translation language modeling multilingual language processing lstm combined convolutional neural networks improved automatic image captioning given computation memory overheads running lstms efforts accelerating lstm using hardware accelerators rnns come many variants basic rnns network neuron like nodes organized successive layers node given layer connected directed connection every node next successive layer citation needed node time varying real valued activation connection modifiable real valued weight nodes either input nodes output nodes hidden nodes supervised learning discrete time settings sequences real valued input vectors arrive input nodes one vector time given time step non input unit computes current activation nonlinear function weighted sum activations units connect supervisor given target activations supplied output units certain time steps example input sequence speech signal corresponding spoken digit final target output end sequence may label classifying digit reinforcement learning settings teacher provides target signals instead fitness function reward function occasionally used evaluate rnn performance influences input stream output units connected actuators affect environment might used play game progress measured number points sequence produces error sum deviations target signals corresponding activations computed network training set numerous sequences total error sum errors individual sequences elman network three layer network addition set context units middle layer connected context units fixed weight one time step input fed forward learning rule applied fixed back connections save copy previous values hidden units context units thus network maintain sort state allowing perform tasks sequence prediction beyond power standard multilayer perceptron jordan networks similar elman networks context units fed output layer instead hidden layer context units jordan network also referred state layer recurrent connection elman jordan networks also known simple recurrent networks variables functions hopfield network rnn connections symmetric requires stationary inputs thus general rnn process sequences patterns guarantees converge connections trained using hebbian learning hopfield network perform robust content addressable memory resistant connection alteration introduced bart kosko bidirectional associative memory network variant hopfield network stores associative data vector bi directionality comes passing information matrix transpose typically bipolar encoding preferred binary encoding associative pairs recently stochastic bam models using markov stepping optimized increased network stability relevance real world applications bam network two layers either driven input recall association produce output layer echo state network sparsely connected random hidden layer weights output neurons part network change esns good reproducing certain time series variant spiking neurons known liquid state machine independently recurrent neural network addresses gradient vanishing exploding problems traditional fully connected rnn neuron one layer receives past state context information thus neurons independent history gradient backpropagation regulated avoid gradient vanishing exploding order keep long short term memory cross neuron information explored next layers indrnn robustly trained non saturated nonlinear functions relu using skip connections deep networks trained recursive neural network created applying set weights recursively differentiable graph like structure traversing structure topological order networks typically also trained reverse mode automatic differentiation process distributed representations structure logical terms special case recursive neural networks rnn whose structure corresponds linear chain recursive neural networks applied natural language processing recursive neural tensor network uses tensor based composition function nodes tree neural history compressor unsupervised stack rnns input level learns predict next input previous inputs unpredictable inputs rnn hierarchy become inputs next higher level rnn therefore recomputes internal state rarely higher level rnn thus studies compressed representation information rnn done input sequence precisely reconstructed representation highest level system effectively minimises description length negative logarithm probability data given lot learnable predictability incoming data sequence highest level rnn use supervised learning easily classify even deep sequences long intervals important events possible distill rnn hierarchy two rnns conscious chunker subconscious automatizer chunker learned predict compress inputs unpredictable automatizer automatizer forced next learning phase predict imitate additional units hidden units slowly changing chunker makes easy automatizer learn appropriate rarely changing memories across long intervals turn helps automatizer make many unpredictable inputs predictable chunker focus remaining unpredictable events generative model partially overcame vanishing gradient problem automatic differentiation backpropagation neural networks system solved deep learning task required subsequent layers rnn unfolded time second order rnns use higher order weights w j k displaystyle w ijk instead standard w j displaystyle w ij weights states product allows direct mapping finite state machine training stability representation long short term memory example formal mappings proof stability long short term memory deep learning system avoids vanishing gradient problem lstm normally augmented recurrent gates called forget gates lstm prevents backpropagated errors vanishing exploding instead errors flow backwards unlimited numbers virtual layers unfolded space lstm learn tasks require memories events happened thousands even millions discrete time steps earlier problem specific lstm like topologies evolved lstm works even given long delays significant events handle signals mix low high frequency components many applications use stacks lstm rnns train connectionist temporal classification find rnn weight matrix maximizes probability label sequences training set given corresponding input sequences ctc achieves alignment recognition lstm learn recognize context sensitive languages unlike previous models based hidden markov models similar concepts gated recurrent units gating mechanism recurrent neural networks introduced used full form several simplified variants performance polyphonic music modeling speech signal modeling found similar long short term memory fewer parameters lstm lack output gate bi directional rnns use finite sequence predict label element sequence based element past future contexts done concatenating outputs two rnns one processing sequence left right one right left combined outputs predictions teacher given target signals technique proven especially useful combined lstm rnns continuous time recurrent neural network uses system ordinary differential equations model effects neuron incoming spike train neuron displaystyle network action potential displaystyle rate change activation given ctrnns applied evolutionary robotics used address vision co operation minimal cognitive behaviour note shannon sampling theorem discrete time recurrent neural networks viewed continuous time recurrent neural networks differential equations transformed equivalent difference equations transformation thought occurring post synaptic node activation functions displaystyle low pass filtered prior sampling hierarchical rnns connect neurons various ways decompose hierarchical behavior useful subprograms generally recurrent multilayer perceptron network network consists cascaded subnetworks contains multiple layers nodes subnetworks feed forward except last layer feedback connections subnets connected feed forward connections multiple timescales recurrent neural network neural based computational model simulate functional hierarchy brain self organization depends spatial connection neurons distinct types neuron activities distinct time properties varied neuronal activities continuous sequences set behaviors segmented reusable primitives turn flexibly integrated diverse sequential behaviors biological approval type hierarchy discussed memory prediction theory brain function hawkins book intelligence citation needed neural turing machines method extending recurrent neural networks coupling external memory resources interact attentional processes combined system analogous turing machine von neumann architecture differentiable end end allowing efficiently trained gradient descent differentiable neural computers extension neural turing machines allowing usage fuzzy amounts memory address record chronology neural network pushdown automata similar ntms tapes replaced analogue stacks differentiable trained way similar complexity recognizers context free grammars greg snider hp labs describes system cortical computing memristive nanodevices memristors implemented thin film materials resistance electrically tuned via transport ions oxygen vacancies within film darpa synapse project funded ibm research hp labs collaboration boston university department cognitive neural systems develop neuromorphic architectures may based memristive systems memristive networks particular type physical neural network similar properties hopfield networks continuous dynamics limited memory capacity natural relax via minimization function asymptotic ising model sense dynamics memristive circuit advantage compared resistor capacitor network interesting non linear behavior point view engineering analog memristive networks accounts peculiar type neuromorphic engineering device behavior depends circuit wiring topology gradient descent first order iterative optimization algorithm finding minimum function neural networks used minimize error term changing weight proportion derivative error respect weight provided non linear activation functions differentiable various methods developed early werbos williams robinson schmidhuber hochreiter pearlmutter others standard method called backpropagation time bptt generalization back propagation feed forward networks like method instance automatic differentiation reverse accumulation mode pontryagin minimum principle computationally expensive online variant called real time recurrent learning rtrl instance automatic differentiation forward accumulation mode stacked tangent vectors unlike bptt algorithm local time local space context local space means unit weight vector updated using information stored connected units unit update complexity single unit linear dimensionality weight vector local time means updates take place continually depend recent time step rather multiple time steps within given time horizon bptt biological neural networks appear local respect time space recursively computing partial derivatives rtrl time complexity per time step computing jacobian matrices bptt takes per time step cost storing forward activations within given time horizon online hybrid bptt rtrl intermediate complexity exists along variants continuous time major problem gradient descent standard rnn architectures error gradients vanish exponentially quickly size time lag important events lstm combined bptt rtrl hybrid learning method attempts overcome problems problem also solved independently recurrent neural network reducing context neuron past state cross neuron information explored following layers memories different range including long term memory learned without gradient vanishing exploding problem line algorithm called causal recursive backpropagation implements combines bptt rtrl paradigms locally recurrent networks works general locally recurrent networks crbp algorithm minimize global error term fact improves stability algorithm providing unifying view gradient calculation techniques recurrent networks local feedback one approach computation gradient information rnns arbitrary architectures based signal flow graphs diagrammatic derivation uses bptt batch algorithm based lee theorem network sensitivity calculations proposed wan beaufays fast online version proposed campolucci uncini piazza training weights neural network modeled non linear global optimization problem target function formed evaluate fitness error particular weight vector follows first weights network set according weight vector next network evaluated training sequence typically sum squared difference predictions target values specified training sequence used represent error current weight vector arbitrary global optimization techniques may used minimize target function common global optimization method training rnns genetic algorithms especially unstructured networks initially genetic algorithm encoded neural network weights predefined manner one gene chromosome represents one weight link whole network represented single chromosome fitness function evaluated follows many chromosomes make population therefore many different neural networks evolved stopping criterion satisfied common stopping scheme stopping criterion evaluated fitness function gets reciprocal mean squared error network training therefore goal genetic algorithm maximize fitness function reducing mean squared error global optimization techniques may used seek good set weights simulated annealing particle swarm optimization rnns may behave chaotically cases dynamical systems theory may used analysis fact recursive neural networks particular structure linear chain whereas recursive neural networks operate hierarchical structure combining child representations parent representations recurrent neural networks operate linear progression time combining previous time step hidden representation representation current time step particular rnns appear nonlinear versions finite impulse response infinite impulse response filters also nonlinear autoregressive exogenous model applications recurrent neural networks include
https://en.wikipedia.org/wiki/Long_short-term_memory,long short term memory artificial recurrent neural network architecture used field deep learning unlike standard feedforward neural networks lstm feedback connections process single data points also entire sequences data example lstm applicable tasks unsegmented connected handwriting recognition speech recognition anomaly detection network traffic idss common lstm unit composed cell input gate output gate forget gate cell remembers values arbitrary time intervals three gates regulate flow information cell lstm networks well suited classifying processing making predictions based time series data since lags unknown duration important events time series lstms developed deal vanishing gradient problem encountered training traditional rnns relative insensitivity gap length advantage lstm rnns hidden markov models sequence learning methods numerous applications citation needed advantage lstm cell compared common recurrent unit cell memory unit cell vector ability encapsulate notion forgetting part previously stored memory well add part new information illustrate one inspect equations cell way processes sequences hood lstm proposed sepp hochreiter j rgen schmidhuber introducing constant error carousel units lstm deals vanishing gradient problem initial version lstm block included cells input output gates felix gers advisor j rgen schmidhuber fred cummins introduced forget gate lstm architecture enabling lstm reset state gers schmidhuber cummins added peephole connections architecture additionally output activation function omitted lstm based model icdar connected handwriting recognition competition three models submitted team lead alex graves one accurate model competition another fastest lstm networks major component network achieved record phoneme error rate classic timit natural speech dataset kyunghyun cho et al put forward simplified variant called gated recurrent unit google started using lstm speech recognition google voice according official blog post new model cut transcription errors google started using lstm suggest messages allo conversation app year google released google neural machine translation system google translate used lstms reduce translation errors apple announced worldwide developers conference would start using lstm quicktype iphone siri amazon released polly generates voices behind alexa using bidirectional lstm text speech technology facebook performed billion automatic translations every day using long short term memory networks researchers michigan state university ibm research cornell university published study knowledge discovery data mining conference study describes novel neural network performs better certain data sets widely used long short term memory neural network microsoft reported reaching recognition accuracy switchboard corpus incorporating vocabulary words approach used dialog session based long short term memory researchers university waterloo proposed related rnn architecture represents continuous windows time derived using legendre polynomials outperforms lstm memory related benchmarks lstm model climbed third place large text compression benchmark theory classic rnns keep track arbitrary long term dependencies input sequences problem vanilla rnns computational nature training vanilla rnn using back propagation gradients back propagated vanish explode computations involved process use finite precision numbers rnns using lstm units partially solve vanishing gradient problem lstm units allow gradients also flow unchanged however lstm networks still suffer exploding gradient problem several architectures lstm units common architecture composed cell three regulators usually called gates flow information inside lstm unit input gate output gate forget gate variations lstm unit one gates maybe gates example gated recurrent units output gate intuitively cell responsible keeping track dependencies elements input sequence input gate controls extent new value flows cell forget gate controls extent value remains cell output gate controls extent value cell used compute output activation lstm unit activation function lstm gates often logistic sigmoid function connections lstm gates recurrent weights connections need learned training determine gates operate equations lowercase variables represent vectors matrices w q displaystyle w q u q displaystyle u q contain respectively weights input recurrent connections subscript q displaystyle q either input gate displaystyle output gate displaystyle forget gate f displaystyle f memory cell c displaystyle c depending activation calculated section thus using vector notation example c r h displaystyle c mathbb r h one cell one lstm unit contains h displaystyle h lstm unit cells compact forms equations forward pass lstm unit forget gate initial values c displaystyle c h displaystyle h operator displaystyle circ denotes hadamard product subscript displaystyle indexes time step superscripts displaystyle h displaystyle h refer number input features number hidden units respectively figure right graphical representation lstm unit peephole connections peephole connections allow gates access constant error carousel whose activation cell state h displaystyle h used c displaystyle c used instead places peephole convolutional lstm displaystyle denotes convolution operator rnn using lstm units trained supervised fashion set training sequences using optimization algorithm like gradient descent combined backpropagation time compute gradients needed optimization process order change weight lstm network proportion derivative error respect corresponding weight problem using gradient descent standard rnns error gradients vanish exponentially quickly size time lag important events due lim n w n displaystyle lim n infty w n spectral radius w displaystyle w smaller however lstm units error values back propagated output layer error remains lstm unit cell error carousel continuously feeds error back lstm unit gates learn cut value many applications use stacks lstm rnns train connectionist temporal classification find rnn weight matrix maximizes probability label sequences training set given corresponding input sequences ctc achieves alignment recognition sometimes advantageous train lstm neuroevolution policy gradient methods especially teacher several successful stories training non supervised fashion rnns lstm units bill gates called huge milestone advancing artificial intelligence bots developed openai able beat humans game dota openai five consists five independent coordinated neural networks network trained policy gradient method without supervising teacher contains single layer unit long short term memory sees current game state emits actions several possible action heads openai also trained similar lstm policy gradients control human like robot hand manipulates physical objects unprecedented dexterity deepmind program alphastar used deep lstm core excel complex video game starcraft ii viewed significant progress towards artificial general intelligence applications lstm include
https://en.wikipedia.org/wiki/Gated_recurrent_unit,gated recurrent units gating mechanism recurrent neural networks introduced kyunghyun cho et al gru like long short term memory forget gate fewer parameters lstm lacks output gate gru performance certain tasks polyphonic music modeling speech signal modeling natural language processing found similar lstm grus shown exhibit better performance certain smaller less frequent datasets however shown gail weiss yoav goldberg eran yahav lstm strictly stronger gru easily perform unbounded counting gru cannot gru fails learn simple languages learnable lstm similarly shown denny britz anna goldie minh thang luong quoc le google brain lstm cells consistently outperform gru cells first large scale analysis architecture variations neural machine translation several variations full gated unit gating done using previous hidden state bias various combinations simplified form called minimal gated unit operator displaystyle odot denotes hadamard product following initially displaystyle output vector h displaystyle h variables activation functions alternative activation functions possible provided g displaystyle sigma g alternate forms created changing z displaystyle z r displaystyle r minimal gated unit similar fully gated unit except update reset gate vector merged forget gate also implies equation output vector must changed variables
https://en.wikipedia.org/wiki/Echo_state_network,echo state network recurrent neural network sparsely connected hidden layer connectivity weights hidden neurons fixed randomly assigned weights output neurons learned network produce reproduce specific temporal patterns main interest network although behaviour non linear weights modified training synapses connect hidden neurons output neurons thus error function quadratic respect parameter vector differentiated easily linear system alternatively one may consider nonparametric bayesian formulation output layer prior distribution imposed output weights output weights marginalized context prediction generation given training data idea demonstrated using gaussian priors whereby gaussian process model esn driven kernel function obtained solution shown outperform esns trainable sets weights several benchmarks publicly available implementations esns aureservoir efficient c library various kinds echo state networks python numpy bindings matlab code efficient matlab echo state network echo state network belongs recurrent neural network family provide architecture supervised learning principle unlike feedforward neural networks recurrent neural networks dynamic systems functions recurrent neural networks typically used learn dynamical process signal treatment engineering telecommunications vibration analysis seismology control engines generators signal forecasting generation text music electric signals modeling biological systems neurosciences memory modeling brain computer interfaces filtering kalman processes military applications volatility modeling etc training rnn number learning algorithms available backpropagation time real time recurrent learning convergence guaranteed due instability bifurcation phenomena main approach esn firstly operate random large fixed recurring neural network input signal induces nonlinear response signal neuron within reservoir network secondly connect desired output signal trainable linear combination response signals another feature esn autonomous operation prediction echo state network trained input backshifted version output used signal generation prediction using previous output input main idea esns tied liquid state machines independently simultaneously developed esns wolfgang maass lsms esns newly researched backpropagation decorrelation learning rule rnns summarized name reservoir computing schiller steil also demonstrated conventional training approaches rnns weights adapted dominant changes output weights cognitive neuroscience peter f dominey analysed related process related modelling sequence processing mammalian brain particular speech recognition human brain basic idea also included model temporal input discrimination biological neuronal networks early clear formulation reservoir computing idea due k kirby disclosed concept largely forgotten conference contribution first formulation reservoir computing idea known today stems l schomaker described desired target output obtained rnn learning combine signals randomly configured ensemble spiking neural oscillators echo state networks built different ways set without directly trainable input output connections without output reservation feedback different neurotypes different reservoir internal connectivity patterns etc output weight calculated linear regression algorithms whether online offline addition solutions errors smallest squares margin maximization criteria called training support vector machines used determine output values fixed rnn acts random nonlinear medium whose dynamic response echo used signal base linear combination base trained reconstruct desired output minimizing error criteria rnns rarely used practice introduction esn models fit need version gradient descent adjust connections result algorithms slow much worse making learning process vulnerable branching errors convergence cannot therefore guaranteed problem branching esn training additionally easy implement esns outperform nonlinear dynamic models however today problem rnns made slow error prone solved advent deep learning unique selling point esns lost addition rnns proven several practical areas language processing cope tasks similar complexity using reservoir calculation methods would require memory excessive size however used areas many signal processing applications however esns widely used computing principle mixes non digital computer substrates example optical microchips mechanical nanooscillators polymer mixtures even artificial soft limbs
https://en.wikipedia.org/wiki/Restricted_Boltzmann_machine,restricted boltzmann machine generative stochastic artificial neural network learn probability distribution set inputs rbms initially invented name harmonium paul smolensky rose prominence geoffrey hinton collaborators invented fast learning algorithms mid rbms found applications dimensionality reduction classification collaborative filtering feature learning topic modelling even many body quantum mechanics trained either supervised unsupervised ways depending task name implies rbms variant boltzmann machines restriction neurons must form bipartite graph pair nodes two groups units may symmetric connection connections nodes within group contrast unrestricted boltzmann machines may connections hidden units restriction allows efficient training algorithms available general class boltzmann machines particular gradient based contrastive divergence algorithm restricted boltzmann machines also used deep learning networks particular deep belief networks formed stacking rbms optionally fine tuning resulting deep network gradient descent backpropagation standard type rbm binary valued hidden visible units consists matrix weights w displaystyle w associated connection hidden unit h j displaystyle h j visible unit v displaystyle v well bias weights displaystyle visible units b j displaystyle b j hidden units given energy configuration defined matrix notation energy function analogous hopfield network general boltzmann machines probability distributions hidden visible vectors defined terms energy function z displaystyle z partition function defined sum e e displaystyle e e possible configurations similarly probability visible vector booleans sum possible hidden layer configurations since rbm shape bipartite graph intra layer connections hidden unit activations mutually independent given visible unit activations conversely visible unit activations mutually independent given hidden unit activations displaystyle visible units n displaystyle n hidden units conditional probability configuration visible units v given configuration hidden units h conversely conditional probability h given v individual activation probabilities given displaystyle sigma denotes logistic sigmoid visible units restricted boltzmann machine multinomial although hidden units bernoulli case logistic function visible units replaced softmax function k number discrete values visible values applied topic modeling recommender systems restricted boltzmann machines special case boltzmann machines markov random fields graphical model corresponds factor analysis restricted boltzmann machines trained maximize product probabilities assigned training set v displaystyle v equivalently maximize expected log probability training sample v displaystyle v selected randomly v displaystyle v algorithm often used train rbms optimize weight vector w displaystyle w contrastive divergence algorithm due hinton originally developed train poe models algorithm performs gibbs sampling used inside gradient descent procedure compute weight update basic single step contrastive divergence procedure single sample summarized follows practical guide training rbms written hinton found homepage
https://en.wikipedia.org/wiki/Generative_adversarial_network,generative adversarial network class machine learning frameworks designed ian goodfellow colleagues two neural networks contest game given training set technique learns generate new data statistics training set example gan trained photographs generate new photographs look least superficially authentic human observers many realistic characteristics though originally proposed form generative model unsupervised learning gans also proven useful semi supervised learning fully supervised learning reinforcement learning core idea gans based indirect training discriminator also updated dynamically basically means generator trained minimize distance specific image rather fool discriminator enables model learn unsupervised manner generative network generates candidates discriminative network evaluates contest operates terms data distributions typically generative network learns map latent space data distribution interest discriminative network distinguishes candidates produced generator true data distribution generative network training objective increase error rate discriminative network known dataset serves initial training data discriminator training involves presenting samples training dataset achieves acceptable accuracy generator trains based whether succeeds fooling discriminator typically generator seeded randomized input sampled predefined latent space thereafter candidates synthesized generator evaluated discriminator independent backpropagation procedures applied networks generator produces better images discriminator becomes skilled flagging synthetic images generator typically deconvolutional neural network discriminator convolutional neural network gans often suffer mode collapse fail generalize properly missing entire modes input data example gan trained mnist dataset containing many samples digit might nevertheless timidly omit subset digits output researchers perceive root problem weak discriminative network fails notice pattern omission others assign blame bad choice objective function many solutions proposed gan applications increased rapidly gans used generate art verge wrote march images created gans become defining look contemporary ai art gans also used inpaint photographs create photos imaginary fashion models need hire model photographer makeup artist pay studio transportation gans improve astronomical images simulate gravitational lensing dark matter research used successfully model distribution dark matter particular direction space predict gravitational lensing occur gans proposed fast accurate way modeling high energy jet formation modeling showers calorimeters high energy physics experiments gans also trained accurately approximate bottlenecks computationally expensive simulations particle physics experiments applications context present proposed cern experiments demonstrated potential methods accelerating simulation improving simulation fidelity gans reached video game modding community method scaling low resolution textures old video games recreating k higher resolutions via image training sampling fit game native resolution proper training gans provide clearer sharper texture image magnitudes higher quality original fully retaining original level details colors etc known examples extensive gan usage include final fantasy viii final fantasy ix resident evil remake hd remaster max payne citation needed concerns raised potential use gan based human image synthesis sinister purposes e g produce fake possibly incriminating photographs videos gans used generate unique realistic profile photos people exist order automate creation fake social media profiles state california considered passed october bill ab bans use human image synthesis technologies make fake pornography without consent people depicted bill ab prohibits distribution manipulated videos political candidate within days election bills authored assembly member marc berman signed governor gavin newsom laws come effect darpa media forensics program studies ways counteract fake media including fake media produced using gans gan used detect glaucomatous images helping early diagnosis essential avoid partial total loss vision gans produce photorealistic images used visualize interior design industrial design shoes bags clothing items items computer games scenes citation needed networks reported used facebook gans reconstruct models objects images model patterns motion video gans used age face photographs show individual appearance might change age gans also used transfer map styles cartography augment street view imagery relevance feedback gans used generate images replace image search systems variation gans used training network generate optimal control inputs nonlinear dynamical systems discriminatory network known critic checks optimality solution generative network known adaptive network generates optimal control critic adaptive network train approximate nonlinear optimal control gans used visualize effect climate change specific houses gan model called speech face reconstruct image person face listening voice gans used generate new molecules variety protein targets implicated cancer inflammation fibrosis gan generated molecules validated experimentally way mice direct inspiration gans noise contrastive estimation uses loss function gans goodfellow studied phd people similar ideas develop similarly idea involving adversarial networks published blog post olli niemitalo idea never implemented involve stochasticity generator thus generative model known conditional gan cgan idea similar gans used model animal behavior li gauci gross adversarial machine learning uses besides generative modeling applied models neural networks control theory adversarial learning based neural networks used train robust controllers game theoretic sense alternating iterations minimizer policy controller maximizer policy disturbance gan used image enhancement focusing realistic textures rather pixel accuracy producing higher image quality high magnification first faces generated exhibited february grand palais faces generated stylegan drew comparisons deepfakes beginning gan technology began make presence felt fine arts arena appearance newly developed implementation said crossed threshold able generate unique appealing abstract paintings thus dubbed creative adversarial network gan system used create painting edmond de belamy sold us early article members original team discussed progress system gave consideration well overall prospects ai enabled art may researchers samsung demonstrated gan based system produces videos person speaking given single photo person august large dataset consisting midi songs paired lyrics melody alignment created neural melody generation lyrics using conditional gan lstm may nvidia researchers taught ai system recreate game pac man simply watching played bidirectional gan aims introduce generator model act discriminator whereby discriminator naturally considers entire translation space inadequate training problem alleviated satisfy property generator discriminator designed model joint probability sentence pairs difference generator decomposes joint probability source language model source target translation model discriminator formulated target language model target source translation model leverage symmetry auxiliary gan introduced adopts generator discriminator models original one discriminator generator respectively two gans alternately trained update parameters resulting learned feature representation useful auxiliary supervised discrimination tasks competitive contemporary approaches unsupervised self supervised feature learning
https://en.wikipedia.org/wiki/Self-organizing_map,self organizing map self organizing feature map type artificial neural network trained using unsupervised learning produce low dimensional discretized representation input space training samples called map therefore method dimensionality reduction self organizing maps differ artificial neural networks apply competitive learning opposed error correction learning sense use neighborhood function preserve topological properties input space makes soms useful visualization creating low dimensional views high dimensional data akin multidimensional scaling artificial neural network introduced finnish professor teuvo kohonen sometimes called kohonen map network kohonen net computationally convenient abstraction building biological models neural systems morphogenesis models dating back alan turing typical consider type network structure related feedforward networks nodes visualized attached type architecture fundamentally different arrangement motivation useful extensions include using toroidal grids opposite edges connected using large numbers nodes also common use u matrix u matrix value particular node average distance node weight vector closest neighbors square grid instance closest nodes might considered six nodes hexagonal grid large soms display emergent properties maps consisting thousands nodes possible perform cluster operations map like artificial neural networks soms operate two modes training mapping training builds map using input examples mapping automatically classifies new input vector visible part self organizing map map space consists components called nodes neurons map space defined beforehand usually finite two dimensional region nodes arranged regular hexagonal rectangular grid node associated weight vector position input space dimension input vector nodes map space stay fixed training consists moving weight vectors toward input data without spoiling topology induced map space thus self organizing map describes mapping higher dimensional input space lower dimensional map space trained map classify vector input space finding node closest weight vector input space vector goal learning self organizing map cause different parts network respond similarly certain input patterns partly motivated visual auditory sensory information handled separate parts cerebral cortex human brain weights neurons initialized either small random values sampled evenly subspace spanned two largest principal component eigenvectors latter alternative learning much faster initial weights already give good approximation som weights network must fed large number example vectors represent close possible kinds vectors expected mapping examples usually administered several times iterations training utilizes competitive learning training example fed network euclidean distance weight vectors computed neuron whose weight vector similar input called best matching unit weights bmu neurons close som grid adjusted towards input vector magnitude change decreases time grid distance bmu update formula neuron v weight vector wv step index index training sample u index bmu input vector monotonically decreasing learning coefficient neighborhood function gives distance neuron u neuron v step depending implementations scan training data set systematically randomly drawn data set implement sampling method neighborhood function depends grid distance bmu neuron v simplest form neurons close enough bmu others gaussian mexican hat functions common choices regardless functional form neighborhood function shrinks time beginning neighborhood broad self organizing takes place global scale neighborhood shrunk couple neurons weights converging local estimates implementations learning coefficient neighborhood function decrease steadily increasing others decrease step wise fashion every steps process repeated input vector number cycles network winds associating output nodes groups patterns input data set patterns named names attached associated nodes trained net mapping one single winning neuron neuron whose weight vector lies closest input vector simply determined calculating euclidean distance input vector weight vector representing input data vectors emphasized article kind object represented digitally appropriate distance measure associated necessary operations training possible used construct self organizing map includes matrices continuous functions even self organizing maps variables needed vectors bold variant algorithm selection good initial approximation well known problem iterative methods learning neural networks kohonen used random initiation som weights recently principal component initialization initial map weights chosen space first principal components become popular due exact reproducibility results careful comparison random initiation approach principal component initialization one dimensional som demonstrated advantages principal component som initialization universal best initialization method depends geometry specific dataset principal component initialization preferable principal curve approximating dataset univalently linearly projected first principal component nonlinear datasets however random initiation performs better consider n array nodes contains weight vector aware location array weight vector dimension node input vector weights may initially set random values need input feed map colors represented red green blue components consequently represent colors vectors unit cube free vector space generated basis diagram shown compares results training data sets note original images note striking resemblance two similarly training grid neurons iterations learning rate fisher iris map already detect main differences species two ways interpret som training phase weights whole neighborhood moved direction similar items tend excite adjacent neurons therefore som forms semantic map similar samples mapped close together dissimilar ones apart may visualized u matrix som way think neuronal weights pointers input space form discrete approximation distribution training samples neurons point regions high training sample concentration fewer samples scarce som may considered nonlinear generalization principal components analysis shown using artificial real geophysical data som many advantages conventional feature extraction methods empirical orthogonal functions pca originally som formulated solution optimisation problem nevertheless several attempts modify definition som formulate optimisation problem gives similar results example elastic maps use mechanical metaphor elasticity approximate principal manifolds analogy elastic membrane plate
https://en.wikipedia.org/wiki/Convolutional_neural_network,deep learning convolutional neural network class deep neural networks commonly applied analyzing visual imagery also known shift invariant space invariant artificial neural networks based shared weights architecture translation invariance characteristics applications image video recognition recommender systems image classification medical image analysis natural language processing financial time series cnns regularized versions multilayer perceptrons multilayer perceptrons usually mean fully connected networks neuron one layer connected neurons next layer fully connectedness networks makes prone overfitting data typical ways regularization include adding form magnitude measurement weights loss function cnns take different approach towards regularization take advantage hierarchical pattern data assemble complex patterns using smaller simpler patterns therefore scale connectedness complexity cnns lower extreme convolutional networks inspired biological processes connectivity pattern neurons resembles organization animal visual cortex individual cortical neurons respond stimuli restricted region visual field known receptive field receptive fields different neurons partially overlap cover entire visual field cnns use relatively little pre processing compared image classification algorithms means network learns filters traditional algorithms hand engineered independence prior knowledge human effort feature design major advantage name convolutional neural network indicates network employs mathematical operation called convolution convolution specialized kind linear operation convolutional networks simply neural networks use convolution place general matrix multiplication least one layers convolutional neural network consists input output layer well multiple hidden layers hidden layers cnn typically consist series convolutional layers convolve multiplication dot product activation function commonly relu layer subsequently followed additional convolutions pooling layers fully connected layers normalization layers referred hidden layers inputs outputs masked activation function final convolution though layers colloquially referred convolutions convention mathematically technically sliding dot product cross correlation significance indices matrix affects weight determined specific index point programming cnn input tensor shape x x x passing convolutional layer image becomes abstracted feature map shape x x x convolutional layer within neural network following attributes convolutional layers convolve input pass result next layer similar response neuron visual cortex specific stimulus convolutional neuron processes data receptive field although fully connected feedforward neural networks used learn features well classify data practical apply architecture images high number neurons would necessary even shallow architecture due large input sizes associated images pixel relevant variable instance fully connected layer image size x weights neuron second layer convolution operation brings solution problem reduces number free parameters allowing network deeper fewer parameters instance regardless image size tiling regions size x shared weights requires learnable parameters using regularized weights fewer parameters vanishing gradient exploding gradient problems seen backpropagation traditional neural networks avoided convolutional networks may include local global pooling layers streamline underlying computation pooling layers reduce dimensions data combining outputs neuron clusters one layer single neuron next layer local pooling combines small clusters typically x global pooling acts neurons convolutional layer addition pooling may compute max average max pooling uses maximum value cluster neurons prior layer average pooling uses average value cluster neurons prior layer fully connected layers connect every neuron one layer every neuron another layer principle traditional multi layer perceptron neural network flattened matrix goes fully connected layer classify images neural networks neuron receives input number locations previous layer fully connected layer neuron receives input every element previous layer convolutional layer neurons receive input restricted subarea previous layer typically subarea square shape input area neuron called receptive field fully connected layer receptive field entire previous layer convolutional layer receptive area smaller entire previous layer subarea original input image receptive field increasingly growing getting deeper network architecture due applying convolution takes account value specific pixel also surrounding pixels neuron neural network computes output value applying specific function input values coming receptive field previous layer function applied input values determined vector weights bias learning neural network progresses making iterative adjustments biases weights vector weights bias called filters represent particular features input distinguishing feature cnns many neurons share filter reduces memory footprint single bias single vector weights used across receptive fields sharing filter opposed receptive field bias vector weighting cnn design follows vision processing living organisms citation needed work hubel wiesel showed cat monkey visual cortexes contain neurons individually respond small regions visual field provided eyes moving region visual space within visual stimuli affect firing single neuron known receptive field neighboring cells similar overlapping receptive fields citation needed receptive field size location varies systematically across cortex form complete map visual space citation needed cortex hemisphere represents contralateral visual field citation needed paper identified two basic visual cell types brain hubel wiesel also proposed cascading model two types cells use pattern recognition tasks neocognitron introduced kunihiko fukushima inspired mentioned work hubel wiesel neocognitron introduced two basic types layers cnns convolutional layers downsampling layers convolutional layer contains units whose receptive fields cover patch previous layer weight vector unit often called filter units share filters downsampling layers contain units whose receptive fields cover patches previous convolutional layers unit typically computes average activations units patch downsampling helps correctly classify objects visual scenes even objects shifted variant neocognitron called cresceptron instead using fukushima spatial averaging j weng et al introduced method called max pooling downsampling unit computes maximum activations units patch max pooling often used modern cnns several supervised unsupervised learning algorithms proposed decades train weights neocognitron today however cnn architecture usually trained backpropagation neocognitron first cnn requires units located multiple network positions shared weights neocognitrons adapted analyze time varying signals time delay neural network introduced alex waibel et al first convolutional network achieved shift invariance utilizing weight sharing combination backpropagation training thus also using pyramidal structure neocognitron performed global optimization weights instead local one tdnns convolutional networks share weights along temporal dimension allow speech signals processed time invariantly hampshire waibel introduced variant performs two dimensional convolution since tdnns operated spectrograms resulting phoneme recognition system invariant shifts time frequency inspired translation invariance image processing cnns tiling neuron outputs cover timed stages tdnns achieve best performance far distance speech recognition yamaguchi et al introduced concept max pooling combining tdnns max pooling order realize speaker independent isolated word recognition system system used several tdnns per word one syllable results tdnn input signal combined using max pooling outputs pooling layers passed networks performing actual word classification system recognize hand written zip code numbers involved convolutions kernel coefficients laboriously hand designed yann lecun et al used back propagation learn convolution kernel coefficients directly images hand written numbers learning thus fully automatic performed better manual coefficient design suited broader range image recognition problems image types approach became foundation modern computer vision lenet pioneering level convolutional network lecun et al classifies digits applied several banks recognize hand written numbers checks digitized x pixel images ability process higher resolution images requires larger layers convolutional neural networks technique constrained availability computing resources similarly shift invariant neural network proposed w zhang et al image character recognition architecture training algorithm modified applied medical image processing automatic detection breast cancer mammograms different convolution based design proposed application decomposition one dimensional electromyography convolved signals via de convolution design modified de convolution based designs feed forward architecture convolutional neural networks extended neural abstraction pyramid lateral feedback connections resulting recurrent convolutional network allows flexible incorporation contextual information iteratively resolve local ambiguities contrast previous models image like outputs highest resolution generated e g semantic segmentation image reconstruction object localization tasks although cnns invented breakthrough required fast implementations graphics processing units shown k oh k jung standard neural networks greatly accelerated gpus implementation times faster equivalent implementation cpu another paper also emphasised value gpgpu machine learning first gpu implementation cnn described k chellapilla et al implementation times faster equivalent implementation cpu subsequent work also used gpus initially types neural networks especially unsupervised neural networks dan ciresan et al idsia showed even deep standard neural networks many layers quickly trained gpu supervised learning old method known backpropagation network outperformed previous machine learning methods mnist handwritten digits benchmark extended gpu approach cnns achieving acceleration factor impressive results used cnns gpu win image recognition contest achieved superhuman performance first time may september cnns less four image competitions also significantly improved best performance literature multiple image databases including mnist database norb database hwdb dataset cifar dataset subsequently similar gpu based cnn alex krizhevsky et al imagenet large scale visual recognition challenge deep cnn layers microsoft imagenet contest compared training cnns using gpus much attention given intel xeon phi coprocessor notable development parallelization method training convolutional neural networks intel xeon phi named controlled hogwild arbitrary order synchronization chaos exploits thread simd level parallelism available intel xeon phi past traditional multilayer perceptron models used image recognition example needed however due full connectivity nodes suffered curse dimensionality scale well higher resolution images pixel image rgb color channels million weights high feasibly process efficiently scale full connectivity example cifar images size single fully connected neuron first hidden layer regular neural network would weights image however would lead neurons weights also network architecture take account spatial structure data treating input pixels far apart way pixels close together ignores locality reference image data computationally semantically thus full connectivity neurons wasteful purposes image recognition dominated spatially local input patterns convolutional neural networks biologically inspired variants multilayer perceptrons designed emulate behavior visual cortex models mitigate challenges posed mlp architecture exploiting strong spatially local correlation present natural images opposed mlps cnns following distinguishing features together properties allow cnns achieve better generalization vision problems weight sharing dramatically reduces number free parameters learned thus lowering memory requirements running network allowing training larger powerful networks cnn architecture formed stack distinct layers transform input volume output volume differentiable function distinct types layers commonly used discussed convolutional layer core building block cnn layer parameters consist set learnable filters small receptive field extend full depth input volume forward pass filter convolved across width height input volume computing dot product entries filter input producing dimensional activation map filter result network learns filters activate detects specific type feature spatial position input nb stacking activation maps filters along depth dimension forms full output volume convolution layer every entry output volume thus also interpreted output neuron looks small region input shares parameters neurons activation map dealing high dimensional inputs images impractical connect neurons neurons previous volume network architecture take spatial structure data account convolutional networks exploit spatially local correlation enforcing sparse local connectivity pattern neurons adjacent layers neuron connected small region input volume extent connectivity hyperparameter called receptive field neuron connections local space always extend along entire depth input volume architecture ensures learnt filters produce strongest response spatially local input pattern three hyperparameters control size output volume convolutional layer depth stride zero padding spatial size output volume computed function input volume size w displaystyle w kernel field size convolutional layer neurons k displaystyle k stride applied displaystyle amount zero padding p displaystyle p used border formula calculating many neurons fit given volume given w k p displaystyle frac w k p number integer strides incorrect neurons cannot tiled fit across input volume symmetric way general setting zero padding p textstyle p stride displaystyle ensures input volume output volume size spatially however always completely necessary use neurons previous layer example neural network designer may decide use portion padding parameter sharing scheme used convolutional layers control number free parameters relies assumption patch feature useful compute spatial position also useful compute positions denoting single dimensional slice depth depth slice neurons depth slice constrained use weights bias since neurons single depth slice share parameters forward pass depth slice convolutional layer computed convolution neuron weights input volume nb therefore common refer sets weights filter convolved input result convolution activation map set activation maps different filter stacked together along depth dimension produce output volume parameter sharing contributes translation invariance cnn architecture sometimes parameter sharing assumption may make sense especially case input images cnn specific centered structure expect completely different features learned different spatial locations one practical example inputs faces centered image might expect different eye specific hair specific features learned different parts image case common relax parameter sharing scheme instead simply call layer locally connected layer another important concept cnns pooling form non linear sampling several non linear functions implement pooling among max pooling common partitions input image set non overlapping rectangles sub region outputs maximum intuitively exact location feature less important rough location relative features idea behind use pooling convolutional neural networks pooling layer serves progressively reduce spatial size representation reduce number parameters memory footprint amount computation network hence also control overfitting common periodically insert pooling layer successive convolutional layers cnn architecture pooling operation used another form translation invariance pooling layer operates independently every depth slice input resizes spatially common form pooling layer filters size applied stride downsamples every depth slice input along width height discarding activations f x max b x b displaystyle f x max b x b case every max operation numbers depth dimension remains unchanged addition max pooling pooling units use functions average pooling norm pooling average pooling often used historically recently fallen favor compared max pooling performs better practice due aggressive reduction size representation recent trend towards using smaller filters discarding pooling layers altogether region interest pooling variant max pooling output size fixed input rectangle parameter pooling important component convolutional neural networks object detection based fast r cnn architecture relu abbreviation rectified linear unit applies non saturating activation function f max textstyle f max effectively removes negative values activation map setting zero increases nonlinear properties decision function overall network without affecting receptive fields convolution layer functions also used increase nonlinearity example saturating hyperbolic tangent f tanh displaystyle f tanh f tanh displaystyle f tanh sigmoid function textstyle sigma relu often preferred functions trains neural network several times faster without significant penalty generalization accuracy finally several convolutional max pooling layers high level reasoning neural network done via fully connected layers neurons fully connected layer connections activations previous layer seen regular artificial neural networks activations thus computed affine transformation matrix multiplication followed bias offset reference needed loss layer specifies training penalizes deviation predicted true labels normally final layer neural network various loss functions appropriate different tasks may used softmax loss used predicting single class k mutually exclusive classes nb sigmoid cross entropy loss used predicting k independent probability values displaystyle euclidean loss used regressing real valued labels displaystyle cnns use hyperparameters standard multilayer perceptron usual rules learning rates regularization constants still apply following kept mind optimizing since feature map size decreases depth layers near input layer tend fewer filters higher layers equalize computation layer product feature values va pixel position kept roughly constant across layers preserving information input would require keeping total number activations non decreasing one layer next number feature maps directly controls capacity depends number available examples task complexity common filter shapes found literature vary greatly usually chosen based dataset challenge thus find right level granularity create abstractions proper scale given particular dataset without overfitting typical values large input volumes may warrant pooling lower layers however choosing larger shapes dramatically reduce dimension signal may result excess information loss often non overlapping pooling windows perform best regularization process introducing additional information solve ill posed problem prevent overfitting cnns use various types regularization fully connected layer occupies parameters prone overfitting one method reduce overfitting dropout training stage individual nodes either dropped net probability p displaystyle p kept probability p displaystyle p reduced network left incoming outgoing edges dropped node also removed reduced network trained data stage removed nodes reinserted network original weights training stages probability hidden node dropped usually input nodes however probability typically much lower since information directly lost input nodes ignored dropped testing time training finished would ideally like find sample average possible n displaystyle n dropped networks unfortunately unfeasible large values n displaystyle n however find approximation using full network node output weighted factor p displaystyle p expected value output node training stages biggest contribution dropout method although effectively generates n displaystyle n neural nets allows model combination test time single network needs tested avoiding training nodes training data dropout decreases overfitting method also significantly improves training speed makes model combination practical even deep neural networks technique seems reduce node interactions leading learn robust features clarification needed better generalize new data dropconnect generalization dropout connection rather output unit dropped probability p displaystyle p unit thus receives input random subset units previous layer dropconnect similar dropout introduces dynamic sparsity within model differs sparsity weights rather output vectors layer words fully connected layer dropconnect becomes sparsely connected layer connections chosen random training stage major drawback dropout benefits convolutional layers neurons fully connected stochastic pooling conventional deterministic pooling operations replaced stochastic procedure activation within pooling region picked randomly according multinomial distribution given activities within pooling region approach free hyperparameters combined regularization approaches dropout data augmentation alternate view stochastic pooling equivalent standard max pooling many copies input image small local deformations similar explicit elastic deformations input images delivers excellent performance mnist data set using stochastic pooling multilayer model gives exponential number deformations since selections higher layers independent since degree model overfitting determined power amount training receives providing convolutional network training examples reduce overfitting since networks usually trained available data one approach either generate new data scratch perturb existing data create new ones example input images could asymmetrically cropped percent create new examples label original one simplest methods prevent overfitting network simply stop training overfitting chance occur comes disadvantage learning process halted another simple way prevent overfitting limit number parameters typically limiting number hidden units layer limiting network depth convolutional networks filter size also affects number parameters limiting number parameters restricts predictive power network directly reducing complexity function perform data thus limits amount overfitting equivalent zero norm simple form added regularizer weight decay simply adds additional error proportional sum weights squared magnitude weight vector error node level acceptable model complexity reduced increasing proportionality constant thus increasing penalty large weight vectors l regularization common form regularization implemented penalizing squared magnitude parameters directly objective l regularization intuitive interpretation heavily penalizing peaky weight vectors preferring diffuse weight vectors due multiplicative interactions weights inputs useful property encouraging network use inputs little rather inputs lot l regularization another common form possible combine l l regularization l regularization leads weight vectors become sparse optimization words neurons l regularization end using sparse subset important inputs become nearly invariant noisy inputs another form regularization enforce absolute upper bound magnitude weight vector every neuron use projected gradient descent enforce constraint practice corresponds performing parameter update normal enforcing constraint clamping weight vector w displaystyle vec w every neuron satisfy w c displaystyle vec w
https://en.wikipedia.org/wiki/U-Net,u net convolutional neural network developed biomedical image segmentation computer science department university freiburg germany network based fully convolutional network architecture modified extended work fewer training images yield precise segmentations segmentation image takes less second modern gpu u net architecture stems called fully convolutional network first proposed long shelhamer main idea supplement usual contracting network successive layers pooling operations replaced upsampling operators hence layers increase resolution output successive convolutional layer learn assemble precise output based information one important modification u net large number feature channels upsampling part allow network propagate context information higher resolution layers consequence expansive path less symmetric contracting part yields u shaped architecture network uses valid part convolution without fully connected layers predict pixels border region image missing context extrapolated mirroring input image tiling strategy important apply network large images since otherwise resolution would limited gpu memory u net created olaf ronneberger philipp fischer thomas brox paper u net convolutional networks biomedical image segmentation improvement development fcn evan shelhamer jonathan long trevor darrell fully convolutional networks semantic segmentation network consists contracting path expansive path gives u shaped architecture contracting path typical convolutional network consists repeated application convolutions followed rectified linear unit max pooling operation contraction spatial information reduced feature information increased expansive pathway combines feature spatial information sequence convolutions concatenations high resolution features contracting path many applications u net biomedical image segmentation brain image segmentation liver image segmentation variations u net also applied medical image reconstruction variants applications u net follows jakeret tensorflow unet u net source code pattern recognition image processing computer science department university freiburg germany basic articles system cited times respectively google scholar december
https://en.wikipedia.org/wiki/Transformer_(machine_learning_model),transformer deep learning model introduced used primarily field natural language processing like recurrent neural networks transformers designed handle sequential data natural language tasks translation text summarization however unlike rnns transformers require sequential data processed order example input data natural language sentence transformer need process beginning end due feature transformer allows much parallelization rnns therefore reduced training times since introduction transformers become model choice tackling many problems nlp replacing older recurrent neural network models long short term memory since transformer model facilitates parallelization training enabled training larger datasets possible introduced led development pretrained systems bert gpt trained huge general language datasets fine tuned specific language tasks introduction transformers state art nlp systems relied gated recurrent neural networks lstms gated recurrent units added attention mechanisms transformer built attention technologies without using rnn structure highlighting fact attention mechanisms alone without recurrent sequential processing powerful enough achieve performance rnns attention gated rnns process tokens sequentially maintaining state vector contains representation data seen every token process n h textstyle n th token model combines state representing sentence token n textstyle n information new token create new state representing sentence token n textstyle n theoretically information one token propagate arbitrarily far sequence every point state continues encode information token practice mechanism imperfect due part vanishing gradient problem model state end long sentence often contain precise extractable information early tokens problem addressed introduction attention mechanisms attention mechanisms let model directly look draw state earlier point sentence attention layer access previous states weighs according learned measure relevancy current token providing sharper information far away relevant tokens clear example utility attention translation english french translation system first word french output probably depends heavily beginning english input however classic encoder decoder lstm model order produce first word french output model given state vector last english word theoretically vector encode information whole english sentence giving model necessary knowledge practice information often well preserved attention mechanism introduced model instead learn attend states early english tokens producing beginning french output giving much better concept translating added rnns attention mechanisms led large gains performance introduction transformer brought light fact attention mechanisms powerful sequential recurrent processing data necessary achieving performance gains rnns attention transformer uses attention mechanism without rnn processing tokens time calculating attention weights fact transformers rely sequential processing lend easily parallelization allows transformers trained efficiently larger datasets like models invented transformer encoder decoder architecture encoder consists set encoding layers processes input iteratively one layer another decoder consists set decoding layers thing output encoder function encoder layer process input generate encodings containing information parts inputs relevant passes set encodings next encoder layer inputs decoder layer opposite taking encodings processes using incorporated contextual information generate output sequence achieve encoder decoder layer makes use attention mechanism input weighs relevance every input draws information accordingly produce output decoder layer also additional attention mechanism draws information outputs previous decoders decoder layer draws information encodings encoder decoder layers feed forward neural network additional processing outputs contain residual connections layer normalization steps basic building blocks transformer scaled dot product attention units sentence passed transformer model attention weights calculated every token simultaneously attention unit produces embeddings every token context contain information token also weighted combination relevant tokens weighted attention weights concretely attention unit transformer model learns three weight matrices query weights w q displaystyle w q key weights w k displaystyle w k value weights w v displaystyle w v token displaystyle input word embedding x displaystyle x multiplied three weight matrices produce query vector q x w q displaystyle q x w q key vector k x w k displaystyle k x w k value vector v x w v displaystyle v x w v attention weights calculated using query key vectors attention weight j displaystyle ij token displaystyle token j displaystyle j dot product q displaystyle q k j displaystyle k j attention weights divided square root dimension key vectors k displaystyle sqrt k stabilizes gradients training passed softmax normalizes weights sum displaystyle fact w q displaystyle w q w k displaystyle w k different matrices allows attention non symmetric token displaystyle attends token j displaystyle j necessarily mean token j displaystyle j attend token displaystyle output attention unit token displaystyle weighted sum value vectors tokens weighted j displaystyle ij attention displaystyle token attention calculation tokens expressed one large matrix calculation useful training due computational matrix operation optimizations make matrix operations fast compute matrices q displaystyle q k displaystyle k v displaystyle v defined matrices displaystyle th rows vectors q displaystyle q k displaystyle k v displaystyle v respectively attention softmax v displaystyle begin aligned text attention text softmax leftv end aligned one set displaystyle left matrices called attention head layer transformer model multiple attention heads one attention head attends tokens relevant token multiple attention heads model learn different definitions relevance research shown many attention heads transformers encode relevance relations interpretable humans example attention heads every token attend mostly next word attention heads mainly attend verbs direct objects since transformer models multiple attention heads possibility capturing many levels types relevance relations surface level semantic multiple outputs multi head attention layer concatenated pass feed forward neural network layers encoder consists two major components self attention mechanism feed forward neural network self attention mechanism takes set input encodings previous encoder weighs relevance generate set output encodings feed forward neural network processes output encoding individually output encodings finally passed next encoder input well decoders first encoder takes positional information embeddings input sequence input rather encodings positional information necessary transformer make use order sequence part transformer makes use decoder consists three major components self attention mechanism attention mechanism encodings feed forward neural network decoder functions similar fashion encoder additional attention mechanism inserted instead draws relevant information encodings generated encoders like first encoder first decoder takes positional information embeddings output sequence input rather encodings since transformer use current future output predict output though output sequence must partially masked prevent reverse information flow last decoder followed final linear transformation softmax layer produce output probabilities vocabulary transformers typically undergo semi supervised learning involving unsupervised pretraining followed supervised fine tuning pretraining typically done much larger dataset fine tuning due restricted availability labeled training data tasks pretraining fine tuning commonly include transformer model implemented major deep learning frameworks tensorflow pytorch pseudo code implementation transformer variant known vanilla transformer transformer finds applications field natural language processing example tasks machine translation time series prediction many pretrained models gpt gpt bert xlnet roberta demonstrate ability transformers perform wide variety nlp related tasks potential find real world applications may include shown transformer architecture specifically gpt could fine tuned play chess
https://en.wikipedia.org/wiki/Reinforcement_learning,reinforcement learning area machine learning concerned software agents ought take actions environment order maximize notion cumulative reward reinforcement learning one three basic machine learning paradigms alongside supervised learning unsupervised learning reinforcement learning differs supervised learning needing labelled input output pairs presented needing sub optimal actions explicitly corrected instead focus finding balance exploration exploitation environment typically stated form markov decision process many reinforcement learning algorithms context utilize dynamic programming techniques main difference classical dynamic programming methods reinforcement learning algorithms latter assume knowledge exact mathematical model mdp target large mdps exact methods become infeasible mw parser output toclimit toclevel ul mw parser output toclimit toclevel ul mw parser output toclimit toclevel ul mw parser output toclimit toclevel ul mw parser output toclimit toclevel ul mw parser output toclimit toclevel ul display none due generality reinforcement learning studied many disciplines game theory control theory operations research information theory simulation based optimization multi agent systems swarm intelligence statistics operations research control literature reinforcement learning called approximate dynamic programming neuro dynamic programming problems interest reinforcement learning also studied theory optimal control concerned mostly existence characterization optimal solutions algorithms exact computation less learning approximation particularly absence mathematical model environment economics game theory reinforcement learning may used explain equilibrium may arise bounded rationality basic reinforcement modeled markov decision process reinforcement learning agent interacts environment discrete time steps time agent receives current state displaystyle reward r displaystyle r chooses action displaystyle set available actions subsequently sent environment environment moves new state displaystyle reward r displaystyle r associated transition displaystyle determined goal reinforcement learning agent learn policy displaystyle pi times rightarrow pr displaystyle pi pr maximizes expected cumulative reward formulating problem mdp assumes agent directly observes current environmental state case problem said full observability agent access subset states observed states corrupted noise agent said partial observability formally problem must formulated partially observable markov decision process cases set actions available agent restricted example state account balance could restricted positive current value state state transition attempts reduce value transition allowed agent performance compared agent acts optimally difference performance gives rise notion regret order act near optimally agent must reason long term consequences actions although immediate reward associated might negative thus reinforcement learning particularly well suited problems include long term versus short term reward trade applied successfully various problems including robot control elevator scheduling telecommunications backgammon checkers go two elements make reinforcement learning powerful use samples optimize performance use function approximation deal large environments thanks two key components reinforcement learning used large environments following situations first two problems could considered planning problems last one could considered genuine learning problem however reinforcement learning converts planning problems machine learning problems exploration vs exploitation trade thoroughly studied multi armed bandit problem finite state space mdps burnetas katehakis reinforcement learning requires clever exploration mechanisms randomly selecting actions without reference estimated probability distribution shows poor performance case finite markov decision processes relatively well understood however due lack algorithms scale well number states simple exploration methods practical one method displaystyle varepsilon greedy displaystyle
https://en.wikipedia.org/wiki/Q-learning,q learning model free reinforcement learning algorithm learn quality actions telling agent action take circumstances require model environment handle problems stochastic transitions rewards without requiring adaptations finite markov decision process q learning finds optimal policy sense maximizing expected value total reward successive steps starting current state q learning identify optimal action selection policy given fmdp given infinite exploration time partly random policy q names function returns reward used provide reinforcement said stand quality action taken given state reinforcement learning involves agent set states displaystyle set displaystyle actions per state performing action displaystyle agent transitions state state executing action specific state provides agent reward goal agent maximize total reward adding maximum reward attainable future states reward achieving current state effectively influencing current action potential future reward potential reward weighted sum expected values rewards future steps starting current state example consider process boarding train reward measured negative total time spent boarding one strategy enter train door soon open minimizing initial wait time train crowded however slow entry initial action entering door people fighting depart train attempt board total boarding time cost next day random chance decide wait let people depart first initially results longer wait time however time fighting passengers less overall path higher reward previous day since total boarding time exploration despite initial action resulting larger cost forceful strategy overall cost lower thus revealing rewarding strategy displaystyle delta steps future agent decide next step weight step calculated displaystyle gamma delta displaystyle gamma number effect valuing rewards received earlier higher received later displaystyle gamma may also interpreted probability succeed every step displaystyle delta algorithm therefore function calculates quality state action combination learning begins q displaystyle q initialized possibly arbitrary fixed value time displaystyle agent selects action displaystyle observes reward r displaystyle r enters new state displaystyle q displaystyle q updated core algorithm bellman equation simple value iteration update using weighted average old value new information r displaystyle r reward received moving state displaystyle state displaystyle displaystyle alpha learning rate displaystyle
https://en.wikipedia.org/wiki/State%E2%80%93action%E2%80%93reward%E2%80%93state%E2%80%93action,state action reward state action algorithm learning markov decision process policy used reinforcement learning area machine learning proposed rummery niranjan technical note name modified connectionist q learning alternative name sarsa proposed rich sutton mentioned footnote name simply reflects fact main function updating q value depends current state agent action agent chooses reward r agent gets choosing action state agent enters taking action finally next action agent chooses new state acronym quintuple sarsa authors use slightly different convention write quintuple depending time step reward formally assigned rest article uses former convention sarsa agent interacts environment updates policy based actions taken hence known policy learning algorithm q value state action updated error adjusted learning rate alpha q values represent possible reward received next time step taking action state plus discounted future reward received next state action observation watkin q learning updates estimate optimal state action value function q displaystyle q based maximum reward available actions sarsa learns q values associated taking policy follows watkin q learning learns q values associated taking optimal policy following exploration exploitation policy optimizations watkin q learning may applied sarsa learning rate determines extent newly acquired information overrides old information factor make agent learn anything factor would make agent consider recent information discount factor determines importance future rewards factor makes agent opportunistic considering current rewards factor approaching make strive long term high reward discount factor meets exceeds q displaystyle q values may diverge since sarsa iterative algorithm implicitly assumes initial condition first update occurs low initial value also known optimistic initial conditions encourage exploration matter action takes place update rule causes higher values alternative thus increasing choice probability suggested first reward r could used reset initial conditions according idea first time action taken reward used set value q allows immediate learning case fixed deterministic rewards resetting initial conditions approach seems consistent human behavior repeated binary choice experiments
https://en.wikipedia.org/wiki/Temporal_difference_learning,temporal difference learning refers class model free reinforcement learning methods learn bootstrapping current estimate value function methods sample environment like monte carlo methods perform updates based current estimates like dynamic programming methods monte carlo methods adjust estimates final outcome known td methods adjust predictions match later accurate predictions future final outcome known form bootstrapping illustrated following example suppose wish predict weather saturday model predicts saturday weather given weather day week standard case would wait saturday adjust models however example friday pretty good idea weather would saturday thus able change say saturday model saturday arrives temporal difference methods related temporal difference model animal learning tabular td method one simplest td methods special case general stochastic approximation methods estimates state value function finite state markov decision process policy displaystyle pi let v displaystyle v pi denote state value function mdp states n displaystyle mathbb n rewards n displaystyle mathbb n discount rate displaystyle gamma policy displaystyle pi drop action notion convenience v displaystyle v pi satisfies hamilton jacobi bellman equation r v displaystyle r gamma v pi unbiased estimate v displaystyle v pi observation motivates following algorithm estimating v displaystyle v pi algorithm starts initializing table v displaystyle v arbitrarily one value state mdp positive learning rate displaystyle alpha chosen repeatedly evaluate policy displaystyle pi obtain reward r displaystyle r update value function old state using rule displaystyle displaystyle old new states respectively value r v displaystyle r gamma v known td target td lambda learning algorithm invented richard sutton based earlier work temporal difference learning arthur samuel algorithm famously applied gerald tesauro create td gammon program learned play game backgammon level expert human players lambda parameter refers trace decay parameter displaystyle leqslant lambda leqslant higher settings lead longer lasting traces larger proportion credit reward given distant states actions displaystyle lambda higher displaystyle lambda producing parallel learning monte carlo rl algorithms td algorithm also received attention field neuroscience researchers discovered firing rate dopamine neurons ventral tegmental area substantia nigra appear mimic error function algorithm error function reports back difference estimated reward given state time step actual reward received larger error function larger difference expected actual reward paired stimulus accurately reflects future reward error used associate stimulus future reward dopamine cells appear behave similar manner one experiment measurements dopamine cells made training monkey associate stimulus reward juice initially dopamine cells increased firing rates monkey received juice indicating difference expected actual rewards time increase firing back propagated earliest reliable stimulus reward monkey fully trained increase firing rate upon presentation predicted reward continually firing rate dopamine cells decreased normal activation expected reward produced mimics closely error function td used reinforcement learning relationship model potential neurological function produced research attempting use td explain many aspects behavioral research also used study conditions schizophrenia consequences pharmacological manipulations dopamine learning
https://en.wikipedia.org/wiki/Bias%E2%80%93variance_dilemma,statistics machine learning bias variance tradeoff property model variance parameter estimates across samples reduced increasing bias estimated parameters bias variance dilemma bias variance problem conflict trying simultaneously minimize two sources error prevent supervised learning algorithms generalizing beyond training set trade universal shown model asymptotically unbiased must unbounded variance bias variance decomposition way analyzing learning algorithm expected generalization error respect particular problem sum three terms bias variance quantity called irreducible error resulting noise problem bias variance tradeoff central problem supervised learning ideally one wants choose model accurately captures regularities training data also generalizes well unseen data unfortunately typically impossible simultaneously high variance learning methods may able represent training set well risk overfitting noisy unrepresentative training data contrast algorithms high bias typically produce simpler models tend overfit may underfit training data failing capture important regularities often made fallacy assume complex models must high variance high variance models complex sense reverse needs true addition one careful define complexity particular number parameters used describe model poor measure complexity illustrated example adapted model f b sin displaystyle f b sin two parameters interpolate number points oscillating high enough frequency resulting high bias high variance intuitively bias reduced using local information whereas variance reduced averaging multiple observations inherently means using information larger region enlightening example see section k nearest neighbors figure right balance much information used neighboring observations model smoothed via explicit regularization shrinkage suppose training set consisting set points x x n displaystyle x dots x n real values displaystyle associated point x displaystyle x assume function noise f displaystyle f varepsilon noise displaystyle varepsilon zero mean variance displaystyle sigma want find function f displaystyle hat f approximates true function f displaystyle f well possible means learning algorithm based training dataset displaystyle dots make well possible precise measuring mean squared error displaystyle f displaystyle hat f want displaystyle minimal x x n displaystyle x dots x n points outside sample course cannot hope perfectly since displaystyle contain noise displaystyle varepsilon means must prepared accept irreducible error function come finding f displaystyle hat f generalizes points outside training set done countless algorithms used supervised learning turns whichever function f displaystyle hat f select decompose expected error unseen sample x displaystyle x follows expectation ranges different choices training set displaystyle dots sampled joint distribution p displaystyle p three terms represent since three terms non negative forms lower bound expected error unseen samples complex model f displaystyle hat f data points capture lower bias however complexity make model move capture data points hence variance larger derivation bias variance decomposition squared error proceeds follows notational convenience abbreviate f f displaystyle f f f f displaystyle hat f hat f drop displaystyle subscript expectation operators first recall definition random variable x displaystyle x rearranging get since f displaystyle f deterministic e independent displaystyle thus given f displaystyle f varepsilon e displaystyle operatorname e varepsilon implies e e f e f f displaystyle operatorname e operatorname e f varepsilon operatorname e f f also since var displaystyle operatorname var varepsilon sigma thus since displaystyle varepsilon f displaystyle hat f independent write finally mse loss function obtained taking expectation value x p displaystyle x sim p dimensionality reduction feature selection decrease variance simplifying models similarly larger training set tends decrease variance adding features tends decrease bias expense introducing additional variance learning algorithms typically tunable parameters control bias variance example one way resolving trade use mixture models ensemble learning example boosting combines many weak models ensemble lower bias individual models bagging combines strong learners way reduces variance model validation methods cross validation used tune models optimize trade case k nearest neighbors regression expectation taken possible labeling fixed training set closed form expression exists relates bias variance decomposition parameter k n n k displaystyle n dots n k k nearest neighbors x training set bias monotone rising function k variance drops k increased fact reasonable assumptions bias first nearest neighbor estimator vanishes entirely size training set approaches infinity bias variance decomposition forms conceptual basis regression regularization methods lasso ridge regression regularization methods introduce bias regression solution reduce variance considerably relative ordinary least squares solution although ols solution provides non biased regression estimates lower variance solutions produced regularization techniques provide superior mse performance bias variance decomposition originally formulated least squares regression case classification loss possible find similar decomposition alternatively classification problem phrased probabilistic classification expected squared error predicted probabilities respect true probabilities decomposed even though bias variance decomposition directly apply reinforcement learning similar tradeoff also characterize generalization agent limited information environment suboptimality rl algorithm decomposed sum two terms term related asymptotic bias term due overfitting asymptotic bias directly related learning algorithm overfitting term comes fact amount data limited widely discussed context machine learning bias variance dilemma examined context human cognition notably gerd gigerenzer co workers context learned heuristics argued human brain resolves dilemma case typically sparse poorly characterised training sets provided experience adopting high bias low variance heuristics reflects fact zero bias approach poor generalisability new situations also unreasonably presumes precise knowledge true state world resulting heuristics relatively simple produce better inferences wider variety situations geman et al argue bias variance dilemma implies abilities generic object recognition cannot learned scratch require certain degree hard wiring later tuned experience model free approaches inference require impractically large training sets avoid high variance
https://en.wikipedia.org/wiki/Computational_learning_theory,computer science computational learning theory subfield artificial intelligence devoted studying design analysis machine learning algorithms theoretical results machine learning mainly deal type inductive learning called supervised learning supervised learning algorithm given samples labeled useful way example samples might descriptions mushrooms labels could whether mushrooms edible algorithm takes previously labeled samples uses induce classifier classifier function assigns labels samples including samples seen previously algorithm goal supervised learning algorithm optimize measure performance minimizing number mistakes made new samples addition performance bounds computational learning theory studies time complexity feasibility learning citation needed computational learning theory computation considered feasible done polynomial time citation needed two kinds time complexity results negative results often rely commonly believed yet unproven assumptions citation needed several different approaches computational learning theory based making different assumptions inference principles used generalize limited data includes different definitions probability different assumptions generation samples citation needed different approaches include citation needed primary goal understand learning abstractly computational learning theory led development practical algorithms example pac theory inspired boosting vc theory led support vector machines bayesian inference led belief networks description publications given important publications machine learning
https://en.wikipedia.org/wiki/Empirical_risk_minimization,empirical risk minimization principle statistical learning theory defines family learning algorithms used give theoretical bounds performance core idea cannot know exactly well algorithm work practice know true distribution data algorithm work instead measure performance known set training data consider following situation general setting many supervised learning problems two spaces objects x displaystyle x displaystyle would like learn function h x displaystyle h x outputs object displaystyle given x x displaystyle x x disposal training set n displaystyle n examples displaystyle ldots x x displaystyle x x input displaystyle corresponding response wish get h displaystyle h put formally assume joint probability distribution p displaystyle p x displaystyle x displaystyle training set consists n displaystyle n instances displaystyle ldots drawn p displaystyle p note assumption joint probability distribution allows us model uncertainty predictions displaystyle deterministic function x displaystyle x rather random variable conditional distribution p displaystyle p fixed x displaystyle x also assume given non negative real valued loss function l displaystyle l measures different prediction displaystyle hat hypothesis true outcome displaystyle risk associated hypothesis h displaystyle h defined expectation loss function loss function commonly used theory loss function l displaystyle l begin cases mbox quad hat neq mbox quad hat end cases ultimate goal learning algorithm find hypothesis h displaystyle h among fixed class functions h displaystyle mathcal h risk r displaystyle r minimal general risk r displaystyle r cannot computed distribution p displaystyle p unknown learning algorithm however compute approximation called empirical risk averaging loss function training set empirical risk minimization principle states learning algorithm choose hypothesis h displaystyle hat h minimizes empirical risk thus learning algorithm defined erm principle consists solving optimization problem empirical risk minimization classification problem loss function known np hard problem even relatively simple class functions linear classifiers though solved efficiently minimal empirical risk zero e data linearly separable practice machine learning algorithms cope either employing convex approximation loss function easier optimize imposing assumptions distribution p displaystyle p
https://en.wikipedia.org/wiki/Occam_learning,computational learning theory occam learning model algorithmic learning objective learner output succinct representation received training data closely related probably approximately correct learning learner evaluated predictive power test set occam learnability implies pac learning wide variety concept classes converse also true pac learnability implies occam learnability occam learning named occam razor principle stating given things equal shorter explanation observed data favored lengthier explanation theory occam learning formal mathematical justification principle first shown blumer et al occam learning implies pac learning standard model learning computational learning theory words parsimony implies predictive power succinctness concept c displaystyle c concept class c displaystyle mathcal c expressed length z e displaystyle size shortest bit string represent c displaystyle c c displaystyle mathcal c occam learning connects succinctness learning algorithm output predictive power unseen data let c displaystyle mathcal c h displaystyle mathcal h concept classes containing target concepts hypotheses respectively constants displaystyle alpha geq displaystyle leq beta learning algorithm l displaystyle l displaystyle occam algorithm c displaystyle mathcal c using h displaystyle mathcal h iff given set x x displaystyle x dots x displaystyle samples labeled according concept c c displaystyle c mathcal c l displaystyle l outputs hypothesis h h displaystyle h mathcal h n displaystyle n maximum length sample x displaystyle x occam algorithm called efficient runs time polynomial n displaystyle n displaystyle z e displaystyle size say concept class c displaystyle mathcal c occam learnable respect hypothesis class h displaystyle mathcal h exists efficient occam algorithm c displaystyle mathcal c using h displaystyle mathcal h occam learnability implies pac learnability following theorem blumer et al shows let l displaystyle l efficient displaystyle occam algorithm c displaystyle mathcal c using h displaystyle mathcal h exists constant displaystyle displaystyle displaystyle thus cardinality version theorem l displaystyle l output consistent hypothesis h displaystyle h probability least displaystyle delta concludes proof first theorem though occam pac learnability equivalent occam framework used produce tighter bounds sample complexity classical problems including conjunctions conjunctions relevant variables decision lists occam algorithms also shown successful pac learning presence errors probabilistic concepts function learning markovian non independent examples
https://en.wikipedia.org/wiki/Probably_approximately_correct_learning,computational learning theory probably approximately correct learning framework mathematical analysis machine learning proposed leslie valiant framework learner receives samples must select generalization function certain class possible functions goal high probability selected function low generalization error learner must able learn concept given arbitrary approximation ratio probability success distribution samples model later extended treat noise important innovation pac framework introduction computational complexity theory concepts machine learning particular learner expected find efficient functions learner must implement efficient procedure order give definition something pac learnable first introduce terminology following definitions two examples used first problem character recognition given array n displaystyle n bits encoding binary valued image example problem finding interval correctly classify points within interval positive points outside range negative let x displaystyle x set called instance space encoding samples character recognition problem instance space x n displaystyle x n interval problem instance space x displaystyle x set bounded intervals r displaystyle mathbb r r displaystyle mathbb r denotes set real numbers concept subset c x displaystyle c subset x one concept set patterns bits x n displaystyle x n encode picture letter p example concept second example set open intervals b displaystyle mid leq leq pi pi leq b leq sqrt contain positive points concept class c displaystyle c set concepts x displaystyle x could set subsets array bits skeletonized connected let e x displaystyle ex procedure draws example x displaystyle x using probability distribution displaystyle gives correct label c displaystyle c x c displaystyle x c otherwise given displaystyle
https://en.wikipedia.org/wiki/Statistical_learning_theory,statistical learning theory framework machine learning drawing fields statistics functional analysis statistical learning theory deals problem finding predictive function based data statistical learning theory led successful applications fields computer vision speech recognition bioinformatics goals learning understanding prediction learning falls many categories including supervised learning unsupervised learning online learning reinforcement learning perspective statistical learning theory supervised learning best understood supervised learning involves learning training set data every point training input output pair input maps output learning problem consists inferring function maps input output learned function used predict output future input depending type output supervised learning problems either problems regression problems classification output takes continuous range values regression problem using ohm law example regression could performed voltage input current output regression would find functional relationship voltage current r displaystyle r classification problems output element discrete set labels classification common machine learning applications facial recognition instance picture person face would input output label would person name input would represented large multidimensional vector whose elements represent pixels picture learning function based training set data function validated test set data data appear training set take x displaystyle x vector space possible inputs displaystyle vector space possible outputs statistical learning theory takes perspective unknown probability distribution product space z x displaystyle z x times e exists unknown p p displaystyle p p training set made n displaystyle n samples probability distribution notated every x displaystyle vec x input vector training data displaystyle output corresponds formalism inference problem consists finding function f x displaystyle f x f displaystyle f sim let h displaystyle mathcal h space functions f x displaystyle f x called hypothesis space hypothesis space space functions algorithm search let v displaystyle v loss function metric difference predicted value f displaystyle f actual value displaystyle expected risk defined target function best possible function f displaystyle f chosen given f displaystyle f satisfies probability distribution p displaystyle p unknown proxy measure expected risk must used measure based training set sample unknown probability distribution called empirical risk learning algorithm chooses function f displaystyle f minimizes empirical risk called empirical risk minimization choice loss function determining factor function f displaystyle f chosen learning algorithm loss function also affects convergence rate algorithm important loss function convex different loss functions used depending whether problem one regression one classification common loss function regression square loss function familiar loss function used ordinary least squares regression form absolute value loss also sometimes used sense indicator function natural loss function classification takes value predicted output actual output takes value predicted output different actual output binary classification displaystyle displaystyle theta heaviside step function machine learning problems major problem arises overfitting learning prediction problem goal find function closely fits data find one accurately predict output future input empirical risk minimization runs risk overfitting finding function matches data exactly predict future output well overfitting symptomatic unstable solutions small perturbation training set data would cause large variation learned function shown stability solution guaranteed generalization consistency guaranteed well regularization solve overfitting problem give problem stability regularization accomplished restricting hypothesis space h displaystyle mathcal h common example would restricting h displaystyle mathcal h linear functions seen reduction standard problem linear regression h displaystyle mathcal h could also restricted polynomial degree p displaystyle p exponentials bounded functions l restriction hypothesis space avoids overfitting form potential functions limited allow choice function gives empirical risk arbitrarily close zero one example regularization tikhonov regularization consists minimizing displaystyle gamma fixed positive parameter regularization parameter tikhonov regularization ensures existence uniqueness stability solution
https://en.wikipedia.org/wiki/Vapnik%E2%80%93Chervonenkis_theory,vapnik chervonenkis theory developed vladimir vapnik alexey chervonenkis theory form computational learning theory attempts explain learning process statistical point view vc theory related statistical learning theory empirical processes richard dudley vladimir vapnik among others applied vc theory empirical processes vc theory covers least four parts vc theory major subbranch statistical learning theory one main applications statistical learning theory provide generalization conditions learning algorithms point view vc theory related stability alternative approach characterizing generalization addition vc theory vc dimension instrumental theory empirical processes case processes indexed vc classes arguably important applications vc theory employed proving generalization several techniques introduced widely used empirical process vc theory discussion mainly based book weak convergence empirical processes applications statistics let x x n displaystyle x ldots x n random elements defined measurable space displaystyle measure q displaystyle q displaystyle measurable functions f x r displaystyle f mathcal x mathbf r define measurability issues ignored technical detail see let f displaystyle mathcal f class measurable functions f x r displaystyle f mathcal x mathbf r define define empirical measure stands dirac measure empirical measure induces map f r displaystyle mathcal f mathbf r given suppose p underlying true distribution data unknown empirical processes theory aims identifying classes f displaystyle mathcal f statements following hold p n p f n displaystyle mathbb p n p mathcal f underset n n f f p displaystyle left frac n f int fdp right former case f displaystyle mathcal f called glivenko cantelli class latter case p f displaystyle forall x sup nolimits f mathcal f vert f pf vert displaystyle set cannot picked since f displaystyle f f displaystyle f would imply lhs strictly positive rhs non positive generalizations notion vc subgraph class e g notion pseudo dimension interested reader look similar setting considered common machine learning let x displaystyle mathcal x feature space displaystyle mathcal function f x displaystyle f mathcal x mathcal called classifier let f displaystyle mathcal f set classifiers similarly previous section define shattering coefficient note go functions f displaystyle mathcal f set function thus define c displaystyle mathcal c collection subsets obtained mapping every f f displaystyle f mathcal f therefore terms previous section shattering coefficient precisely equivalence together sauer lemma implies displaystyle going polynomial n sufficiently large n provided collection c displaystyle mathcal c finite vc index let n displaystyle n ldots observed dataset assume data generated unknown probability distribution p x displaystyle p xy define r p displaystyle r p neq expected loss course since p x displaystyle p xy unknown general one access r displaystyle r however empirical risk given certainly evaluated one following theorem binary classification loss function following generalization bounds words vc inequality saying sample increases provided f displaystyle mathcal f finite vc dimension empirical risk becomes good proxy expected risk note rhs two inequalities converge provided displaystyle grows polynomially n connection framework empirical process framework evident one dealing modified empirical process surprisingly ideas proof vc inequality relies symmetrization argue conditionally data using concentration inequalities interested reader check book theorems
https://en.wikipedia.org/wiki/Conference_on_Neural_Information_Processing_Systems,conference workshop neural information processing systems machine learning computational neuroscience conference held every december conference currently double track meeting includes invited talks well oral poster presentations refereed papers followed parallel track workshops held ski resorts neurips meeting first proposed nips annual invitation snowbird meeting neural networks computing organized california institute technology bell laboratories nips designed complementary open interdisciplinary meeting researchers exploring biological artificial neural networks reflecting multidisciplinary approach nips began information theorist ed posner conference president learning theorist yaser abu mostafa program chairman research presented early nips meetings included wide range topics efforts solve purely engineering problems use computer models tool understanding biological nervous systems since biological artificial systems research streams diverged recent neurips proceedings dominated papers machine learning artificial intelligence statistics nips held denver united states since conference held vancouver canada granada spain lake tahoe united states conference held montreal canada barcelona spain long beach united states montreal canada vancouver canada reflecting origins snowbird utah meeting accompanied workshops organized nearby ski resort outgrew ski resorts abbreviation changed neurips first neurips conference sponsored ieee following neurips conferences organized neurips foundation established ed posner terrence sejnowski president neurips foundation since posner bicycle accident board trustees consists previous general chairs neurips conference first proceedings published book form american institute physics entitled neural information processing systems proceedings following conferences published morgan kaufmann mit press curran associates name advances neural information processing systems conference organizers considered abandoning conference name slang connotation abbreviation nips word nipples slur japanese comment period survey conference participants conference organizers decided keep name instead considered changing acronym since main concern key question survey think change name nips conference absolute number answers response survey respondents men women women agreed name change occur disagreed men agreed name change occur disagreed half surveyed change surveyed favor response neural information processing systems foundation board trustees decided change official abbreviation acronym conference nips neurips conference registered participants citation needed citation needed citation needed citation needed making largest conference artificial intelligence citation needed besides machine learning neuroscience fields represented neurips include cognitive science psychology computer vision statistical linguistics information theory years neurips became premier conference machine learning although neural neurips acronym become something historical relic resurgence deep learning neural networks since fueled faster computers big data led impressive achievement speech recognition object recognition images image captioning language translation world championship performance game go based neural architectures inspired hierarchy areas visual cortex reinforcement learning inspired basal ganglia foundational advances accompanied resurgence artificial intelligence addition invited talks symposia neurips also organizes two named lectureships recognize distinguished researchers neurips board introduced posner lectureship honor neurips founder ed posner two posner lectures given year past lecturers included neurips board introduced breiman lectureship highlight work statistics relevant conference topics lectureship named statistician leo breiman served neurips board past lecturers included nips program chairs duplicated submissions sent separate reviewers evaluate randomness reviewing process several researchers interpreted result regarding whether decision nips completely random john langford writes clearly purely random decision would arbitrariness however quite notable much closer concludes result reviewing process mostly arbitrary past editions future editions
https://en.wikipedia.org/wiki/International_Conference_on_Machine_Learning,international conference machine learning leading international academic conference machine learning along neurips iclr one three primary conferences high impact machine learning artificial intelligence research supported international machine learning society precise dates vary year year paper submissions generally due end january conference generally held following july first icml held pittsburgh
https://en.wikipedia.org/wiki/Machine_Learning_(journal),machine learning peer reviewed scientific journal published since distinguished journal machine intelligence established mid forty editors members editorial board machine learning resigned order support journal machine learning research saying era internet detrimental researchers continue publishing papers expensive journals pay access archives instead wrote supported model jmlr authors retained copyright papers archives freely available internet following mass resignation kluwer changed publishing policy allow authors self archive papers online peer review
https://en.wikipedia.org/wiki/Journal_of_Machine_Learning_Research,journal machine learning research peer reviewed open access scientific journal covering machine learning established first editor chief leslie kaelbling current editors chief francis bach david blei bernhard sch lkopf journal established open access alternative journal machine learning forty editorial board members machine learning resigned saying era internet detrimental researchers continue publishing papers expensive journals pay access archives open access model employed journal machine learning research allows authors publish articles free retain copyright archives freely available online print editions journal published mit press microtome publishing thereafter inception journal received revenue print edition paid subvention mit press microtome publishing response prohibitive costs arranging workshop conference proceedings publication traditional academic publishing companies journal launched proceedings publication arm publishes proceedings several leading machine learning conferences including international conference machine learning colt aistats workshops held conference neural information processing systems
https://en.wikipedia.org/wiki/Glossary_of_artificial_intelligence,glossary artificial intelligence list definitions terms concepts relevant study artificial intelligence sub disciplines related fields related glossaries include glossary computer science glossary robotics glossary machine vision also abduction also adaptive network based fuzzy inference system also artificial emotional intelligence emotion ai also fuzzy string searching also argumentation system also machine intelligence also connectionist system also simply ai planning also self driving car robot car driverless car also backward reasoning also stochastic hopfield network hidden units also propositional satisfiability problem abbreviated satisfiability sat also self learning know system also exhaustive search generate test also smartbot talkbot chatterbot bot im bot interactive agent conversational interface artificial conversational entity also clustering also artificial creativity mechanical creativity creative computing creative computation also theoretical neuroscience mathematical neuroscience also algorithmic number theory also statistical computing also conlang also recombination also dataset also enterprise data warehouse also theory choice also deep structured learning hierarchical learning also epigenetic robotics also conversational agent also dimension reduction also decentralized artificial intelligence also interface agent also known first order predicate calculus predicate logic also forward reasoning also friendly ai fai also graph search also cognitive augmentation machine augmented intelligence enhanced intelligence also virtual assistant personal digital assistant also logic tree also clique tree also mathematical programming also computer audition also mechatronic engineering also self organized system also entity identification entity chunking entity extraction also brain computer interface neural control interface mind machine interface direct neural interface brain machine interface also neuromorphic computing also non deterministic polynomial time hardness also ockham razor ocham razor also ontology extraction ontology generation ontology acquisition also pathing also first order logic predicate logic first order predicate calculus also rationality principle also propositional logic statement logic sentential calculus sentential logic zeroth order logic also random decision forest also frame network also reasoning engine rules engine simply reasoner also simply sld resolution also simply singularity abbreviated h h also tree search also narrow ai
https://en.wikipedia.org/wiki/Glossary_of_artificial_intelligence,glossary artificial intelligence list definitions terms concepts relevant study artificial intelligence sub disciplines related fields related glossaries include glossary computer science glossary robotics glossary machine vision also abduction also adaptive network based fuzzy inference system also artificial emotional intelligence emotion ai also fuzzy string searching also argumentation system also machine intelligence also connectionist system also simply ai planning also self driving car robot car driverless car also backward reasoning also stochastic hopfield network hidden units also propositional satisfiability problem abbreviated satisfiability sat also self learning know system also exhaustive search generate test also smartbot talkbot chatterbot bot im bot interactive agent conversational interface artificial conversational entity also clustering also artificial creativity mechanical creativity creative computing creative computation also theoretical neuroscience mathematical neuroscience also algorithmic number theory also statistical computing also conlang also recombination also dataset also enterprise data warehouse also theory choice also deep structured learning hierarchical learning also epigenetic robotics also conversational agent also dimension reduction also decentralized artificial intelligence also interface agent also known first order predicate calculus predicate logic also forward reasoning also friendly ai fai also graph search also cognitive augmentation machine augmented intelligence enhanced intelligence also virtual assistant personal digital assistant also logic tree also clique tree also mathematical programming also computer audition also mechatronic engineering also self organized system also entity identification entity chunking entity extraction also brain computer interface neural control interface mind machine interface direct neural interface brain machine interface also neuromorphic computing also non deterministic polynomial time hardness also ockham razor ocham razor also ontology extraction ontology generation ontology acquisition also pathing also first order logic predicate logic first order predicate calculus also rationality principle also propositional logic statement logic sentential calculus sentential logic zeroth order logic also random decision forest also frame network also reasoning engine rules engine simply reasoner also simply sld resolution also simply singularity abbreviated h h also tree search also narrow ai
https://en.wikipedia.org/wiki/List_of_datasets_for_machine-learning_research,datasets used machine learning research cited peer reviewed academic journals datasets integral part field machine learning major advances field result advances learning algorithms computer hardware less intuitively availability high quality training datasets high quality labeled training datasets supervised semi supervised machine learning algorithms usually difficult expensive produce large amount time needed label data although need labeled high quality datasets unsupervised learning also difficult costly produce datasets consisting primarily images videos tasks object detection facial recognition multi label classification computer vision face images used extensively develop facial recognition systems face detection many projects use images faces ijcv fg images text images text images text label files label files character dataset text classification datasets consisting primarily text tasks natural language processing sentiment analysis translation cluster analysis categorization citation analysis datasets sounds sound features datasets containing electric signal information requiring sort signal processing analysis datasets physical systems datasets biological systems section includes datasets deals structured data datasets consisting rows observations columns attributes characterizing observations typically used regression analysis classification types algorithms also used section includes datasets fit categories datasets come myriad formats sometimes difficult use considerable work put curating standardizing format datasets make easier use machine learning research
https://en.wikipedia.org/wiki/Outline_of_machine_learning,following outline provided overview topical guide machine learning machine learning subfield soft computing within computer science evolved study pattern recognition computational learning theory artificial intelligence arthur samuel defined machine learning field study gives computers ability learn without explicitly programmed machine learning explores study construction algorithms learn make predictions data algorithms operate building model example training set input observations order make data driven predictions decisions expressed outputs rather following strictly static program instructions subfields machine learning cross disciplinary fields involving machine learning applications machine learning machine learning hardware machine learning tools machine learning framework proprietary machine learning frameworks open source machine learning frameworks machine learning library machine learning algorithm machine learning method dimensionality reduction ensemble learning meta learning reinforcement learning supervised learning bayesian statistics decision tree algorithm linear classifier unsupervised learning artificial neural network association rule learning hierarchical clustering cluster analysis anomaly detection semi supervised learning deep learning history machine learning machine learning projects machine learning organizations books machine learning
https://en.wikipedia.org/wiki/Artificial_intelligence,artificial intelligence intelligence demonstrated machines unlike natural intelligence displayed humans animals leading ai textbooks define field study intelligent agents device perceives environment takes actions maximize chance successfully achieving goals colloquially term artificial intelligence often used describe machines mimic cognitive functions humans associate human mind learning problem solving machines become increasingly capable tasks considered require intelligence often removed definition ai phenomenon known ai effect quip tesler theorem says ai whatever done yet instance optical character recognition frequently excluded things considered ai become routine technology modern machine capabilities generally classified ai include successfully understanding human speech competing highest level strategic game systems autonomously operating cars intelligent routing content delivery networks military simulations artificial intelligence founded academic discipline years since experienced several waves optimism followed disappointment loss funding followed new approaches success renewed funding history ai research divided sub fields often fail communicate sub fields based technical considerations particular goals use particular tools deep philosophical differences sub fields also based social factors traditional problems ai research include reasoning knowledge representation planning learning natural language processing perception ability move manipulate objects general intelligence among field long term goals approaches include statistical methods computational intelligence traditional symbolic ai many tools used ai including versions search mathematical optimization artificial neural networks methods based statistics probability economics ai field draws upon computer science information engineering mathematics psychology linguistics philosophy many fields field founded assumption human intelligence precisely described machine made simulate raises philosophical arguments mind ethics creating artificial beings endowed human like intelligence issues explored myth fiction philosophy since antiquity people also consider ai danger humanity progresses unabated others believe ai unlike previous technological revolutions create risk mass unemployment twenty first century ai techniques experienced resurgence following concurrent advances computer power large amounts data theoretical understanding ai techniques become essential part technology industry helping solve many challenging problems computer science software engineering operations research thought capable artificial beings appeared storytelling devices antiquity common fiction mary shelley frankenstein karel apek r u r characters fates raised many issues discussed ethics artificial intelligence study mechanical formal reasoning began philosophers mathematicians antiquity study mathematical logic led directly alan turing theory computation suggested machine shuffling symbols simple could simulate conceivable act mathematical deduction insight digital computers simulate process formal reasoning known church turing thesis along concurrent discoveries neurobiology information theory cybernetics led researchers consider possibility building electronic brain turing proposed changing question whether machine intelligent whether possible machinery show intelligent behaviour first work generally recognized ai mccullouch pitts formal design turing complete artificial neurons field ai research born workshop dartmouth college term artificial intelligence coined john mccarthy distinguish field cybernetics escape influence cyberneticist norbert wiener attendees allen newell herbert simon john mccarthy marvin minsky arthur samuel became founders leaders ai research students produced programs press described astonishing computers learning checkers strategies solving word problems algebra proving logical theorems speaking english middle research u heavily funded department defense laboratories established around world ai founders optimistic future herbert simon predicted machines capable within twenty years work man marvin minsky agreed writing within generation problem creating artificial intelligence substantially solved failed recognize difficulty remaining tasks progress slowed response criticism sir james lighthill ongoing pressure us congress fund productive projects u british governments cut exploratory research ai next years would later called ai winter period obtaining funding ai projects difficult early ai research revived commercial success expert systems form ai program simulated knowledge analytical skills human experts market ai reached billion dollars time japan fifth generation computer project inspired u british governments restore funding academic research however beginning collapse lisp machine market ai fell disrepute second longer lasting hiatus began development metal oxide semiconductor large scale integration form complementary mos transistor technology enabled development practical artificial neural network technology landmark publication field book analog vlsi implementation neural systems carver mead mohammed ismail late early st century ai began used logistics data mining medical diagnosis areas success due increasing computational power greater emphasis solving specific problems new ties ai fields commitment researchers mathematical methods scientific standards deep blue became first computer chess playing system beat reigning world chess champion garry kasparov may jeopardy quiz show exhibition match ibm question answering system watson defeated two greatest jeopardy champions brad rutter ken jennings significant margin faster computers algorithmic improvements access large amounts data enabled advances machine learning perception data hungry deep learning methods started dominate accuracy benchmarks around kinect provides body motion interface xbox xbox one uses algorithms emerged lengthy ai research intelligent personal assistants smartphones march alphago games go match go champion lee sedol becoming first computer go playing system beat professional go player without handicaps future go summit alphago three game match ke jie time continuously held world ranking two years marked completion significant milestone development artificial intelligence go relatively complex game chess according bloomberg jack clark landmark year artificial intelligence number software projects use ai within google increased sporadic usage projects clark also presents factual data indicating improvements ai since supported lower error rates image processing tasks attributes increase affordable neural networks due rise cloud computing infrastructure increase research tools datasets cited examples include microsoft development skype system automatically translate one language another facebook system describe images blind people survey one five companies reported incorporated ai offerings processes around china greatly accelerated government funding given large supply data rapidly increasing research output observers believe may track becoming ai superpower however acknowledged reports regarding artificial intelligence tended exaggerated computer science defines ai research study intelligent agents device perceives environment takes actions maximize chance successfully achieving goals elaborate definition characterizes ai system ability correctly interpret external data learn data use learnings achieve specific goals tasks flexible adaptation typical ai analyzes environment takes actions maximize chance success ai intended utility function simple complex goals explicitly defined induced ai programmed reinforcement learning goals implicitly induced rewarding types behavior punishing others alternatively evolutionary system induce goals using fitness function mutate preferentially replicate high scoring ai systems similar animals evolved innately desire certain goals finding food ai systems nearest neighbor instead reason analogy systems generally given goals except degree goals implicit training data systems still benchmarked non goal system framed system whose goal successfully accomplish narrow classification task ai often revolves around use algorithms algorithm set unambiguous instructions mechanical computer execute b complex algorithm often built top simpler algorithms simple example algorithm following recipe play tic tac toe many ai algorithms capable learning data enhance learning new heuristics write algorithms learners described including bayesian networks decision trees nearest neighbor could theoretically learn approximate function including combination mathematical functions would best describe world citation needed learners could therefore derive possible knowledge considering every possible hypothesis matching data practice seldom possible consider every possibility phenomenon combinatorial explosion time needed solve problem grows exponentially much ai research involves figuring identify avoid considering broad range possibilities unlikely beneficial example viewing map looking shortest driving route denver new york east one cases skip looking path san francisco areas far west thus ai wielding pathfinding algorithm like avoid combinatorial explosion would ensue every possible route ponderously considered earliest approach ai symbolism otherwise healthy adult fever may influenza second general approach bayesian inference current patient fever adjust probability influenza way third major approach extremely popular routine business ai applications analogizers svm nearest neighbor examining records known past patients whose temperature symptoms age factors mostly match current patient x patients turned influenza fourth approach harder intuitively understand inspired brain machinery works artificial neural network approach uses artificial neurons learn comparing desired output altering strengths connections internal neurons reinforce connections seemed useful four main approaches overlap evolutionary systems example neural nets learn make inferences generalize make analogies systems implicitly explicitly use multiple approaches alongside many ai non ai algorithms best approach often different depending problem learning algorithms work basis strategies algorithms inferences worked well past likely continue working well future inferences obvious since sun rose every morning last days probably rise tomorrow morning well nuanced x families geographically separate species color variants chance undiscovered black swans exist learners also work basis occam razor simplest theory explains data likeliest therefore according occam razor principle learner must designed prefers simpler theories complex theories except cases complex theory proven substantially better settling bad overly complex theory gerrymandered fit past training data known overfitting many systems attempt reduce overfitting rewarding theory accordance well fits data penalizing theory accordance complex theory besides classic overfitting learners also disappoint learning wrong lesson toy example image classifier trained pictures brown horses black cats might conclude brown patches likely horses real world example unlike humans current image classifiers determine spatial relationship components picture instead learn abstract patterns pixels humans oblivious linearly correlate images certain types real objects faintly superimposing pattern legitimate image results adversarial image system misclassifies c compared humans existing ai lacks several features human commonsense reasoning notably humans powerful mechanisms reasoning na physics space time physical interactions enables even young children easily make inferences like roll pen table fall floor humans also powerful mechanism folk psychology helps interpret natural language sentences city councilmen refused demonstrators permit advocated violence lack common knowledge means ai often makes different mistakes humans make ways seem incomprehensible example existing self driving cars cannot reason location intentions pedestrians exact way humans instead must use non human modes reasoning avoid accidents cognitive capabilities current architectures limited using simplified version intelligence really capable instance human mind come ways reason beyond measure logical explanations different occurrences life would otherwise straightforward equivalently difficult problem may challenging solve computationally opposed using human mind gives rise two classes models structuralist functionalist structural models aim loosely mimic basic intelligence operations mind reasoning logic functional model refers correlating data computed counterpart overall research goal artificial intelligence create technology allows computers machines function intelligent manner general problem simulating intelligence broken sub problems consist particular traits capabilities researchers expect intelligent system display traits described received attention early researchers developed algorithms imitated step step reasoning humans use solve puzzles make logical deductions late ai research developed methods dealing uncertain incomplete information employing concepts probability economics algorithms proved insufficient solving large reasoning problems experienced combinatorial explosion became exponentially slower problems grew larger even humans rarely use step step deduction early ai research could model solve problems using fast intuitive judgments knowledge representation knowledge engineering central classical ai research expert systems attempt gather explicit knowledge possessed experts narrow domain addition projects attempt gather commonsense knowledge known average person database containing extensive knowledge world among things comprehensive commonsense knowledge base would contain objects properties categories relations objects situations events states time causes effects knowledge knowledge many less well researched domains representation exists ontology set objects relations concepts properties formally described software agents interpret semantics captured description logic concepts roles individuals typically implemented classes properties individuals web ontology language general ontologies called upper ontologies attempt provide foundation knowledge acting mediators domain ontologies cover specific knowledge particular knowledge domain formal knowledge representations used content based indexing retrieval scene interpretation clinical decision support knowledge discovery areas among difficult problems knowledge representation intelligent agents must able set goals achieve need way visualize future representation state world able make predictions actions change able make choices maximize utility available choices classical planning problems agent assume system acting world allowing agent certain consequences actions however agent actor requires agent reason uncertainty calls agent assess environment make predictions also evaluate predictions adapt based assessment multi agent planning uses cooperation competition many agents achieve given goal emergent behavior used evolutionary algorithms swarm intelligence machine learning fundamental concept ai research since field inception study computer algorithms improve automatically experience unsupervised learning ability find patterns stream input without requiring human label inputs first supervised learning includes classification numerical regression requires human label input data first classification used determine category something belongs occurs program sees number examples things several categories regression attempt produce function describes relationship inputs outputs predicts outputs change inputs change classifiers regression learners viewed function approximators trying learn unknown function example spam classifier viewed learning function maps text email one two categories spam spam computational learning theory assess learners computational complexity sample complexity notions optimization reinforcement learning agent rewarded good responses punished bad ones agent uses sequence rewards punishments form strategy operating problem space natural language processing allows machines read understand human language sufficiently powerful natural language processing system would enable natural language user interfaces acquisition knowledge directly human written sources newswire texts straightforward applications natural language processing include information retrieval text mining question answering machine translation many current approaches use word co occurrence frequencies construct syntactic representations text keyword spotting strategies search popular scalable dumb search query dog might match documents literal word dog miss document word poodle lexical affinity strategies use occurrence words accident assess sentiment document modern statistical nlp approaches combine strategies well others often achieve acceptable accuracy page paragraph level beyond semantic nlp ultimate goal narrative nlp embody full understanding commonsense reasoning transformer based deep learning architectures could generate coherent text machine perception ability use input sensors microphones wireless signals active lidar sonar radar tactile sensors deduce aspects world applications include speech recognition facial recognition object recognition computer vision ability analyze visual input input usually ambiguous giant fifty meter tall pedestrian far away may produce pixels nearby normal sized pedestrian requiring ai judge relative likelihood reasonableness different interpretations example using object model assess fifty meter pedestrians exist ai heavily used robotics advanced robotic arms industrial robots widely used modern factories learn experience move efficiently despite presence friction gear slippage modern mobile robot given small static visible environment easily determine location map environment however dynamic environments interior patient breathing body pose greater challenge motion planning process breaking movement task primitives individual joint movements movement often involves compliant motion process movement requires maintaining physical contact object moravec paradox generalizes low level sensorimotor skills humans take granted counterintuitively difficult program robot paradox named hans moravec stated comparatively easy make computers exhibit adult level performance intelligence tests playing checkers difficult impossible give skills one year old comes perception mobility attributed fact unlike checkers physical dexterity direct target natural selection millions years moravec paradox extended many forms social intelligence distributed multi agent coordination autonomous vehicles remains difficult problem affective computing interdisciplinary umbrella comprises systems recognize interpret process simulate human affects moderate successes related affective computing include textual sentiment analysis recently multimodal affect analysis wherein ai classifies affects displayed videotaped subject long run social skills understanding human emotion game theory would valuable social agent ability predict actions others understanding motives emotional states would allow agent make better decisions computer systems mimic human emotion expressions appear sensitive emotional dynamics human interaction otherwise facilitate human computer interaction similarly virtual assistants programmed speak conversationally even banter humorously tends give na users unrealistic conception intelligent existing computer agents actually historically projects cyc knowledge base massive japanese fifth generation computer systems initiative attempted cover breadth human cognition early projects failed escape limitations non quantitative symbolic logic models retrospect greatly underestimated difficulty cross domain ai nowadays current ai researchers work instead tractable narrow ai applications many researchers predict narrow ai work different individual domains eventually incorporated machine artificial general intelligence combining narrow skills mentioned article point even exceeding human ability areas many advances general cross domain significance one high profile example deepmind developed generalized artificial intelligence could learn many diverse atari games later developed variant system succeeds sequential learning besides transfer learning hypothetical agi breakthroughs could include development reflective architectures engage decision theoretic metareasoning figuring slurp comprehensive knowledge base entire unstructured web argue kind conceptually straightforward mathematically difficult master algorithm could lead agi finally emergent approaches look simulating human intelligence extremely closely believe anthropomorphic features like artificial brain simulated child development may someday reach critical point general intelligence emerges many problems article may also require general intelligence machines solve problems well people example even specific straightforward tasks like machine translation require machine read write languages follow author argument know talked faithfully reproduce author original intent problem like machine translation considered ai complete problems need solved simultaneously order reach human level machine performance established unifying theory paradigm guides ai research researchers disagree many issues long standing questions remained unanswered artificial intelligence simulate natural intelligence studying psychology neurobiology human biology irrelevant ai research bird biology aeronautical engineering intelligent behavior described using simple elegant principles necessarily require solving large number unrelated problems number researchers explored connection neurobiology information theory cybernetics built machines used electronic networks exhibit rudimentary intelligence w grey walter turtles johns hopkins beast many researchers gathered meetings teleological society princeton university ratio club england approach largely abandoned although elements would revived access digital computers became possible mid ai research began explore possibility human intelligence could reduced symbol manipulation research centered three institutions carnegie mellon university stanford mit described one developed style research john haugeland named symbolic approaches ai good old fashioned ai gofai symbolic approaches achieved great success simulating high level thinking small demonstration programs approaches based cybernetics artificial neural networks abandoned pushed background researchers convinced symbolic approaches would eventually succeed creating machine artificial general intelligence considered goal field economist herbert simon allen newell studied human problem solving skills attempted formalize work laid foundations field artificial intelligence well cognitive science operations research management science research team used results psychological experiments develop programs simulated techniques people used solve problems tradition centered carnegie mellon university would eventually culminate development soar architecture middle unlike simon newell john mccarthy felt machines need simulate human thought instead try find essence abstract reasoning problem solving regardless whether people used algorithms laboratory stanford focused using formal logic solve wide variety problems including knowledge representation planning learning logic also focus work university edinburgh elsewhere europe led development programming language prolog science logic programming researchers mit found solving difficult problems vision natural language processing required ad hoc solutions argued simple general principle would capture aspects intelligent behavior roger schank described anti logic approaches scruffy commonsense knowledge bases example scruffy ai since must built hand one complicated concept time computers large memories became available around researchers three traditions began build knowledge ai applications knowledge revolution led development deployment expert systems first truly successful form ai software key component system architecture expert systems knowledge base stores facts rules illustrate ai knowledge revolution also driven realization enormous amounts knowledge would required many simple ai applications progress symbolic ai seemed stall many believed symbolic systems would never able imitate processes human cognition especially perception robotics learning pattern recognition number researchers began look sub symbolic approaches specific ai problems sub symbolic methods manage approach intelligence without specific representations knowledge includes embodied situated behavior based nouvelle ai researchers related field robotics rodney brooks rejected symbolic ai focused basic engineering problems would allow robots move survive work revived non symbolic point view early cybernetics researchers reintroduced use control theory ai coincided development embodied mind thesis related field cognitive science idea aspects body required higher intelligence within developmental robotics developmental learning approaches elaborated upon allow robots accumulate repertoires novel skills autonomous self exploration social interaction human teachers use guidance mechanisms interest neural networks connectionism revived david rumelhart others middle artificial neural networks example soft computing solutions problems cannot solved complete logical certainty approximate solution often sufficient soft computing approaches ai include fuzzy systems grey system theory evolutionary computation many statistical tools application soft computing ai studied collectively emerging discipline computational intelligence much traditional gofai got bogged ad hoc patches symbolic computation worked toy models failed generalize real world results however around ai researchers adopted sophisticated mathematical tools hidden markov models information theory normative bayesian decision theory compare unify competing architectures shared mathematical language permitted high level collaboration established fields compared gofai new statistical learning techniques hmm neural networks gaining higher levels accuracy many practical domains data mining without necessarily acquiring semantic understanding datasets increased successes real world data led increasing emphasis comparing different approaches shared test data see approach performed best broader context provided idiosyncratic toy models ai research becoming scientific nowadays results experiments often rigorously measurable sometimes reproducible different statistical learning techniques different limitations example basic hmm cannot model infinite possible combinations natural language critics note shift gofai statistical learning often also shift away explainable ai agi research scholars caution reliance statistical learning argue continuing research gofai still necessary attain general intelligence ai relevant intellectual task modern artificial intelligence techniques pervasive numerous list frequently technique reaches mainstream use longer considered artificial intelligence phenomenon described ai effect high profile examples ai include autonomous vehicles medical diagnosis creating art proving mathematical theorems playing games search engines online assistants image recognition photographs spam filtering predicting flight delays prediction judicial decisions targeting online advertisements energy storage social media sites overtaking tv source news young people news organizations increasingly reliant social media platforms generating distribution major publishers use artificial intelligence technology post stories effectively generate higher volumes traffic ai also produce deepfakes content altering technology zdnet reports presents something actually occur though americans believe deepfakes cause harm good believe targeted boom election year also opens public discourse threats videos falsified politician media three philosophical questions related ai citation needed machines intelligence potential use intelligence prevent harm minimize risks may ability use ethical reasoning better choose actions world need policy making devise policies regulate artificial intelligence robotics research area includes machine ethics artificial moral agents friendly ai discussion towards building human rights framework also talks joseph weizenbaum computer power human reason wrote ai applications cannot definition successfully simulate genuine human empathy use ai technology fields customer service psychotherapy deeply misguided weizenbaum also bothered ai researchers willing view human mind nothing computer program weizenbaum points suggest ai research devalues human life wendell wallach introduced concept artificial moral agents book moral machines wallach amas become part research landscape artificial intelligence guided two central questions identifies humanity want computers making moral decisions bots really moral wallach question centered issue whether machines demonstrate equivalent moral behavior unlike constraints society may place development amas field machine ethics concerned giving machines ethical principles procedure discovering way resolve ethical dilemmas might encounter enabling function ethically responsible manner ethical decision making field delineated aaai fall symposium machine ethics past research concerning relationship technology ethics largely focused responsible irresponsible use technology human beings people interested human beings ought treat machines cases human beings engaged ethical reasoning time come adding ethical dimension least machines recognition ethical ramifications behavior involving machines well recent potential developments machine autonomy necessitate contrast computer hacking software property issues privacy issues topics normally ascribed computer ethics machine ethics concerned behavior machines towards human users machines research machine ethics key alleviating concerns autonomous systems could argued notion autonomous machines without dimension root fear concerning machine intelligence investigation machine ethics could enable discovery problems current ethical theories advancing thinking ethics machine ethics sometimes referred machine morality computational ethics computational morality variety perspectives nascent field found collected edition machine ethics stems aaai fall symposium machine ethics political scientist charles rubin believes ai neither designed guaranteed benevolent argues sufficiently advanced benevolence may indistinguishable malevolence humans assume machines robots would treat us favorably priori reason believe would sympathetic system morality evolved along particular biology hyper intelligent software may necessarily decide support continued existence humanity would extremely difficult stop topic also recently begun discussed academic publications real source risks civilization humans planet earth one proposal deal ensure first generally intelligent ai friendly ai able control subsequently developed ais question whether kind check could actually remain place leading ai researcher rodney brooks writes think mistake worrying us developing malevolent ai anytime next hundred years think worry stems fundamental error distinguishing difference real recent advances particular aspect ai enormity complexity building sentient volitional intelligence lethal autonomous weapons concern currently countries researching battlefield robots including united states china russia united kingdom many people concerned risk superintelligent ai also want limit use artificial soldiers drones ai system replicates key aspects human intelligence system also sentient mind conscious experiences question closely related philosophical problem nature human consciousness generally referred hard problem consciousness david chalmers identified two problems understanding mind named hard easy problems consciousness easy problem understanding brain processes signals makes plans controls behavior hard problem explaining feels feel like anything human information processing easy explain however human subjective experience difficult explain example consider happens person shown color swatch identifies saying red easy problem requires understanding machinery brain makes possible person know color swatch red hard problem people also know something else also know red looks like e everyone knows subjective experience exists every day hard problem explaining brain creates exists different knowledge aspects brain computationalism position philosophy mind human mind human brain information processing system thinking form computing computationalism argues relationship mind body similar identical relationship software hardware thus may solution mind body problem philosophical position inspired work ai researchers cognitive scientists originally proposed philosophers jerry fodor hilary putnam philosophical position john searle named strong ai states appropriately programmed computer right inputs outputs would thereby mind exactly sense human beings minds searle counters assertion chinese room argument asks us look inside computer try find mind might machine created intelligence could also feel feel rights human issue known robot rights currently considered example california institute future although many critics believe discussion premature critics transhumanism argue hypothetical robot rights would lie spectrum animal rights human rights subject profoundly discussed documentary film plug pray many sci fi media star trek next generation character commander data fought disassembled research wanted become human robotic holograms voyager limits intelligent machines human machine hybrids superintelligence hyperintelligence superhuman intelligence hypothetical agent would possess intelligence far surpassing brightest gifted human mind superintelligence may also refer form degree intelligence possessed agent research strong ai produced sufficiently intelligent software might able reprogram improve improved software would even better improving leading recursive self improvement new intelligence could thus increase exponentially dramatically surpass humans science fiction writer vernor vinge named scenario singularity technological singularity accelerating progress technologies cause runaway effect wherein artificial intelligence exceed human intellectual capacity control thus radically changing even ending civilization capabilities intelligence may impossible comprehend technological singularity occurrence beyond events unpredictable even unfathomable ray kurzweil used moore law calculate desktop computers processing power human brains year predicts singularity occur robot designer hans moravec cyberneticist kevin warwick inventor ray kurzweil predicted humans machines merge future cyborgs capable powerful either idea called transhumanism roots aldous huxley robert ettinger edward fredkin argues artificial intelligence next stage evolution idea first proposed samuel butler darwin among machines far back expanded upon george dyson book name long term economic effects ai uncertain survey economists showed disagreement whether increasing use robots ai cause substantial increase long term unemployment generally agree could net benefit productivity gains redistributed february european union white paper artificial intelligence advocated artificial intelligence economic benefits including improving healthcare increasing efficiency farming contributing climate change mitigation adaptation improving efficiency production systems predictive maintenance acknowledging potential risks relationship automation employment complicated automation eliminates old jobs also creates new jobs micro economic macro economic effects unlike previous waves automation many middle class jobs may eliminated artificial intelligence economist states worry ai could white collar jobs steam power blue collar ones industrial revolution worth taking seriously subjective estimates risk vary widely example michael osborne carl benedikt frey estimate u jobs high risk potential automation oecd report classifies u jobs high risk jobs extreme risk range paralegals fast food cooks job demand likely increase care related professions ranging personal healthcare clergy author martin ford others go argue many jobs routine repetitive predictable ford warns jobs may automated next couple decades many new jobs may accessible people average capability even retraining economists point past technology tended increase rather reduce total employment acknowledge uncharted territory ai potential negative effects ai automation major issue andrew yang presidential campaign united states irakli beridze head centre artificial intelligence robotics unicri united nations expressed think dangerous applications ai point view would criminals large terrorist organizations using disrupt large processes simply pure harm terrorists could cause harm via digital warfare could combination robotics drones ai things well could really dangerous course risks come things like job losses massive numbers people losing jobs find solution extremely dangerous things like lethal autonomous weapons systems properly governed otherwise massive potential misuse widespread use artificial intelligence could unintended consequences dangerous undesirable scientists future life institute among others described short term research goals see ai influences economy laws ethics involved ai minimize ai security risks long term scientists proposed continue optimizing function minimizing possible security risks come along new technologies concerned algorithmic bias ai programs may unintentionally become biased processing data exhibits bias algorithms already numerous applications legal systems example compas commercial program widely used u courts assess likelihood defendant becoming recidivist propublica claims average compas assigned recidivism risk level black defendants significantly higher average compas assigned risk level white defendants physicist stephen hawking microsoft founder bill gates spacex founder elon musk expressed concerns possibility ai could evolve point humans could control hawking theorizing could spell end human race development full artificial intelligence could spell end human race humans develop artificial intelligence take redesign ever increasing rate humans limited slow biological evolution compete would superseded book superintelligence philosopher nick bostrom provides argument artificial intelligence pose threat humankind argues sufficiently intelligent ai chooses actions based achieving goal exhibit convergent behavior acquiring resources protecting shut ai goals fully reflect humanity one example ai told compute many digits pi possible might harm humanity order acquire resources prevent shut ultimately better achieve goal bostrom also emphasizes difficulty fully conveying humanity values advanced ai uses hypothetical example giving ai goal make humans smile illustrate misguided attempt ai scenario become superintelligent bostrom argues may resort methods humans would find horrifying inserting electrodes facial muscles humans cause constant beaming grins would efficient way achieve goal making humans smile book human compatible ai researcher stuart j russell echoes bostrom concerns also proposing approach developing provably beneficial machines focused uncertainty deference humans possibly involving inverse reinforcement learning concern risk artificial intelligence led high profile donations investments group prominent tech titans including peter thiel amazon web services musk committed billion openai nonprofit company aimed championing responsible ai development opinion experts within field artificial intelligence mixed sizable fractions concerned unconcerned risk eventual superhumanly capable ai technology industry leaders believe artificial intelligence helpful current form continue assist humans oracle ceo mark hurd stated ai actually create jobs less jobs humans needed manage ai systems facebook ceo mark zuckerberg believes ai unlock huge amount positive things curing disease increasing safety autonomous cars january musk donated million future life institute fund research understanding ai decision making goal institute grow wisdom manage growing power technology musk also funds companies developing artificial intelligence deepmind vicarious keep eye going artificial intelligence think potentially dangerous outcome danger uncontrolled advanced ai realized hypothetical ai would overpower think humanity minority experts argue possibility far enough future worth researching counterarguments revolve around humans either intrinsically convergently valuable perspective artificial intelligence regulation artificial intelligence development public sector policies laws promoting regulating artificial intelligence therefore related broader regulation algorithms regulatory policy landscape ai emerging issue jurisdictions globally including european union regulation considered necessary encourage ai manage associated risks regulation ai mechanisms review boards also seen social means approach ai control problem thought capable artificial beings appeared storytelling devices since antiquity persistent theme science fiction common trope works began mary shelley frankenstein human creation becomes threat masters includes works arthur c clarke stanley kubrick space odyssey hal murderous computer charge discovery one spaceship well terminator matrix contrast rare loyal robots gort day earth stood still bishop aliens less prominent popular culture isaac asimov introduced three laws robotics many books stories notably multivac series super intelligent computer name asimov laws often brought lay discussions machine ethics almost artificial intelligence researchers familiar asimov laws popular culture generally consider laws useless many reasons one ambiguity transhumanism explored manga ghost shell science fiction series dune artist hajime sorayama sexy robots series painted published japan depicting actual organic human form lifelike muscular metallic skins later gynoids book followed used influenced movie makers including george lucas creatives sorayama never considered organic robots real part nature always unnatural product human mind fantasy existing mind even realized actual form several works use ai force us confront fundamental question makes us human showing us artificial beings ability feel thus suffer appears karel apek r u r films artificial intelligence ex machina well novel androids dream electric sheep philip k dick dick considers idea understanding human subjectivity altered technology created artificial intelligence see also logic machines fiction list fictional computers
https://en.wikipedia.org/wiki/Mathematical_model,mathematical model description system using mathematical concepts language process developing mathematical model termed mathematical modeling mathematical models used natural sciences engineering disciplines well non physical systems social sciences mathematical models also used music linguistics philosophy model may help explain system study effects different components make predictions behaviour mathematical models take many forms including dynamical systems statistical models differential equations game theoretic models types models overlap given model involving variety abstract structures general mathematical models may include logical models many cases quality scientific field depends well mathematical models developed theoretical side agree results repeatable experiments lack agreement theoretical mathematical models experimental measurements often leads important advances better theories developed physical sciences traditional mathematical model contains following elements mathematical models usually composed relationships variables relationships described operators algebraic operators functions differential operators etc variables abstractions system parameters interest quantified several classification criteria used mathematical models according structure business engineering mathematical models may used maximize certain output system consideration require certain inputs system relating inputs outputs depends variables decision variables state variables exogenous variables random variables decision variables sometimes known independent variables exogenous variables sometimes known parameters constants variables independent state variables dependent decision input random exogenous variables furthermore output variables dependent state system objectives constraints system users represented functions output variables state variables objective functions depend perspective model user depending context objective function also known index performance measure interest user although limit number objective functions constraints model using optimizing model becomes involved number increases example economists often apply linear algebra using input output models complicated mathematical models many variables may consolidated use vectors one symbol represents several variables mathematical modeling problems often classified black box white box models according much priori information system available black box model system priori information available white box model system necessary information available practically systems somewhere black box white box models concept useful intuitive guide deciding approach take usually preferable use much priori information possible make model accurate therefore white box models usually considered easier used information correctly model behave correctly often priori information comes forms knowing type functions relating different variables example make model medicine works human system know usually amount medicine blood exponentially decaying function still left several unknown parameters rapidly medicine amount decay initial amount medicine blood example therefore completely white box model parameters estimated means one use model black box models one tries estimate functional form relations variables numerical parameters functions using priori information could end example set functions probably could describe system adequately priori information would try use functions general possible cover different models often used approach black box models neural networks usually make assumptions incoming data alternatively narmax algorithms developed part nonlinear system identification used select model terms determine model structure estimate unknown parameters presence correlated nonlinear noise advantage narmax models compared neural networks narmax produces models written related underlying process whereas neural networks produce approximation opaque sometimes useful incorporate subjective information mathematical model done based intuition experience expert opinion based convenience mathematical form bayesian statistics provides theoretical framework incorporating subjectivity rigorous analysis specify prior probability distribution update distribution based empirical data example approach would necessary situation experimenter bends coin slightly tosses recording whether comes heads given task predicting probability next flip comes heads bending coin true probability coin come heads unknown experimenter would need make decision prior distribution use incorporation subjective information might important get accurate estimate probability general model complexity involves trade simplicity accuracy model occam razor principle particularly relevant modeling essential idea among models roughly equal predictive power simplest one desirable added complexity usually improves realism model make model difficult understand analyze also pose computational problems including numerical instability thomas kuhn argues science progresses explanations tend become complex paradigm shift offers radical simplification example modeling flight aircraft could embed mechanical part aircraft model would thus acquire almost white box model system however computational cost adding huge amount detail would effectively inhibit usage model additionally uncertainty would increase due overly complex system separate part induces amount variance model therefore usually appropriate make approximations reduce model sensible size engineers often accept approximations order get robust simple model example newton classical mechanics approximated model real world still newton model quite sufficient ordinary life situations long particle speeds well speed light study macro particles note better accuracy necessarily mean better model statistical models prone overfitting means model fitted data much lost ability generalize new events observed model pure white box contains parameters used fit model system intended describe modeling done artificial neural network machine learning optimization parameters called training optimization model hyperparameters called tuning often uses cross validation conventional modeling explicitly given mathematical functions parameters often determined curve fitting citation needed crucial part modeling process evaluation whether given mathematical model describes system accurately question difficult answer involves several different types evaluation usually easiest part model evaluation checking whether model fits experimental measurements empirical data models parameters common approach test fit split data two disjoint subsets training data verification data training data used estimate model parameters accurate model closely match verification data even though data used set model parameters practice referred cross validation statistics defining metric measure distances observed predicted data useful tool assessing model fit statistics decision theory economic models loss function plays similar role rather straightforward test appropriateness parameters difficult test validity general mathematical form model general mathematical tools developed test fit statistical models models involving differential equations tools nonparametric statistics sometimes used evaluate well data fit known distribution come general model makes minimal assumptions model mathematical form assessing scope model determining situations model applicable less straightforward model constructed based set data one must determine systems situations known data typical set data question whether model describes well properties system data points called interpolation question events data points outside observed data called extrapolation example typical limitations scope model evaluating newtonian classical mechanics note newton made measurements without advanced equipment could measure properties particles travelling speeds close speed light likewise measure movements molecules small particles macro particles surprising model extrapolate well domains even though model quite sufficient ordinary life physics many types modeling implicitly involve claims causality usually true models involving differential equations purpose modeling increase understanding world validity model rests fit empirical observations also ability extrapolate situations data beyond originally described model one think differentiation qualitative quantitative predictions one also argue model worthless unless provides insight goes beyond already known direct investigation phenomenon studied example criticism argument mathematical models optimal foraging theory offer insight goes beyond common sense conclusions evolution basic principles ecology mathematical models great importance natural sciences particularly physics physical theories almost invariably expressed using mathematical models throughout history accurate mathematical models developed newton laws accurately describe many everyday phenomena certain limits theory relativity quantum mechanics must used common use idealized models physics simplify things massless ropes point particles ideal gases particle box among many simplified models used physics laws physics represented simple equations newton laws maxwell equations schr dinger equation laws basis making mathematical models real situations many real situations complex thus modeled approximate computer model computationally feasible compute made basic laws approximate models made basic laws example molecules modeled molecular orbital models approximate solutions schr dinger equation engineering physics models often made mathematical methods finite element analysis different mathematical models use different geometries necessarily accurate descriptions geometry universe euclidean geometry much used classical physics special relativity general relativity examples theories use geometries euclidean since prehistorical times simple models maps diagrams used often engineers analyze system controlled optimized use mathematical model analysis engineers build descriptive model system hypothesis system could work try estimate unforeseeable event could affect system similarly control system engineers try different control approaches simulations mathematical model usually describes system set variables set equations establish relationships variables variables may many types real integer numbers boolean values strings example variables represent properties system example measured system outputs often form signals timing data counters event occurrence actual model set functions describe relations different variables state represents even number input far signifies odd number input change state automaton input ends state show whether input contained even number input contain even number finish state accepting state input string accepted language recognized regular language given regular expression kleene star e g denotes non negative number symbols written also
https://en.wikipedia.org/wiki/Training_data,machine learning common task study construction algorithms learn make predictions data algorithms function making data driven predictions decisions building mathematical model input data data used build final model usually comes multiple datasets particular three datasets commonly used different stages creation model model initially fit training dataset set examples used fit parameters model model trained training dataset using supervised learning method example using optimization methods gradient descent stochastic gradient descent practice training dataset often consists pairs input vector corresponding output vector answer key commonly denoted target current model run training dataset produces result compared target input vector training dataset based result comparison specific learning algorithm used parameters model adjusted model fitting include variable selection parameter estimation successively fitted model used predict responses observations second dataset called validation dataset validation dataset provides unbiased evaluation model fit training dataset tuning model hyperparameters neural network validation datasets used regularization early stopping simple procedure complicated practice fact validation dataset error may fluctuate training producing multiple local minima complication led creation many ad hoc rules deciding overfitting truly begun finally test dataset dataset used provide unbiased evaluation final model fit training dataset data test dataset never used training test dataset also called holdout dataset training dataset dataset examples used learning process used fit parameters example classifier approaches search training data empirical relationships tend overfit data meaning identify exploit apparent relationships training data hold general validation dataset dataset examples used tune hyperparameters classifier sometimes also called development set dev set example hyperparameter artificial neural networks includes number hidden units layer well testing set follow probability distribution training dataset order avoid overfitting classification parameter needs adjusted necessary validation dataset addition training test datasets example suitable classifier problem sought training dataset used train candidate algorithms validation dataset used compare performances decide one take finally test dataset used obtain performance characteristics accuracy sensitivity specificity f measure validation dataset functions hybrid training data used testing neither part low level training part final testing basic process using validation dataset model selection since goal find network best performance new data simplest approach comparison different networks evaluate error function using data independent used training various networks trained minimization appropriate error function defined respect training data set performance networks compared evaluating error function using independent validation set network smallest error respect validation set selected approach called hold method since procedure lead overfitting validation set performance selected network confirmed measuring performance third independent set data called test set application process early stopping candidate models successive iterations network training stops error validation set grows choosing previous model test dataset dataset independent training dataset follows probability distribution training dataset model fit training dataset also fits test dataset well minimal overfitting taken place better fitting training dataset opposed test dataset usually points overfitting test set therefore set examples used assess performance fully specified classifier part original dataset set aside used test set known holdout method terms test set validation set sometimes used way flips meaning industry academia erroneous usage test set becomes development set validation set independent set used evaluate performance fully specified classifier literature machine learning often reverses meaning validation test sets blatant example terminological confusion pervades artificial intelligence research dataset repeatedly split training dataset validation dataset known cross validation repeated partitions done various ways dividing equal datasets using training validation validation training repeatedly selecting random subset validation dataset citation needed validate model performance sometimes additional test dataset held cross validation used citation needed another example parameter adjustment hierarchical classification splits complete multi class problem set smaller classification problems serves learning accurate concepts due simpler classification boundaries subtasks individual feature selection procedures subtasks classification decomposition central choice order combination smaller classification steps called classification path depending application derived confusion matrix uncovering reasons typical errors finding ways prevent system make future example validation set one see classes frequently mutually confused system instance space decomposition done follows firstly classification done among well recognizable classes difficult separate classes treated single joint class finally second classification step joint class classified two initially mutually confused classes citation needed
https://en.wikipedia.org/wiki/Email_filtering,email filtering processing email organize according specified criteria term apply intervention human intelligence often refers automatic processing incoming messages anti spam techniques outgoing emails well received email filtering software may reject item initial smtp connection stage pass unchanged delivery user mailbox alternatively redirect message delivery elsewhere quarantine checking edit tag way common uses mail filters include organizing incoming email removal spam computer viruses less common use inspect outgoing email companies ensure employees comply appropriate policies laws users might also employ mail filter prioritize messages sort folders based subject matter criteria mailbox providers also install mail filters mail transfer agents service customers anti virus anti spam url filtering authentication based rejections common filter types corporations often use filters protect employees information technology assets catch filter catch emails addressed domain exist mail server help avoid losing emails due misspelling users may able install separate programs configure filtering part email program email programs users make personal manual filters automatically filter mail according chosen criteria mail filters operate inbound outbound email traffic inbound email filtering involves scanning messages internet addressed users protected filtering system lawful interception outbound email filtering involves reverse scanning email messages local users potentially harmful messages delivered others internet one method outbound email filtering commonly used internet service providers transparent smtp proxying email traffic intercepted filtered via transparent proxy within network outbound filtering also take place email server many corporations employ data leak prevention technology outbound mail servers prevent leakage sensitive information via email mail filters varying degrees configurability sometimes make decisions based matching regular expression times code may match keywords message body perhaps email address sender message complex control flow logic possible programming languages typically implemented data driven programming language procmail specifies conditions match actions take matching may involve matching advanced filters particularly anti spam filters use statistical document classification techniques naive bayes classifier image filtering use complex image analysis algorithms detect skin tones specific body shapes normally associated pornographic images microsoft outlook includes user generated email filters called rules
https://en.wikipedia.org/wiki/Computer_vision,computer vision interdisciplinary scientific field deals computers gain high level understanding digital images videos perspective engineering seeks understand automate tasks human visual system computer vision tasks include methods acquiring processing analyzing understanding digital images extraction high dimensional data real world order produce numerical symbolic information e g forms decisions understanding context means transformation visual images descriptions world make sense thought processes elicit appropriate action image understanding seen disentangling symbolic information image data using models constructed aid geometry physics statistics learning theory scientific discipline computer vision concerned theory behind artificial systems extract information images image data take many forms video sequences views multiple cameras multi dimensional data scanner medical scanning device technological discipline computer vision seeks apply theories models construction computer vision systems sub domains computer vision include scene reconstruction event detection video tracking object recognition pose estimation learning indexing motion estimation visual servoing scene modeling image restoration computer vision interdisciplinary field deals computers made gain high level understanding digital images videos perspective engineering seeks automate tasks human visual system computer vision concerned automatic extraction analysis understanding useful information single image sequence images involves development theoretical algorithmic basis achieve automatic visual understanding scientific discipline computer vision concerned theory behind artificial systems extract information images image data take many forms video sequences views multiple cameras multi dimensional data medical scanner technological discipline computer vision seeks apply theories models construction computer vision systems late computer vision began universities pioneering artificial intelligence meant mimic human visual system stepping stone endowing robots intelligent behavior believed could achieved summer project attaching camera computer describe saw distinguished computer vision prevalent field digital image processing time desire extract three dimensional structure images goal achieving full scene understanding studies formed early foundations many computer vision algorithms exist today including extraction edges images labeling lines non polyhedral polyhedral modeling representation objects interconnections smaller structures optical flow motion estimation next decade saw studies based rigorous mathematical analysis quantitative aspects computer vision include concept scale space inference shape various cues shading texture focus contour models known snakes researchers also realized many mathematical concepts could treated within optimization framework regularization markov random fields previous research topics became active others research projective reconstructions led better understanding camera calibration advent optimization methods camera calibration realized lot ideas already explored bundle adjustment theory field photogrammetry led methods sparse reconstructions scenes multiple images progress made dense stereo correspondence problem multi view stereo techniques time variations graph cut used solve image segmentation decade also marked first time statistical learning techniques used practice recognize faces images toward end significant change came increased interaction fields computer graphics computer vision included image based rendering image morphing view interpolation panoramic image stitching early light field rendering recent work seen resurgence feature based methods used conjunction machine learning techniques complex optimization frameworks advancement deep learning techniques brought life field computer vision accuracy deep learning algorithms several benchmark computer vision data sets tasks ranging classification segmentation optical flow surpassed prior methods citation needed areas artificial intelligence deal autonomous path planning deliberation robotic systems navigate environment detailed understanding environments required navigate information environment could provided computer vision system acting vision sensor providing high level information environment robot artificial intelligence computer vision share topics pattern recognition learning techniques consequently computer vision sometimes seen part artificial intelligence field computer science field general computer vision often considered part information engineering solid state physics another field closely related computer vision computer vision systems rely image sensors detect electromagnetic radiation typically form either visible infra red light sensors designed using quantum physics process light interacts surfaces explained using physics physics explains behavior optics core part imaging systems sophisticated image sensors even require quantum mechanics provide complete understanding image formation process also various measurement problems physics addressed using computer vision example motion fluids third field plays important role neurobiology specifically study biological vision system last century extensive study eyes neurons brain structures devoted processing visual stimuli humans various animals led coarse yet complicated description real vision systems operate order solve certain vision related tasks results led sub field within computer vision artificial systems designed mimic processing behavior biological systems different levels complexity also learning based methods developed within computer vision background biology strands computer vision research closely related study biological vision indeed many strands ai research closely tied research human consciousness use stored knowledge interpret integrate utilize visual information field biological vision studies models physiological processes behind visual perception humans animals computer vision hand studies describes processes implemented software hardware behind artificial vision systems interdisciplinary exchange biological computer vision proven fruitful fields yet another field related computer vision signal processing many methods processing one variable signals typically temporal signals extended natural way processing two variable signals multi variable signals computer vision however specific nature images many methods developed within computer vision counterpart processing one variable signals together multi dimensionality signal defines subfield signal processing part computer vision beside mentioned views computer vision many related research topics also studied purely mathematical point view example many methods computer vision based statistics optimization geometry finally significant part field devoted implementation aspect computer vision existing methods realized various combinations software hardware methods modified order gain processing speed without losing much performance computer vision also used fashion ecommerce inventory management patent search furniture beauty industry citation needed fields closely related computer vision image processing image analysis machine vision significant overlap range techniques applications cover implies basic techniques used developed fields similar something interpreted one field different names hand appears necessary research groups scientific journals conferences companies present market belonging specifically one fields hence various characterizations distinguish fields others presented computer graphics produces image data models computer vision often produces models image data also trend towards combination two disciplines e g explored augmented reality following characterizations appear relevant taken universally accepted photogrammetry also overlaps computer vision e g stereophotogrammetry vs computer stereo vision applications range tasks industrial machine vision systems say inspect bottles speeding production line research artificial intelligence computers robots comprehend world around computer vision machine vision fields significant overlap computer vision covers core technology automated image analysis used many fields machine vision usually refers process combining automated image analysis methods technologies provide automated inspection robot guidance industrial applications many computer vision applications computers pre programmed solve particular task methods based learning becoming increasingly common examples applications computer vision include systems one prominent application fields medical computer vision medical image processing characterized extraction information image data diagnose patient example detection tumours arteriosclerosis malign changes measurements organ dimensions blood flow etc another example also supports medical research providing new information e g structure brain quality medical treatments applications computer vision medical area also includes enhancement images interpreted humans ultrasonic images x ray images example reduce influence noise second application area computer vision industry sometimes called machine vision information extracted purpose supporting manufacturing process one example quality control details final products automatically inspected order find defects another example measurement position orientation details picked robot arm machine vision also heavily used agricultural process remove undesirable food stuff bulk material process called optical sorting military applications probably one largest areas computer vision obvious examples detection enemy soldiers vehicles missile guidance advanced systems missile guidance send missile area rather specific target target selection made missile reaches area based locally acquired image data modern military concepts battlefield awareness imply various sensors including image sensors provide rich set information combat scene used support strategic decisions case automatic processing data used reduce complexity fuse information multiple sensors increase reliability one newer application areas autonomous vehicles include submersibles land based vehicles aerial vehicles unmanned aerial vehicles level autonomy ranges fully autonomous vehicles vehicles computer vision based systems support driver pilot various situations fully autonomous vehicles typically use computer vision navigation e g knowing producing map environment detecting obstacles also used detecting certain task specific events e g uav looking forest fires examples supporting systems obstacle warning systems cars systems autonomous landing aircraft several car manufacturers demonstrated systems autonomous driving cars technology still reached level put market ample examples military autonomous vehicles ranging advanced missiles uavs recon missions missile guidance space exploration already made autonomous vehicles using computer vision e g nasa curiosity cnsa yutu rover materials rubber silicon used create sensors allow applications detecting micro undulations calibrating robotic hands rubber used order create mold placed finger inside mold would multiple strain gauges finger mold sensors could placed top small sheet rubber containing array rubber pins user wear finger mold trace surface computer read data strain gauges measure one pins pushed upward pin pushed upward computer recognize imperfection surface sort technology useful order receive accurate data imperfections large surface another variation finger mold sensor sensors contain camera suspended silicon silicon forms dome around outside camera embedded silicon point markers equally spaced cameras placed devices robotic hands order allow computer receive highly accurate tactile data application areas include application areas described employ range computer vision tasks less well defined measurement problems processing problems solved using variety methods examples typical computer vision tasks presented computer vision tasks include methods acquiring processing analyzing understanding digital images extraction high dimensional data real world order produce numerical symbolic information e g forms decisions understanding context means transformation visual images descriptions world interface thought processes elicit appropriate action image understanding seen disentangling symbolic information image data using models constructed aid geometry physics statistics learning theory classical problem computer vision image processing machine vision determining whether image data contains specific object feature activity different varieties recognition problem described literature citation needed currently best algorithms tasks based convolutional neural networks illustration capabilities given imagenet large scale visual recognition challenge benchmark object classification detection millions images hundreds object classes performance convolutional neural networks imagenet tests close humans best algorithms still struggle objects small thin small ant stem flower person holding quill hand also trouble images distorted filters contrast kinds images rarely trouble humans humans however tend trouble issues example good classifying objects fine grained classes particular breed dog species bird whereas convolutional neural networks handle ease citation needed several specialized tasks based recognition exist several tasks relate motion estimation image sequence processed produce estimate velocity either points image scene even camera produces images examples tasks given one images scene video scene reconstruction aims computing model scene simplest case model set points sophisticated methods produce complete surface model advent imaging requiring motion scanning related processing algorithms enabling rapid advances field grid based sensing used acquire images multiple angles algorithms available stitch multiple images together point clouds models aim image restoration removal noise images simplest possible approach noise removal various types filters low pass filters median filters sophisticated methods assume model local image structures look distinguish noise first analysing image data terms local image structures lines edges controlling filtering based local information analysis step better level noise removal usually obtained compared simpler approaches example field inpainting organization computer vision system highly application dependent systems stand alone applications solve specific measurement detection problem others constitute sub system larger design example also contains sub systems control mechanical actuators planning information databases man machine interfaces etc specific implementation computer vision system also depends whether functionality pre specified part learned modified operation many functions unique application however typical functions found many computer vision systems image understanding systems include three levels abstraction follows low level includes image primitives edges texture elements regions intermediate level includes boundaries surfaces volumes high level includes objects scenes events many requirements entirely topics research representational requirements designing ius levels representation prototypical concepts concept organization spatial knowledge temporal knowledge scaling description comparison differentiation inference refers process deriving new explicitly represented facts currently known facts control refers process selects many inference search matching techniques applied particular stage processing inference control requirements ius search hypothesis activation matching hypothesis testing generation use expectations change focus attention certainty strength belief inference goal satisfaction many kinds computer vision systems however contain basic elements power source least one image acquisition device processor control communication cables kind wireless interconnection mechanism addition practical vision system contains software well display order monitor system vision systems inner spaces industrial ones contain illumination system may placed controlled environment furthermore completed system includes many accessories camera supports cables connectors computer vision systems use visible light cameras passively viewing scene frame rates frames per second computer vision systems use image acquisition hardware active illumination something visible light structured light scanners thermographic cameras hyperspectral imagers radar imaging lidar scanners magnetic resonance images side scan sonar synthetic aperture sonar etc hardware captures images processed often using computer vision algorithms used process visible light images traditional broadcast consumer video systems operate rate frames per second advances digital signal processing consumer graphics hardware made high speed image acquisition processing display possible real time systems order hundreds thousands frames per second applications robotics fast real time video systems critically important often simplify processing needed certain algorithms combined high speed projector fast image acquisition allows measurement feature tracking realised egocentric vision systems composed wearable camera automatically take pictures first person perspective vision processing units emerging new class processor complement cpus graphics processing units role
